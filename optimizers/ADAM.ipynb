{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/havish/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [255.26937728]\n",
      "Epoch: 1 Loss: [251.39774303]\n",
      "Epoch: 2 Loss: [249.77485871]\n",
      "Epoch: 3 Loss: [247.41999499]\n",
      "Epoch: 4 Loss: [242.8480273]\n",
      "Epoch: 5 Loss: [233.80911716]\n",
      "Epoch: 6 Loss: [217.39008018]\n",
      "Epoch: 7 Loss: [196.61713274]\n",
      "Epoch: 8 Loss: [173.52667157]\n",
      "Epoch: 9 Loss: [151.74173198]\n",
      "Epoch: 10 Loss: [130.94070252]\n",
      "Epoch: 11 Loss: [112.17851765]\n",
      "Epoch: 12 Loss: [95.09389362]\n",
      "Epoch: 13 Loss: [81.09658334]\n",
      "Epoch: 14 Loss: [71.04130103]\n",
      "Epoch: 15 Loss: [64.0643374]\n",
      "Epoch: 16 Loss: [58.93028025]\n",
      "Epoch: 17 Loss: [55.05095466]\n",
      "Epoch: 18 Loss: [52.13222401]\n",
      "Epoch: 19 Loss: [49.71648704]\n",
      "Epoch: 20 Loss: [47.61215499]\n",
      "Epoch: 21 Loss: [45.61933838]\n",
      "Epoch: 22 Loss: [43.80851721]\n",
      "Epoch: 23 Loss: [42.26481952]\n",
      "Epoch: 24 Loss: [40.78296189]\n",
      "Epoch: 25 Loss: [39.115736]\n",
      "Epoch: 26 Loss: [37.56478123]\n",
      "Epoch: 27 Loss: [36.19761]\n",
      "Epoch: 28 Loss: [34.94455688]\n",
      "Epoch: 29 Loss: [33.75372876]\n",
      "Epoch: 30 Loss: [32.62417801]\n",
      "Epoch: 31 Loss: [31.57242477]\n",
      "Epoch: 32 Loss: [30.55136034]\n",
      "Epoch: 33 Loss: [29.59270039]\n",
      "Epoch: 34 Loss: [28.66853197]\n",
      "Epoch: 35 Loss: [27.78293659]\n",
      "Epoch: 36 Loss: [26.94064816]\n",
      "Epoch: 37 Loss: [26.13199778]\n",
      "Epoch: 38 Loss: [25.35635762]\n",
      "Epoch: 39 Loss: [24.6135086]\n",
      "Epoch: 40 Loss: [23.90076643]\n",
      "Epoch: 41 Loss: [23.21668999]\n",
      "Epoch: 42 Loss: [22.56427945]\n",
      "Epoch: 43 Loss: [21.93743578]\n",
      "Epoch: 44 Loss: [21.3385181]\n",
      "Epoch: 45 Loss: [20.76598397]\n",
      "Epoch: 46 Loss: [20.21664587]\n",
      "Epoch: 47 Loss: [19.69237884]\n",
      "Epoch: 48 Loss: [19.18788667]\n",
      "Epoch: 49 Loss: [18.6714684]\n",
      "Epoch: 50 Loss: [18.10693785]\n",
      "Epoch: 51 Loss: [17.46454331]\n",
      "Epoch: 52 Loss: [16.71257776]\n",
      "Epoch: 53 Loss: [15.96790361]\n",
      "Epoch: 54 Loss: [15.39962777]\n",
      "Epoch: 55 Loss: [14.86418653]\n",
      "Epoch: 56 Loss: [14.37384352]\n",
      "Epoch: 57 Loss: [14.00879019]\n",
      "Epoch: 58 Loss: [13.66109791]\n",
      "Epoch: 59 Loss: [13.34504105]\n",
      "Epoch: 60 Loss: [13.06737938]\n",
      "Epoch: 61 Loss: [12.82956214]\n",
      "Epoch: 62 Loss: [12.61653934]\n",
      "Epoch: 63 Loss: [12.42207335]\n",
      "Epoch: 64 Loss: [12.24801923]\n",
      "Epoch: 65 Loss: [12.08452028]\n",
      "Epoch: 66 Loss: [11.94098892]\n",
      "Epoch: 67 Loss: [11.79986221]\n",
      "Epoch: 68 Loss: [11.67010369]\n",
      "Epoch: 69 Loss: [11.54947027]\n",
      "Epoch: 70 Loss: [11.43013626]\n",
      "Epoch: 71 Loss: [11.32493547]\n",
      "Epoch: 72 Loss: [11.22135446]\n",
      "Epoch: 73 Loss: [11.12396472]\n",
      "Epoch: 74 Loss: [11.03030738]\n",
      "Epoch: 75 Loss: [10.94131269]\n",
      "Epoch: 76 Loss: [10.85895391]\n",
      "Epoch: 77 Loss: [10.77603842]\n",
      "Epoch: 78 Loss: [10.69779403]\n",
      "Epoch: 79 Loss: [10.62544375]\n",
      "Epoch: 80 Loss: [10.55354253]\n",
      "Epoch: 81 Loss: [10.48453235]\n",
      "Epoch: 82 Loss: [10.41790599]\n",
      "Epoch: 83 Loss: [10.35386886]\n",
      "Epoch: 84 Loss: [10.29040763]\n",
      "Epoch: 85 Loss: [10.22801199]\n",
      "Epoch: 86 Loss: [10.17015448]\n",
      "Epoch: 87 Loss: [10.11355375]\n",
      "Epoch: 88 Loss: [10.06014571]\n",
      "Epoch: 89 Loss: [10.00666653]\n",
      "Epoch: 90 Loss: [9.95778665]\n",
      "Epoch: 91 Loss: [9.90769837]\n",
      "Epoch: 92 Loss: [9.85970681]\n",
      "Epoch: 93 Loss: [9.81286614]\n",
      "Epoch: 94 Loss: [9.76671993]\n",
      "Epoch: 95 Loss: [9.72353187]\n",
      "Epoch: 96 Loss: [9.67962454]\n",
      "Epoch: 97 Loss: [9.63872981]\n",
      "Epoch: 98 Loss: [9.59679879]\n",
      "Epoch: 99 Loss: [9.55620623]\n",
      "Epoch: 100 Loss: [9.51804343]\n",
      "Epoch: 101 Loss: [9.47770707]\n",
      "Epoch: 102 Loss: [9.43981373]\n",
      "Epoch: 103 Loss: [9.40244164]\n",
      "Epoch: 104 Loss: [9.36664383]\n",
      "Epoch: 105 Loss: [9.33020743]\n",
      "Epoch: 106 Loss: [9.29503644]\n",
      "Epoch: 107 Loss: [9.26058736]\n",
      "Epoch: 108 Loss: [9.22675125]\n",
      "Epoch: 109 Loss: [9.19323087]\n",
      "Epoch: 110 Loss: [9.16045799]\n",
      "Epoch: 111 Loss: [9.12838034]\n",
      "Epoch: 112 Loss: [9.08325586]\n",
      "Epoch: 113 Loss: [9.05475862]\n",
      "Epoch: 114 Loss: [9.02601792]\n",
      "Epoch: 115 Loss: [8.99770425]\n",
      "Epoch: 116 Loss: [8.97009204]\n",
      "Epoch: 117 Loss: [8.94310624]\n",
      "Epoch: 118 Loss: [8.91623641]\n",
      "Epoch: 119 Loss: [8.89065775]\n",
      "Epoch: 120 Loss: [8.86696252]\n",
      "Epoch: 121 Loss: [8.83885566]\n",
      "Epoch: 122 Loss: [8.81702516]\n",
      "Epoch: 123 Loss: [8.79189059]\n",
      "Epoch: 124 Loss: [8.76961931]\n",
      "Epoch: 125 Loss: [8.74448217]\n",
      "Epoch: 126 Loss: [8.72250117]\n",
      "Epoch: 127 Loss: [8.67844945]\n",
      "Epoch: 128 Loss: [8.68282689]\n",
      "Epoch: 129 Loss: [8.65649532]\n",
      "Epoch: 130 Loss: [8.63590308]\n",
      "Epoch: 131 Loss: [8.61446901]\n",
      "Epoch: 132 Loss: [8.57694216]\n",
      "Epoch: 133 Loss: [8.57954819]\n",
      "Epoch: 134 Loss: [8.55465489]\n",
      "Epoch: 135 Loss: [8.53535612]\n",
      "Epoch: 136 Loss: [8.50902959]\n",
      "Epoch: 137 Loss: [8.48467302]\n",
      "Epoch: 138 Loss: [8.48502493]\n",
      "Epoch: 139 Loss: [8.46044304]\n",
      "Epoch: 140 Loss: [8.42854947]\n",
      "Epoch: 141 Loss: [8.43261697]\n",
      "Epoch: 142 Loss: [8.40750005]\n",
      "Epoch: 143 Loss: [8.38621155]\n",
      "Epoch: 144 Loss: [8.37155161]\n",
      "Epoch: 145 Loss: [8.35483503]\n",
      "Epoch: 146 Loss: [8.33900771]\n",
      "Epoch: 147 Loss: [8.3232816]\n",
      "Epoch: 148 Loss: [8.30782506]\n",
      "Epoch: 149 Loss: [8.29258844]\n",
      "Epoch: 150 Loss: [8.27717696]\n",
      "Epoch: 151 Loss: [8.26277612]\n",
      "Epoch: 152 Loss: [8.24813508]\n",
      "Epoch: 153 Loss: [8.23377262]\n",
      "Epoch: 154 Loss: [8.21959368]\n",
      "Epoch: 155 Loss: [8.20599344]\n",
      "Epoch: 156 Loss: [8.19144824]\n",
      "Epoch: 157 Loss: [8.17827327]\n",
      "Epoch: 158 Loss: [8.16478011]\n",
      "Epoch: 159 Loss: [8.151552]\n",
      "Epoch: 160 Loss: [8.13847806]\n",
      "Epoch: 161 Loss: [8.12557869]\n",
      "Epoch: 162 Loss: [8.11284471]\n",
      "Epoch: 163 Loss: [8.10027147]\n",
      "Epoch: 164 Loss: [8.08847203]\n",
      "Epoch: 165 Loss: [8.0755266]\n",
      "Epoch: 166 Loss: [8.06266027]\n",
      "Epoch: 167 Loss: [8.05212351]\n",
      "Epoch: 168 Loss: [8.03957689]\n",
      "Epoch: 169 Loss: [8.02802723]\n",
      "Epoch: 170 Loss: [8.01646741]\n",
      "Epoch: 171 Loss: [8.0050811]\n",
      "Epoch: 172 Loss: [7.99382017]\n",
      "Epoch: 173 Loss: [7.98269878]\n",
      "Epoch: 174 Loss: [7.96999236]\n",
      "Epoch: 175 Loss: [7.95950669]\n",
      "Epoch: 176 Loss: [7.94859863]\n",
      "Epoch: 177 Loss: [7.93797123]\n",
      "Epoch: 178 Loss: [7.92724445]\n",
      "Epoch: 179 Loss: [7.91698758]\n",
      "Epoch: 180 Loss: [7.90668124]\n",
      "Epoch: 181 Loss: [7.89598049]\n",
      "Epoch: 182 Loss: [7.88690423]\n",
      "Epoch: 183 Loss: [7.87645021]\n",
      "Epoch: 184 Loss: [7.86664343]\n",
      "Epoch: 185 Loss: [7.85698536]\n",
      "Epoch: 186 Loss: [7.84678744]\n",
      "Epoch: 187 Loss: [7.83843577]\n",
      "Epoch: 188 Loss: [7.82839517]\n",
      "Epoch: 189 Loss: [7.81913126]\n",
      "Epoch: 190 Loss: [7.80996169]\n",
      "Epoch: 191 Loss: [7.80027307]\n",
      "Epoch: 192 Loss: [7.79237388]\n",
      "Epoch: 193 Loss: [7.78281829]\n",
      "Epoch: 194 Loss: [7.77401752]\n",
      "Epoch: 195 Loss: [7.76529754]\n",
      "Epoch: 196 Loss: [7.75658499]\n",
      "Epoch: 197 Loss: [7.74753052]\n",
      "Epoch: 198 Loss: [7.74003138]\n",
      "Epoch: 199 Loss: [7.73103944]\n",
      "Epoch: 200 Loss: [7.72281277]\n",
      "Epoch: 201 Loss: [7.71446016]\n",
      "Epoch: 202 Loss: [7.70634016]\n",
      "Epoch: 203 Loss: [7.6977357]\n",
      "Epoch: 204 Loss: [7.69066404]\n",
      "Epoch: 205 Loss: [7.68215853]\n",
      "Epoch: 206 Loss: [7.67438446]\n",
      "Epoch: 207 Loss: [7.66652869]\n",
      "Epoch: 208 Loss: [7.65817834]\n",
      "Epoch: 209 Loss: [7.65159478]\n",
      "Epoch: 210 Loss: [7.64339178]\n",
      "Epoch: 211 Loss: [7.63594553]\n",
      "Epoch: 212 Loss: [7.62844706]\n",
      "Epoch: 213 Loss: [7.61423744]\n",
      "Epoch: 214 Loss: [7.61548388]\n",
      "Epoch: 215 Loss: [7.60575546]\n",
      "Epoch: 216 Loss: [7.5988228]\n",
      "Epoch: 217 Loss: [7.5916457]\n",
      "Epoch: 218 Loss: [7.57811626]\n",
      "Epoch: 219 Loss: [7.57933184]\n",
      "Epoch: 220 Loss: [7.57004631]\n",
      "Epoch: 221 Loss: [7.56342679]\n",
      "Epoch: 222 Loss: [7.55074476]\n",
      "Epoch: 223 Loss: [7.55104167]\n",
      "Epoch: 224 Loss: [7.54218148]\n",
      "Epoch: 225 Loss: [7.53085302]\n",
      "Epoch: 226 Loss: [7.53068882]\n",
      "Epoch: 227 Loss: [7.52258691]\n",
      "Epoch: 228 Loss: [7.51625804]\n",
      "Epoch: 229 Loss: [7.50445993]\n",
      "Epoch: 230 Loss: [7.50440888]\n",
      "Epoch: 231 Loss: [7.49696192]\n",
      "Epoch: 232 Loss: [7.48534294]\n",
      "Epoch: 233 Loss: [7.48530479]\n",
      "Epoch: 234 Loss: [7.4724838]\n",
      "Epoch: 235 Loss: [7.47340385]\n",
      "Epoch: 236 Loss: [7.46489407]\n",
      "Epoch: 237 Loss: [7.45447085]\n",
      "Epoch: 238 Loss: [7.45503452]\n",
      "Epoch: 239 Loss: [7.44165511]\n",
      "Epoch: 240 Loss: [7.44314462]\n",
      "Epoch: 241 Loss: [7.43484551]\n",
      "Epoch: 242 Loss: [7.42498696]\n",
      "Epoch: 243 Loss: [7.42539913]\n",
      "Epoch: 244 Loss: [7.41264166]\n",
      "Epoch: 245 Loss: [7.4140279]\n",
      "Epoch: 246 Loss: [7.40117919]\n",
      "Epoch: 247 Loss: [7.40266778]\n",
      "Epoch: 248 Loss: [7.39473854]\n",
      "Epoch: 249 Loss: [7.38551352]\n",
      "Epoch: 250 Loss: [7.38580533]\n",
      "Epoch: 251 Loss: [7.3738476]\n",
      "Epoch: 252 Loss: [7.3750387]\n",
      "Epoch: 253 Loss: [7.36295112]\n",
      "Epoch: 254 Loss: [7.36426509]\n",
      "Epoch: 255 Loss: [7.35229602]\n",
      "Epoch: 256 Loss: [7.35364088]\n",
      "Epoch: 257 Loss: [7.34182363]\n",
      "Epoch: 258 Loss: [7.34317392]\n",
      "Epoch: 259 Loss: [7.33152192]\n",
      "Epoch: 260 Loss: [7.33286133]\n",
      "Epoch: 261 Loss: [7.32138166]\n",
      "Epoch: 262 Loss: [7.32269867]\n",
      "Epoch: 263 Loss: [7.31141]\n",
      "Epoch: 264 Loss: [7.31262694]\n",
      "Epoch: 265 Loss: [7.30157181]\n",
      "Epoch: 266 Loss: [7.30280528]\n",
      "Epoch: 267 Loss: [7.29186711]\n",
      "Epoch: 268 Loss: [7.29307696]\n",
      "Epoch: 269 Loss: [7.28232743]\n",
      "Epoch: 270 Loss: [7.2834267]\n",
      "Epoch: 271 Loss: [7.27314529]\n",
      "Epoch: 272 Loss: [7.27375629]\n",
      "Epoch: 273 Loss: [7.26386334]\n",
      "Epoch: 274 Loss: [7.26461813]\n",
      "Epoch: 275 Loss: [7.25449786]\n",
      "Epoch: 276 Loss: [7.25545048]\n",
      "Epoch: 277 Loss: [7.24544082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 278 Loss: [7.24247377]\n",
      "Epoch: 279 Loss: [7.24128746]\n",
      "Epoch: 280 Loss: [7.2318284]\n",
      "Epoch: 281 Loss: [7.23288363]\n",
      "Epoch: 282 Loss: [7.22304158]\n",
      "Epoch: 283 Loss: [7.22415724]\n",
      "Epoch: 284 Loss: [7.21445652]\n",
      "Epoch: 285 Loss: [7.2155159]\n",
      "Epoch: 286 Loss: [7.20598344]\n",
      "Epoch: 287 Loss: [7.20698527]\n",
      "Epoch: 288 Loss: [7.19761428]\n",
      "Epoch: 289 Loss: [7.19856556]\n",
      "Epoch: 290 Loss: [7.18934731]\n",
      "Epoch: 291 Loss: [7.19025492]\n",
      "Epoch: 292 Loss: [7.18118144]\n",
      "Epoch: 293 Loss: [7.18205167]\n",
      "Epoch: 294 Loss: [7.17311515]\n",
      "Epoch: 295 Loss: [7.17048633]\n",
      "Epoch: 296 Loss: [7.16938312]\n",
      "Epoch: 297 Loss: [7.16095423]\n",
      "Epoch: 298 Loss: [7.16188527]\n",
      "Epoch: 299 Loss: [7.15308971]\n",
      "Epoch: 300 Loss: [7.15408782]\n",
      "Epoch: 301 Loss: [7.14540024]\n",
      "Epoch: 302 Loss: [7.14635977]\n",
      "Epoch: 303 Loss: [7.13780446]\n",
      "Epoch: 304 Loss: [7.13872528]\n",
      "Epoch: 305 Loss: [7.13029631]\n",
      "Epoch: 306 Loss: [7.13118612]\n",
      "Epoch: 307 Loss: [7.12287533]\n",
      "Epoch: 308 Loss: [7.12374002]\n",
      "Epoch: 309 Loss: [7.11553938]\n",
      "Epoch: 310 Loss: [7.11638475]\n",
      "Epoch: 311 Loss: [7.10828762]\n",
      "Epoch: 312 Loss: [7.10911935]\n",
      "Epoch: 313 Loss: [7.10111945]\n",
      "Epoch: 314 Loss: [7.10194273]\n",
      "Epoch: 315 Loss: [7.09403471]\n",
      "Epoch: 316 Loss: [7.09485355]\n",
      "Epoch: 317 Loss: [7.08703176]\n",
      "Epoch: 318 Loss: [7.08785004]\n",
      "Epoch: 319 Loss: [7.08010967]\n",
      "Epoch: 320 Loss: [7.08093094]\n",
      "Epoch: 321 Loss: [7.0732675]\n",
      "Epoch: 322 Loss: [7.07409484]\n",
      "Epoch: 323 Loss: [7.06650418]\n",
      "Epoch: 324 Loss: [7.06734047]\n",
      "Epoch: 325 Loss: [7.06264889]\n",
      "Epoch: 326 Loss: [7.05721305]\n",
      "Epoch: 327 Loss: [7.05737582]\n",
      "Epoch: 328 Loss: [7.05024233]\n",
      "Epoch: 329 Loss: [7.05088534]\n",
      "Epoch: 330 Loss: [7.04368184]\n",
      "Epoch: 331 Loss: [7.04437733]\n",
      "Epoch: 332 Loss: [7.03722566]\n",
      "Epoch: 333 Loss: [7.03504879]\n",
      "Epoch: 334 Loss: [7.02969591]\n",
      "Epoch: 335 Loss: [7.02484588]\n",
      "Epoch: 336 Loss: [7.02073797]\n",
      "Epoch: 337 Loss: [7.0162257]\n",
      "Epoch: 338 Loss: [7.02981396]\n",
      "Epoch: 339 Loss: [7.0101413]\n",
      "Epoch: 340 Loss: [7.01560042]\n",
      "Epoch: 341 Loss: [7.00799983]\n",
      "Epoch: 342 Loss: [7.00402252]\n",
      "Epoch: 343 Loss: [6.99978073]\n",
      "Epoch: 344 Loss: [6.99546955]\n",
      "Epoch: 345 Loss: [7.00908077]\n",
      "Epoch: 346 Loss: [6.9897395]\n",
      "Epoch: 347 Loss: [6.99507754]\n",
      "Epoch: 348 Loss: [6.9876928]\n",
      "Epoch: 349 Loss: [6.98383576]\n",
      "Epoch: 350 Loss: [6.97967979]\n",
      "Epoch: 351 Loss: [6.97550162]\n",
      "Epoch: 352 Loss: [6.98928726]\n",
      "Epoch: 353 Loss: [6.97009725]\n",
      "Epoch: 354 Loss: [6.97535965]\n",
      "Epoch: 355 Loss: [6.96816127]\n",
      "Epoch: 356 Loss: [6.96442828]\n",
      "Epoch: 357 Loss: [6.95998463]\n",
      "Epoch: 358 Loss: [6.9563517]\n",
      "Epoch: 359 Loss: [6.97052339]\n",
      "Epoch: 360 Loss: [6.95107686]\n",
      "Epoch: 361 Loss: [6.95408653]\n",
      "Epoch: 362 Loss: [6.94961052]\n",
      "Epoch: 363 Loss: [6.94479769]\n",
      "Epoch: 364 Loss: [6.94120866]\n",
      "Epoch: 365 Loss: [6.9372484]\n",
      "Epoch: 366 Loss: [6.9535721]\n",
      "Epoch: 367 Loss: [6.92922467]\n",
      "Epoch: 368 Loss: [6.93726271]\n",
      "Epoch: 369 Loss: [6.92876439]\n",
      "Epoch: 370 Loss: [6.92601249]\n",
      "Epoch: 371 Loss: [6.92203201]\n",
      "Epoch: 372 Loss: [6.93855861]\n",
      "Epoch: 373 Loss: [6.91441419]\n",
      "Epoch: 374 Loss: [6.9223188]\n",
      "Epoch: 375 Loss: [6.91386727]\n",
      "Epoch: 376 Loss: [6.91134807]\n",
      "Epoch: 377 Loss: [6.9075506]\n",
      "Epoch: 378 Loss: [6.92371354]\n",
      "Epoch: 379 Loss: [6.90028317]\n",
      "Epoch: 380 Loss: [6.90769109]\n",
      "Epoch: 381 Loss: [6.89978554]\n",
      "Epoch: 382 Loss: [6.89712003]\n",
      "Epoch: 383 Loss: [6.8934158]\n",
      "Epoch: 384 Loss: [6.90951642]\n",
      "Epoch: 385 Loss: [6.88644081]\n",
      "Epoch: 386 Loss: [6.89371248]\n",
      "Epoch: 387 Loss: [6.88598506]\n",
      "Epoch: 388 Loss: [6.88337418]\n",
      "Epoch: 389 Loss: [6.87941702]\n",
      "Epoch: 390 Loss: [6.89609628]\n",
      "Epoch: 391 Loss: [6.87270179]\n",
      "Epoch: 392 Loss: [6.87848549]\n",
      "Epoch: 393 Loss: [6.8727422]\n",
      "Epoch: 394 Loss: [6.86925408]\n",
      "Epoch: 395 Loss: [6.86595968]\n",
      "Epoch: 396 Loss: [6.88378195]\n",
      "Epoch: 397 Loss: [6.8589454]\n",
      "Epoch: 398 Loss: [6.86510522]\n",
      "Epoch: 399 Loss: [6.85935979]\n",
      "Epoch: 400 Loss: [6.85602449]\n",
      "Epoch: 401 Loss: [6.87116144]\n",
      "Epoch: 402 Loss: [6.84987535]\n",
      "Epoch: 403 Loss: [6.85543986]\n",
      "Epoch: 404 Loss: [6.84954152]\n",
      "Epoch: 405 Loss: [6.84630555]\n",
      "Epoch: 406 Loss: [6.8427786]\n",
      "Epoch: 407 Loss: [6.86013745]\n",
      "Epoch: 408 Loss: [6.83650214]\n",
      "Epoch: 409 Loss: [6.84262986]\n",
      "Epoch: 410 Loss: [6.83674104]\n",
      "Epoch: 411 Loss: [6.83365467]\n",
      "Epoch: 412 Loss: [6.84803966]\n",
      "Epoch: 413 Loss: [6.82801821]\n",
      "Epoch: 414 Loss: [6.83349122]\n",
      "Epoch: 415 Loss: [6.82590849]\n",
      "Epoch: 416 Loss: [6.82469718]\n",
      "Epoch: 417 Loss: [6.82036011]\n",
      "Epoch: 418 Loss: [6.83895039]\n",
      "Epoch: 419 Loss: [6.81444208]\n",
      "Epoch: 420 Loss: [6.82089925]\n",
      "Epoch: 421 Loss: [6.81495643]\n",
      "Epoch: 422 Loss: [6.81208851]\n",
      "Epoch: 423 Loss: [6.82715781]\n",
      "Epoch: 424 Loss: [6.80665229]\n",
      "Epoch: 425 Loss: [6.81235589]\n",
      "Epoch: 426 Loss: [6.8049235]\n",
      "Epoch: 427 Loss: [6.80367327]\n",
      "Epoch: 428 Loss: [6.79958343]\n",
      "Epoch: 429 Loss: [6.81845533]\n",
      "Epoch: 430 Loss: [6.79387119]\n",
      "Epoch: 431 Loss: [6.80051075]\n",
      "Epoch: 432 Loss: [6.79300337]\n",
      "Epoch: 433 Loss: [6.79194745]\n",
      "Epoch: 434 Loss: [6.80800364]\n",
      "Epoch: 435 Loss: [6.78600715]\n",
      "Epoch: 436 Loss: [6.79190853]\n",
      "Epoch: 437 Loss: [6.78476658]\n",
      "Epoch: 438 Loss: [6.78357813]\n",
      "Epoch: 439 Loss: [6.79822449]\n",
      "Epoch: 440 Loss: [6.77788185]\n",
      "Epoch: 441 Loss: [6.7837972]\n",
      "Epoch: 442 Loss: [6.77658042]\n",
      "Epoch: 443 Loss: [6.77526501]\n",
      "Epoch: 444 Loss: [6.78881025]\n",
      "Epoch: 445 Loss: [6.77008111]\n",
      "Epoch: 446 Loss: [6.77552613]\n",
      "Epoch: 447 Loss: [6.7685996]\n",
      "Epoch: 448 Loss: [6.76577624]\n",
      "Epoch: 449 Loss: [6.7812518]\n",
      "Epoch: 450 Loss: [6.76137476]\n",
      "Epoch: 451 Loss: [6.76715367]\n",
      "Epoch: 452 Loss: [6.76023946]\n",
      "Epoch: 453 Loss: [6.7575065]\n",
      "Epoch: 454 Loss: [6.77323002]\n",
      "Epoch: 455 Loss: [6.75311796]\n",
      "Epoch: 456 Loss: [6.75899856]\n",
      "Epoch: 457 Loss: [6.75214579]\n",
      "Epoch: 458 Loss: [6.74947869]\n",
      "Epoch: 459 Loss: [6.76528974]\n",
      "Epoch: 460 Loss: [6.74512942]\n",
      "Epoch: 461 Loss: [6.7510731]\n",
      "Epoch: 462 Loss: [6.74428472]\n",
      "Epoch: 463 Loss: [6.74167546]\n",
      "Epoch: 464 Loss: [6.75744891]\n",
      "Epoch: 465 Loss: [6.73738314]\n",
      "Epoch: 466 Loss: [6.74336004]\n",
      "Epoch: 467 Loss: [6.73665527]\n",
      "Epoch: 468 Loss: [6.73404448]\n",
      "Epoch: 469 Loss: [6.74951802]\n",
      "Epoch: 470 Loss: [6.73007688]\n",
      "Epoch: 471 Loss: [6.73579431]\n",
      "Epoch: 472 Loss: [6.72922875]\n",
      "Epoch: 473 Loss: [6.72643767]\n",
      "Epoch: 474 Loss: [6.74212912]\n",
      "Epoch: 475 Loss: [6.72270784]\n",
      "Epoch: 476 Loss: [6.7282809]\n",
      "Epoch: 477 Loss: [6.72199015]\n",
      "Epoch: 478 Loss: [6.71940405]\n",
      "Epoch: 479 Loss: [6.73458599]\n",
      "Epoch: 480 Loss: [6.71561047]\n",
      "Epoch: 481 Loss: [6.7211449]\n",
      "Epoch: 482 Loss: [6.71490824]\n",
      "Epoch: 483 Loss: [6.7124111]\n",
      "Epoch: 484 Loss: [6.72720785]\n",
      "Epoch: 485 Loss: [6.7084713]\n",
      "Epoch: 486 Loss: [6.71440041]\n",
      "Epoch: 487 Loss: [6.70795714]\n",
      "Epoch: 488 Loss: [6.70556755]\n",
      "Epoch: 489 Loss: [6.71996397]\n",
      "Epoch: 490 Loss: [6.70173632]\n",
      "Epoch: 491 Loss: [6.70759344]\n",
      "Epoch: 492 Loss: [6.70123037]\n",
      "Epoch: 493 Loss: [6.69886837]\n",
      "Epoch: 494 Loss: [6.69542574]\n",
      "Epoch: 495 Loss: [6.71554648]\n",
      "Epoch: 496 Loss: [6.69101665]\n",
      "Epoch: 497 Loss: [6.69820084]\n",
      "Epoch: 498 Loss: [6.69165623]\n",
      "Epoch: 499 Loss: [6.68955241]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 10\n",
    "delta_const = 1e-8\n",
    "num_batches = num_samples/batch_size\n",
    "eps = 1e-3\n",
    "rho_1 = 0.7\n",
    "t=1\n",
    "rho_2 = 0.9\n",
    "loss1 = []\n",
    "r1 = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "r2 = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        for k in range(batch_size):\n",
    "            z1 = relu(np.matmul(W1,x_train_subs[j*batch_size+k]).reshape(-1,1)+b1)\n",
    "        \n",
    "            z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "\n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j*batch_size+k])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j*batch_size+k].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(z2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(z1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j*batch_size+k].reshape(-1,1).T)\n",
    "        \n",
    "        r1[\"W1\"] = (rho_1)*r1[\"W1\"] + (1-rho_1)*W1_upd\n",
    "        r1[\"W2\"] = (rho_1)*r1[\"W2\"] + (1-rho_1)*W2_upd\n",
    "        r1[\"W3\"] = (rho_1)*r1[\"W3\"] + (1-rho_1)*W3_upd\n",
    "        r1[\"b1\"] = (rho_1)*r1[\"b1\"] + (1-rho_1)*b1_upd\n",
    "        r1[\"b2\"] = (rho_1)*r1[\"b2\"] + (1-rho_1)*b2_upd\n",
    "        r1[\"b3\"] = (rho_1)*r1[\"b3\"] + (1-rho_1)*b3_upd\n",
    "        \n",
    "        r1[\"W1\"] = r1[\"W1\"]/(1-(rho_1)**(t+1))\n",
    "        r1[\"W2\"] = r1[\"W2\"]/(1-(rho_1)**(t+1))\n",
    "        r1[\"W3\"] = r1[\"W3\"]/(1-(rho_1)**(t+1))\n",
    "        r1[\"b1\"] = r1[\"b1\"]/(1-(rho_1)**(t+1))\n",
    "        r1[\"b2\"] = r1[\"b2\"]/(1-(rho_1)**(t+1))\n",
    "        r1[\"b3\"] = r1[\"b3\"]/(1-(rho_1)**(t+1))\n",
    "        \n",
    "        \n",
    "        r2[\"W1\"] = (rho_2)*r2[\"W1\"] + (1-rho_2)*W1_upd*W1_upd\n",
    "        r2[\"W2\"] = (rho_2)*r2[\"W2\"] + (1-rho_2)*W2_upd*W2_upd\n",
    "        r2[\"W3\"] = (rho_2)*r2[\"W3\"] + (1-rho_2)*W3_upd*W3_upd\n",
    "        r2[\"b1\"] = (rho_2)*r2[\"b1\"] + (1-rho_2)*b1_upd*b1_upd\n",
    "        r2[\"b2\"] = (rho_2)*r2[\"b2\"] + (1-rho_2)*b2_upd*b2_upd\n",
    "        r2[\"b3\"] = (rho_2)*r2[\"b3\"] + (1-rho_2)*b3_upd*b3_upd\n",
    "        \n",
    "        r2[\"W1\"] = r2[\"W1\"]/(1-(rho_2)**(t+1))\n",
    "        r2[\"W2\"] = r2[\"W2\"]/(1-(rho_2)**(t+1))\n",
    "        r2[\"W3\"] = r2[\"W3\"]/(1-(rho_2)**(t+1))\n",
    "        r2[\"b1\"] = r2[\"b1\"]/(1-(rho_2)**(t+1))\n",
    "        r2[\"b2\"] = r2[\"b2\"]/(1-(rho_2)**(t+1))\n",
    "        r2[\"b3\"] = r2[\"b3\"]/(1-(rho_2)**(t+1))\n",
    "        t = t+1\n",
    "        \n",
    "        \n",
    "        \n",
    "        W3 = W3 - (eps*r1[\"W3\"])/np.sqrt(delta_const + r2[\"W3\"])\n",
    "        W2 = W2 - (eps*r1[\"W2\"])/np.sqrt(delta_const + r2[\"W2\"])\n",
    "        W1 = W1 - (eps*r1[\"W1\"])/np.sqrt(delta_const + r2[\"W1\"])\n",
    "        b3 = b3 - (eps*r1[\"b3\"])/np.sqrt(delta_const + r2[\"b3\"])\n",
    "        b2 = b2 - (eps*r1[\"b2\"])/np.sqrt(delta_const + r2[\"b2\"])\n",
    "        b1 = b1 - (eps*r1[\"b1\"])/np.sqrt(delta_const + r2[\"b1\"])\n",
    "    loss1.append(loss)\n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc =accuracy_score(y_pred=preds,y_true=true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,501)\n",
    "plt.plot(epochs,loss1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.2693772815275,251.39774303036808,249.77485871094632,247.4199949859063,242.84802729811554,233.80911715977933,217.3900801830696,196.6171327393254,173.52667157172118,151.7417319780494,130.94070252143874,112.17851765164089,95.09389362493492,81.09658334257901,71.04130103399002,64.06433739712307,58.930280247511696,55.0509546632815,52.132224009893875,49.71648704276075,47.61215498928812,45.61933838496957,43.80851720990527,42.26481951558981,40.78296188939124,39.115735998755646,37.5647812270534,36.197610004627506,34.94455687680153,33.75372876332871,32.62417801264,31.572424765492222,30.551360337364187,29.592700390905755,28.66853196570917,27.782936594811076,26.940648158940782,26.13199778450615,25.356357624170645,24.61350860169771,23.900766427381598,23.21668998734781,22.564279446832753,21.93743578106423,21.338518099523803,20.765983974468806,20.216645871524484,19.69237884308602,19.18788666825411,18.67146840122971,18.10693784510336,17.464543311521055,16.71257775587919,15.967903605913454,15.399627765608688,14.864186528102552,14.37384352496934,14.008790194933669,13.661097914387332,13.345041054506128,13.067379381291875,12.829562144348577,12.616539336711188,12.42207334824843,12.2480192335996,12.084520276338075,11.94098892005844,11.799862210233531,11.670103689877033,11.549470274032807,11.430136259569124,11.32493546928265,11.221354464663419,11.123964716327897,11.030307382174186,10.94131268842564,10.858953912919803,10.776038417532417,10.697794029007303,10.625443752780862,10.553542527364854,10.484532354141624,10.417905987212048,10.35386885680244,10.290407629882152,10.228011986576925,10.170154477186298,10.113553748794187,10.060145713432611,10.00666653388056,9.957786648042806,9.907698366247724,9.859706810721525,9.812866142685378,9.766719933768044,9.723531872179613,9.679624535861386,9.638729810983763,9.596798791617587,9.556206229934151,9.518043434664113,9.477707072334631,9.439813732906922,9.402441636158377,9.366643825524962,9.33020743137823,9.295036438293975,9.260587360759846,9.226751253656332,9.193230870352306,9.160457991990958,9.128380344254861,9.083255861567919,9.054758620336251,9.026017919654189,8.997704253825953,8.970092035590193,8.94310623542161,8.916236410467295,8.890657749001896,8.866962517505637,8.838855657535863,8.817025156292642,8.791890591659232,8.769619308375074,8.744482172518614,8.722501172830547,8.67844945015931,8.68282688592905,8.656495322255052,8.635903079685278,8.614469011347529,8.576942162821533,8.579548189065566,8.55465489111497,8.535356122199873,8.509029592090428,8.484673024268409,8.485024931064174,8.460443039797498,8.42854947207128,8.432616969827285,8.407500047881932,8.38621154972861,8.371551606507095,8.35483503145375,8.339007713682046,8.323281599423979,8.307825061685126,8.292588438373484,8.277176960034053,8.262776123669362,8.248135084425053,8.233772622703851,8.21959368452985,8.205993438429848,8.191448238612821,8.178273274570623,8.164780114047772,8.15155199786072,8.138478055860425,8.125578689501765,8.11284471271909,8.100271467266202,8.088472025231297,8.075526604009074,8.06266027427961,8.052123505056446,8.039576888323063,8.028027227776716,8.0164674052383,8.005081098794415,7.993820174905023,7.982698778504492,7.96999235911312,7.959506692904636,7.94859863420192,7.93797122647218,7.927244452525444,7.916987577952927,7.906681242535864,7.895980486360163,7.886904226743664,7.876450211113072,7.866643425393422,7.856985361571162,7.846787442414517,7.838435765380511,7.828395171710999,7.819131263682512,7.8099616854535725,7.800273071764645,7.7923738818311055,7.782818288594465,7.774017522016347,7.765297539269669,7.7565849936011855,7.747530520818688,7.740031380026773,7.731039442424876,7.722812765069486,7.714460162996318,7.706340161595713,7.697735703073709,7.690664044873423,7.68215852551278,7.674384459305576,7.6665286924291225,7.658178337008649,7.6515947803401225,7.643391776603146,7.63594553236725,7.628447059879806,7.614237436563493,7.61548388461234,7.605755455575429,7.598822804907982,7.591645701539047,7.578116255142955,7.579331838190326,7.570046306809163,7.563426793033049,7.550744762590699,7.551041671522177,7.542181476009669,7.530853015691131,7.530688824722742,7.522586914911566,7.516258038110561,7.504459928271509,7.504408882391265,7.496961920210347,7.485342937401931,7.485304785237035,7.47248380355199,7.473403848299823,7.464894071448971,7.4544708493232585,7.455034515990545,7.441655109989236,7.443144624335904,7.434845505853109,7.424986957706615,7.425399126263281,7.412641661795599,7.41402790034177,7.401179188884375,7.402667780952225,7.394738537286212,7.38551351847919,7.385805325678436,7.373847599291348,7.375038696749053,7.362951119772975,7.364265094316708,7.352296024730939,7.353640875686,7.341823625630189,7.343173922433559,7.331521923755944,7.332861331833386,7.3213816570165555,7.322698669352098,7.311410003572616,7.312626943527363,7.301571810578285,7.302805282887647,7.291867106323565,7.293076959983943,7.282327426248709,7.283426701778361,7.273145290465778,7.273756286847456,7.2638633422274985,7.264618130883758,7.254497857341318,7.255450476628556,7.245440816424936,7.242473770250555,7.241287455728584,7.231828403119108,7.232883632101115,7.223041577535567,7.224157240766255,7.2144565168325805,7.215515895853819,7.205983441701921,7.2069852732616315,7.197614279277894,7.1985655556374315,7.189347307241941,7.190254919298181,7.181181437036544,7.182051670081369,7.173115147243807,7.1704863275197965,7.169383116380823,7.160954232929486,7.16188527484711,7.153089708102055,7.154087816451827,7.145400244257522,7.14635976872723,7.137804460889744,7.138725278225795,7.130296309160677,7.131186124460675,7.122875328958114,7.123740021791576,7.115539379107501,7.116384750265683,7.108287617956774,7.109119350185113,7.101119452811994,7.101942726478781,7.094034708578913,7.094853554592719,7.087031756123979,7.0878500420289425,7.080109665132824,7.080930939367996,7.073267500574333,7.07409484119969,7.06650418480686,7.067340472880778,7.062648893850494,7.057213051462122,7.057375816507199,7.050242325240712,7.050885337285884,7.043681837789266,7.044377330133394,7.037225661103927,7.0350487851485415,7.029695911615844,7.024845878531153,7.020737974408551,7.016225696254034,7.029813964413549,7.010141296753408,7.01560042184427,7.0079998318874575,7.004022521344301,6.999780727352769,6.9954695474724495,7.0090807705256655,6.989739499726104,6.995077536510439,6.987692804493113,6.983835762218106,6.97967978709421,6.975501622829107,6.989287255316251,6.970097250543042,6.97535964633886,6.968161269164919,6.964428278047479,6.959984625864523,6.956351699221441,6.970523388676944,6.9510768604614,6.954086526175516,6.949610518018591,6.944797687476093,6.941208658275555,6.93724840439574,6.953572101552744,6.929224665184613,6.9372627149834205,6.928764387686056,6.926012490083773,6.922032011722244,6.938558609112909,6.914414185279047,6.922318798861829,6.913867266460804,6.9113480748765985,6.907550604298384,6.923713541996799,6.900283170625173,6.907691091313426,6.899785543800261,6.897120025557181,6.893415804201851,6.9095164207602595,6.886440808214599,6.8937124768673534,6.885985061499645,6.883374179630073,6.879417021370238,6.896096283836494,6.8727017926168985,6.8784854879470325,6.872742195171139,6.869254084101642,6.865959683325716,6.883781950100382,6.858945395780786,6.8651052211844235,6.859359785969909,6.856024491372552,6.871161442514262,6.849875351397179,6.855439855697008,6.849541516015782,6.846305546656684,6.842778597120936,6.860137453716455,6.836502142090581,6.84262985720192,6.836741041735619,6.83365466609382,6.848039661255277,6.828018210545736,6.833491224858622,6.825908486627851,6.824697183699109,6.820360110269798,6.838950394726431,6.81444208065194,6.8208992490838956,6.814956429935395,6.812088510172866,6.827157806029099,6.806652292010231,6.812355890175423,6.804923498034763,6.803673268930061,6.799583428730336,6.818455328784346,6.793871187597036,6.800510749769337,6.793003373360132,6.791947449653454,6.8080036431991955,6.7860071481598965,6.791908534453289,6.784766584059453,6.783578134401256,6.798224491607206,6.777881848880105,6.783797204933191,6.776580419410983,6.775265008613363,6.78881024631038,6.770081112058791,6.775526126433515,6.768599596299715,6.765776239540074,6.781251803635413,6.7613747625202345,6.767153666965576,6.76023946376434,6.757506498825615,6.773230019710494,6.753117960613067,6.758998557533713,6.752145785057007,6.749478687984055,6.765289744402631,6.745129422267446,6.751073100933784,6.744284715013619,6.741675462242302,6.757448907897711,6.737383140502641,6.743360036494929,6.7366552669474995,6.734044480541323,6.7495180179283825,6.730076884505397,6.735794307392315,6.729228752397047,6.726437668393483,6.742129118826625,6.72270783784945,6.7282809012709475,6.72199014646801,6.719404050716907,6.734585986470364,6.715610466550149,6.721144904272817,6.7149082389260375,6.712411100093732,6.72720784527133,6.708471301956035,6.714400410141608,6.707957143357622,6.705567551202443,6.719963971684494,6.70173632130348,6.707593435208817,6.701230373398015,6.698868365325919,6.6954257381970885,6.715546484136789,6.691016648731466,6.698200843262431,6.691656227200623,6.689552410449262,"
     ]
    }
   ],
   "source": [
    "for elem in np.array(loss1):\n",
    "    print(str(elem[0])+',' , end = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
