{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/havish/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [143.77332558]\n",
      "Epoch: 1 Loss: [82.72312054]\n",
      "Epoch: 2 Loss: [73.26902841]\n",
      "Epoch: 3 Loss: [57.10984573]\n",
      "Epoch: 4 Loss: [46.22196649]\n",
      "Epoch: 5 Loss: [37.10100866]\n",
      "Epoch: 6 Loss: [30.54482857]\n",
      "Epoch: 7 Loss: [26.46165909]\n",
      "Epoch: 8 Loss: [24.01965873]\n",
      "Epoch: 9 Loss: [22.82479292]\n",
      "Epoch: 10 Loss: [22.55725676]\n",
      "Epoch: 11 Loss: [22.46784211]\n",
      "Epoch: 12 Loss: [21.49422982]\n",
      "Epoch: 13 Loss: [19.52063465]\n",
      "Epoch: 14 Loss: [17.21425034]\n",
      "Epoch: 15 Loss: [14.9449309]\n",
      "Epoch: 16 Loss: [13.10970575]\n",
      "Epoch: 17 Loss: [11.6067648]\n",
      "Epoch: 18 Loss: [10.38507807]\n",
      "Epoch: 19 Loss: [9.40494989]\n",
      "Epoch: 20 Loss: [8.57371471]\n",
      "Epoch: 21 Loss: [7.87945725]\n",
      "Epoch: 22 Loss: [7.32659078]\n",
      "Epoch: 23 Loss: [6.8751372]\n",
      "Epoch: 24 Loss: [6.50888054]\n",
      "Epoch: 25 Loss: [6.20930976]\n",
      "Epoch: 26 Loss: [5.96331445]\n",
      "Epoch: 27 Loss: [5.76114436]\n",
      "Epoch: 28 Loss: [5.59995103]\n",
      "Epoch: 29 Loss: [5.46956528]\n",
      "Epoch: 30 Loss: [5.37011512]\n",
      "Epoch: 31 Loss: [5.28491912]\n",
      "Epoch: 32 Loss: [5.24631648]\n",
      "Epoch: 33 Loss: [5.18944422]\n",
      "Epoch: 34 Loss: [5.20326696]\n",
      "Epoch: 35 Loss: [5.15892466]\n",
      "Epoch: 36 Loss: [5.22655017]\n",
      "Epoch: 37 Loss: [5.17690985]\n",
      "Epoch: 38 Loss: [5.36795007]\n",
      "Epoch: 39 Loss: [5.1738925]\n",
      "Epoch: 40 Loss: [5.74084143]\n",
      "Epoch: 41 Loss: [4.87341966]\n",
      "Epoch: 42 Loss: [7.28772105]\n",
      "Epoch: 43 Loss: [3.37001879]\n",
      "Epoch: 44 Loss: [12.45152454]\n",
      "Epoch: 45 Loss: [3.1884324]\n",
      "Epoch: 46 Loss: [10.1976732]\n",
      "Epoch: 47 Loss: [2.75116659]\n",
      "Epoch: 48 Loss: [9.02484359]\n",
      "Epoch: 49 Loss: [3.24323787]\n",
      "Epoch: 50 Loss: [12.04933716]\n",
      "Epoch: 51 Loss: [2.95749523]\n",
      "Epoch: 52 Loss: [10.26369158]\n",
      "Epoch: 53 Loss: [3.29777486]\n",
      "Epoch: 54 Loss: [11.1036543]\n",
      "Epoch: 55 Loss: [2.64481269]\n",
      "Epoch: 56 Loss: [8.31765403]\n",
      "Epoch: 57 Loss: [5.97721307]\n",
      "Epoch: 58 Loss: [9.28861124]\n",
      "Epoch: 59 Loss: [3.79238641]\n",
      "Epoch: 60 Loss: [11.48286513]\n",
      "Epoch: 61 Loss: [3.46661269]\n",
      "Epoch: 62 Loss: [9.99297657]\n",
      "Epoch: 63 Loss: [6.38035349]\n",
      "Epoch: 64 Loss: [10.04947416]\n",
      "Epoch: 65 Loss: [5.59410772]\n",
      "Epoch: 66 Loss: [12.64022208]\n",
      "Epoch: 67 Loss: [3.66400023]\n",
      "Epoch: 68 Loss: [10.21231561]\n",
      "Epoch: 69 Loss: [8.63964114]\n",
      "Epoch: 70 Loss: [9.16315667]\n",
      "Epoch: 71 Loss: [8.8973789]\n",
      "Epoch: 72 Loss: [10.1429585]\n",
      "Epoch: 73 Loss: [9.5289589]\n",
      "Epoch: 74 Loss: [10.78818984]\n",
      "Epoch: 75 Loss: [9.45889479]\n",
      "Epoch: 76 Loss: [12.12908502]\n",
      "Epoch: 77 Loss: [11.07066817]\n",
      "Epoch: 78 Loss: [11.91185658]\n",
      "Epoch: 79 Loss: [13.52743964]\n",
      "Epoch: 80 Loss: [13.19839356]\n",
      "Epoch: 81 Loss: [14.2316766]\n",
      "Epoch: 82 Loss: [14.28651858]\n",
      "Epoch: 83 Loss: [14.74493888]\n",
      "Epoch: 84 Loss: [14.99134504]\n",
      "Epoch: 85 Loss: [15.41199047]\n",
      "Epoch: 86 Loss: [15.79676131]\n",
      "Epoch: 87 Loss: [15.55115927]\n",
      "Epoch: 88 Loss: [17.30483167]\n",
      "Epoch: 89 Loss: [17.29144706]\n",
      "Epoch: 90 Loss: [16.89966509]\n",
      "Epoch: 91 Loss: [16.46164393]\n",
      "Epoch: 92 Loss: [13.21924961]\n",
      "Epoch: 93 Loss: [11.49403987]\n",
      "Epoch: 94 Loss: [10.21734763]\n",
      "Epoch: 95 Loss: [9.76691365]\n",
      "Epoch: 96 Loss: [8.87215806]\n",
      "Epoch: 97 Loss: [8.90510529]\n",
      "Epoch: 98 Loss: [8.82900413]\n",
      "Epoch: 99 Loss: [9.20852432]\n",
      "Epoch: 100 Loss: [8.63126232]\n",
      "Epoch: 101 Loss: [9.24726461]\n",
      "Epoch: 102 Loss: [8.23674965]\n",
      "Epoch: 103 Loss: [8.45278129]\n",
      "Epoch: 104 Loss: [8.17435888]\n",
      "Epoch: 105 Loss: [8.32666801]\n",
      "Epoch: 106 Loss: [8.22334913]\n",
      "Epoch: 107 Loss: [8.51926066]\n",
      "Epoch: 108 Loss: [8.47865773]\n",
      "Epoch: 109 Loss: [8.21048438]\n",
      "Epoch: 110 Loss: [8.62697982]\n",
      "Epoch: 111 Loss: [8.49083389]\n",
      "Epoch: 112 Loss: [8.17547315]\n",
      "Epoch: 113 Loss: [8.60644078]\n",
      "Epoch: 114 Loss: [8.5496631]\n",
      "Epoch: 115 Loss: [8.18203115]\n",
      "Epoch: 116 Loss: [8.34628985]\n",
      "Epoch: 117 Loss: [8.16672159]\n",
      "Epoch: 118 Loss: [8.30167123]\n",
      "Epoch: 119 Loss: [8.25464533]\n",
      "Epoch: 120 Loss: [8.28926509]\n",
      "Epoch: 121 Loss: [8.34582717]\n",
      "Epoch: 122 Loss: [8.30004858]\n",
      "Epoch: 123 Loss: [8.33514768]\n",
      "Epoch: 124 Loss: [8.36494875]\n",
      "Epoch: 125 Loss: [8.39974312]\n",
      "Epoch: 126 Loss: [8.66606803]\n",
      "Epoch: 127 Loss: [8.64234638]\n",
      "Epoch: 128 Loss: [8.21667433]\n",
      "Epoch: 129 Loss: [8.36626214]\n",
      "Epoch: 130 Loss: [8.36507012]\n",
      "Epoch: 131 Loss: [8.41674]\n",
      "Epoch: 132 Loss: [8.43565438]\n",
      "Epoch: 133 Loss: [8.44457128]\n",
      "Epoch: 134 Loss: [8.53608632]\n",
      "Epoch: 135 Loss: [8.98439601]\n",
      "Epoch: 136 Loss: [8.73335284]\n",
      "Epoch: 137 Loss: [8.94791122]\n",
      "Epoch: 138 Loss: [8.84090459]\n",
      "Epoch: 139 Loss: [9.27171849]\n",
      "Epoch: 140 Loss: [9.04172962]\n",
      "Epoch: 141 Loss: [9.47759321]\n",
      "Epoch: 142 Loss: [9.24165757]\n",
      "Epoch: 143 Loss: [11.0700436]\n",
      "Epoch: 144 Loss: [12.35151895]\n",
      "Epoch: 145 Loss: [9.90750274]\n",
      "Epoch: 146 Loss: [9.74768909]\n",
      "Epoch: 147 Loss: [10.12873861]\n",
      "Epoch: 148 Loss: [10.32308285]\n",
      "Epoch: 149 Loss: [10.5254587]\n",
      "Epoch: 150 Loss: [10.81087825]\n",
      "Epoch: 151 Loss: [11.07675404]\n",
      "Epoch: 152 Loss: [11.32985953]\n",
      "Epoch: 153 Loss: [11.3959847]\n",
      "Epoch: 154 Loss: [11.40790144]\n",
      "Epoch: 155 Loss: [11.41399093]\n",
      "Epoch: 156 Loss: [11.31991151]\n",
      "Epoch: 157 Loss: [11.18269785]\n",
      "Epoch: 158 Loss: [11.07571176]\n",
      "Epoch: 159 Loss: [10.90235944]\n",
      "Epoch: 160 Loss: [10.67495815]\n",
      "Epoch: 161 Loss: [10.39444437]\n",
      "Epoch: 162 Loss: [10.10805906]\n",
      "Epoch: 163 Loss: [9.91784841]\n",
      "Epoch: 164 Loss: [9.46529787]\n",
      "Epoch: 165 Loss: [9.14019031]\n",
      "Epoch: 166 Loss: [8.80577278]\n",
      "Epoch: 167 Loss: [8.42226712]\n",
      "Epoch: 168 Loss: [8.0046753]\n",
      "Epoch: 169 Loss: [7.31076439]\n",
      "Epoch: 170 Loss: [6.76141723]\n",
      "Epoch: 171 Loss: [6.2785555]\n",
      "Epoch: 172 Loss: [5.77825542]\n",
      "Epoch: 173 Loss: [5.24860287]\n",
      "Epoch: 174 Loss: [4.68035651]\n",
      "Epoch: 175 Loss: [4.06619095]\n",
      "Epoch: 176 Loss: [3.42064241]\n",
      "Epoch: 177 Loss: [2.79195299]\n",
      "Epoch: 178 Loss: [2.37269443]\n",
      "Epoch: 179 Loss: [2.05148995]\n",
      "Epoch: 180 Loss: [1.76296119]\n",
      "Epoch: 181 Loss: [1.48104482]\n",
      "Epoch: 182 Loss: [1.29262397]\n",
      "Epoch: 183 Loss: [1.23679119]\n",
      "Epoch: 184 Loss: [1.20879239]\n",
      "Epoch: 185 Loss: [1.1897996]\n",
      "Epoch: 186 Loss: [1.16101071]\n",
      "Epoch: 187 Loss: [1.13100095]\n",
      "Epoch: 188 Loss: [1.11808789]\n",
      "Epoch: 189 Loss: [1.09235526]\n",
      "Epoch: 190 Loss: [1.07302222]\n",
      "Epoch: 191 Loss: [1.04937514]\n",
      "Epoch: 192 Loss: [1.02257187]\n",
      "Epoch: 193 Loss: [1.00379978]\n",
      "Epoch: 194 Loss: [0.99289501]\n",
      "Epoch: 195 Loss: [0.97077924]\n",
      "Epoch: 196 Loss: [0.9461279]\n",
      "Epoch: 197 Loss: [0.93732693]\n",
      "Epoch: 198 Loss: [0.9171774]\n",
      "Epoch: 199 Loss: [0.8939026]\n",
      "Epoch: 200 Loss: [0.87926811]\n",
      "Epoch: 201 Loss: [0.87013424]\n",
      "Epoch: 202 Loss: [0.85156341]\n",
      "Epoch: 203 Loss: [0.83038196]\n",
      "Epoch: 204 Loss: [0.8177544]\n",
      "Epoch: 205 Loss: [0.80947681]\n",
      "Epoch: 206 Loss: [0.79269242]\n",
      "Epoch: 207 Loss: [0.7736691]\n",
      "Epoch: 208 Loss: [0.76269956]\n",
      "Epoch: 209 Loss: [0.75504825]\n",
      "Epoch: 210 Loss: [0.73978014]\n",
      "Epoch: 211 Loss: [0.72234098]\n",
      "Epoch: 212 Loss: [0.71292439]\n",
      "Epoch: 213 Loss: [0.70595142]\n",
      "Epoch: 214 Loss: [0.69203671]\n",
      "Epoch: 215 Loss: [0.67634459]\n",
      "Epoch: 216 Loss: [0.66812466]\n",
      "Epoch: 217 Loss: [0.65649678]\n",
      "Epoch: 218 Loss: [0.65175164]\n",
      "Epoch: 219 Loss: [0.63862851]\n",
      "Epoch: 220 Loss: [0.6250971]\n",
      "Epoch: 221 Loss: [0.61801959]\n",
      "Epoch: 222 Loss: [0.60742661]\n",
      "Epoch: 223 Loss: [0.6037089]\n",
      "Epoch: 224 Loss: [0.59147558]\n",
      "Epoch: 225 Loss: [0.57986057]\n",
      "Epoch: 226 Loss: [0.57363423]\n",
      "Epoch: 227 Loss: [0.56403475]\n",
      "Epoch: 228 Loss: [0.56117027]\n",
      "Epoch: 229 Loss: [0.54966949]\n",
      "Epoch: 230 Loss: [0.53983611]\n",
      "Epoch: 231 Loss: [0.53416708]\n",
      "Epoch: 232 Loss: [0.52556683]\n",
      "Epoch: 233 Loss: [0.51923486]\n",
      "Epoch: 234 Loss: [0.51570317]\n",
      "Epoch: 235 Loss: [0.50596431]\n",
      "Epoch: 236 Loss: [0.4973627]\n",
      "Epoch: 237 Loss: [0.49256135]\n",
      "Epoch: 238 Loss: [0.484806]\n",
      "Epoch: 239 Loss: [0.4795647]\n",
      "Epoch: 240 Loss: [0.47639129]\n",
      "Epoch: 241 Loss: [0.46763542]\n",
      "Epoch: 242 Loss: [0.46037516]\n",
      "Epoch: 243 Loss: [0.45604045]\n",
      "Epoch: 244 Loss: [0.4492376]\n",
      "Epoch: 245 Loss: [0.44469757]\n",
      "Epoch: 246 Loss: [0.44198564]\n",
      "Epoch: 247 Loss: [0.43394906]\n",
      "Epoch: 248 Loss: [0.42793675]\n",
      "Epoch: 249 Loss: [0.42384194]\n",
      "Epoch: 250 Loss: [0.41800068]\n",
      "Epoch: 251 Loss: [0.41391021]\n",
      "Epoch: 252 Loss: [0.41169629]\n",
      "Epoch: 253 Loss: [0.40418386]\n",
      "Epoch: 254 Loss: [0.39933061]\n",
      "Epoch: 255 Loss: [0.39529328]\n",
      "Epoch: 256 Loss: [0.39044834]\n",
      "Epoch: 257 Loss: [0.38653404]\n",
      "Epoch: 258 Loss: [0.38191367]\n",
      "Epoch: 259 Loss: [0.38112303]\n",
      "Epoch: 260 Loss: [0.37322996]\n",
      "Epoch: 261 Loss: [0.37042743]\n",
      "Epoch: 262 Loss: [0.36549919]\n",
      "Epoch: 263 Loss: [0.36261271]\n",
      "Epoch: 264 Loss: [0.3578666]\n",
      "Epoch: 265 Loss: [0.3579246]\n",
      "Epoch: 266 Loss: [0.34985982]\n",
      "Epoch: 267 Loss: [0.34848218]\n",
      "Epoch: 268 Loss: [0.34286731]\n",
      "Epoch: 269 Loss: [0.34154145]\n",
      "Epoch: 270 Loss: [0.33595448]\n",
      "Epoch: 271 Loss: [0.33752285]\n",
      "Epoch: 272 Loss: [0.32853481]\n",
      "Epoch: 273 Loss: [0.32920082]\n",
      "Epoch: 274 Loss: [0.32209589]\n",
      "Epoch: 275 Loss: [0.32312542]\n",
      "Epoch: 276 Loss: [0.31569107]\n",
      "Epoch: 277 Loss: [0.31980669]\n",
      "Epoch: 278 Loss: [0.30865294]\n",
      "Epoch: 279 Loss: [0.31267553]\n",
      "Epoch: 280 Loss: [0.30255886]\n",
      "Epoch: 281 Loss: [0.30752084]\n",
      "Epoch: 282 Loss: [0.29644048]\n",
      "Epoch: 283 Loss: [0.30502713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284 Loss: [0.28960237]\n",
      "Epoch: 285 Loss: [0.29927638]\n",
      "Epoch: 286 Loss: [0.28344703]\n",
      "Epoch: 287 Loss: [0.29553423]\n",
      "Epoch: 288 Loss: [0.27716114]\n",
      "Epoch: 289 Loss: [0.29442568]\n",
      "Epoch: 290 Loss: [0.27013455]\n",
      "Epoch: 291 Loss: [0.29081255]\n",
      "Epoch: 292 Loss: [0.26354373]\n",
      "Epoch: 293 Loss: [0.2914963]\n",
      "Epoch: 294 Loss: [0.25615961]\n",
      "Epoch: 295 Loss: [0.29234231]\n",
      "Epoch: 296 Loss: [0.24869279]\n",
      "Epoch: 297 Loss: [0.2934371]\n",
      "Epoch: 298 Loss: [0.24149959]\n",
      "Epoch: 299 Loss: [0.29983846]\n",
      "Epoch: 300 Loss: [0.23415698]\n",
      "Epoch: 301 Loss: [0.30851423]\n",
      "Epoch: 302 Loss: [0.2274953]\n",
      "Epoch: 303 Loss: [0.32166855]\n",
      "Epoch: 304 Loss: [0.22195163]\n",
      "Epoch: 305 Loss: [0.33988758]\n",
      "Epoch: 306 Loss: [0.21796579]\n",
      "Epoch: 307 Loss: [0.36288238]\n",
      "Epoch: 308 Loss: [0.21556356]\n",
      "Epoch: 309 Loss: [0.38798638]\n",
      "Epoch: 310 Loss: [0.21399756]\n",
      "Epoch: 311 Loss: [0.40962156]\n",
      "Epoch: 312 Loss: [0.21233007]\n",
      "Epoch: 313 Loss: [0.42291583]\n",
      "Epoch: 314 Loss: [0.21028835]\n",
      "Epoch: 315 Loss: [0.42843361]\n",
      "Epoch: 316 Loss: [0.20812474]\n",
      "Epoch: 317 Loss: [0.43007817]\n",
      "Epoch: 318 Loss: [0.20601911]\n",
      "Epoch: 319 Loss: [0.43048451]\n",
      "Epoch: 320 Loss: [0.20401168]\n",
      "Epoch: 321 Loss: [0.43055004]\n",
      "Epoch: 322 Loss: [0.20210297]\n",
      "Epoch: 323 Loss: [0.43049352]\n",
      "Epoch: 324 Loss: [0.20028807]\n",
      "Epoch: 325 Loss: [0.43035889]\n",
      "Epoch: 326 Loss: [0.19856332]\n",
      "Epoch: 327 Loss: [0.43016663]\n",
      "Epoch: 328 Loss: [0.19692472]\n",
      "Epoch: 329 Loss: [0.42991698]\n",
      "Epoch: 330 Loss: [0.19536784]\n",
      "Epoch: 331 Loss: [0.42961554]\n",
      "Epoch: 332 Loss: [0.19388996]\n",
      "Epoch: 333 Loss: [0.42927618]\n",
      "Epoch: 334 Loss: [0.19248773]\n",
      "Epoch: 335 Loss: [0.42889585]\n",
      "Epoch: 336 Loss: [0.19115743]\n",
      "Epoch: 337 Loss: [0.42847907]\n",
      "Epoch: 338 Loss: [0.18989685]\n",
      "Epoch: 339 Loss: [0.42803863]\n",
      "Epoch: 340 Loss: [0.18870257]\n",
      "Epoch: 341 Loss: [0.42756674]\n",
      "Epoch: 342 Loss: [0.18757269]\n",
      "Epoch: 343 Loss: [0.42707884]\n",
      "Epoch: 344 Loss: [0.1865043]\n",
      "Epoch: 345 Loss: [0.42656859]\n",
      "Epoch: 346 Loss: [0.18549465]\n",
      "Epoch: 347 Loss: [0.42604023]\n",
      "Epoch: 348 Loss: [0.18456073]\n",
      "Epoch: 349 Loss: [0.42550981]\n",
      "Epoch: 350 Loss: [0.183662]\n",
      "Epoch: 351 Loss: [0.42496046]\n",
      "Epoch: 352 Loss: [0.18281552]\n",
      "Epoch: 353 Loss: [0.42440681]\n",
      "Epoch: 354 Loss: [0.18204393]\n",
      "Epoch: 355 Loss: [0.42390291]\n",
      "Epoch: 356 Loss: [0.18129147]\n",
      "Epoch: 357 Loss: [0.42332594]\n",
      "Epoch: 358 Loss: [0.18059017]\n",
      "Epoch: 359 Loss: [0.42275732]\n",
      "Epoch: 360 Loss: [0.17993231]\n",
      "Epoch: 361 Loss: [0.42218911]\n",
      "Epoch: 362 Loss: [0.17931779]\n",
      "Epoch: 363 Loss: [0.42162538]\n",
      "Epoch: 364 Loss: [0.17877499]\n",
      "Epoch: 365 Loss: [0.42117079]\n",
      "Epoch: 366 Loss: [0.17822946]\n",
      "Epoch: 367 Loss: [0.42057149]\n",
      "Epoch: 368 Loss: [0.17773675]\n",
      "Epoch: 369 Loss: [0.42003007]\n",
      "Epoch: 370 Loss: [0.17727448]\n",
      "Epoch: 371 Loss: [0.41947685]\n",
      "Epoch: 372 Loss: [0.17684968]\n",
      "Epoch: 373 Loss: [0.41894298]\n",
      "Epoch: 374 Loss: [0.17649384]\n",
      "Epoch: 375 Loss: [0.41857249]\n",
      "Epoch: 376 Loss: [0.1761476]\n",
      "Epoch: 377 Loss: [0.41816146]\n",
      "Epoch: 378 Loss: [0.17580043]\n",
      "Epoch: 379 Loss: [0.41758015]\n",
      "Epoch: 380 Loss: [0.17550817]\n",
      "Epoch: 381 Loss: [0.4171194]\n",
      "Epoch: 382 Loss: [0.17522862]\n",
      "Epoch: 383 Loss: [0.41659791]\n",
      "Epoch: 384 Loss: [0.17498637]\n",
      "Epoch: 385 Loss: [0.41613817]\n",
      "Epoch: 386 Loss: [0.17480529]\n",
      "Epoch: 387 Loss: [0.4158654]\n",
      "Epoch: 388 Loss: [0.17457655]\n",
      "Epoch: 389 Loss: [0.41527008]\n",
      "Epoch: 390 Loss: [0.17442754]\n",
      "Epoch: 391 Loss: [0.41493991]\n",
      "Epoch: 392 Loss: [0.174258]\n",
      "Epoch: 393 Loss: [0.41442291]\n",
      "Epoch: 394 Loss: [0.17414527]\n",
      "Epoch: 395 Loss: [0.41435582]\n",
      "Epoch: 396 Loss: [0.1738321]\n",
      "Epoch: 397 Loss: [0.41289692]\n",
      "Epoch: 398 Loss: [0.17400496]\n",
      "Epoch: 399 Loss: [0.41373034]\n",
      "Epoch: 400 Loss: [0.17370006]\n",
      "Epoch: 401 Loss: [0.41223434]\n",
      "Epoch: 402 Loss: [0.17388664]\n",
      "Epoch: 403 Loss: [0.41299027]\n",
      "Epoch: 404 Loss: [0.17367478]\n",
      "Epoch: 405 Loss: [0.41321059]\n",
      "Epoch: 406 Loss: [0.1734044]\n",
      "Epoch: 407 Loss: [0.41024777]\n",
      "Epoch: 408 Loss: [0.17417511]\n",
      "Epoch: 409 Loss: [0.41357433]\n",
      "Epoch: 410 Loss: [0.17334326]\n",
      "Epoch: 411 Loss: [0.40912289]\n",
      "Epoch: 412 Loss: [0.17436219]\n",
      "Epoch: 413 Loss: [0.41510625]\n",
      "Epoch: 414 Loss: [0.17268643]\n",
      "Epoch: 415 Loss: [0.40488874]\n",
      "Epoch: 416 Loss: [0.1753742]\n",
      "Epoch: 417 Loss: [0.41784386]\n",
      "Epoch: 418 Loss: [0.17197993]\n",
      "Epoch: 419 Loss: [0.4011695]\n",
      "Epoch: 420 Loss: [0.17628248]\n",
      "Epoch: 421 Loss: [0.42157272]\n",
      "Epoch: 422 Loss: [0.1707451]\n",
      "Epoch: 423 Loss: [0.39205795]\n",
      "Epoch: 424 Loss: [0.17873409]\n",
      "Epoch: 425 Loss: [0.43334843]\n",
      "Epoch: 426 Loss: [0.16659301]\n",
      "Epoch: 427 Loss: [0.3703193]\n",
      "Epoch: 428 Loss: [0.18319122]\n",
      "Epoch: 429 Loss: [0.45626065]\n",
      "Epoch: 430 Loss: [0.15726613]\n",
      "Epoch: 431 Loss: [0.32376946]\n",
      "Epoch: 432 Loss: [0.18483469]\n",
      "Epoch: 433 Loss: [0.45512272]\n",
      "Epoch: 434 Loss: [0.15721649]\n",
      "Epoch: 435 Loss: [0.32097761]\n",
      "Epoch: 436 Loss: [0.18568678]\n",
      "Epoch: 437 Loss: [0.45886676]\n",
      "Epoch: 438 Loss: [0.15498302]\n",
      "Epoch: 439 Loss: [0.3083842]\n",
      "Epoch: 440 Loss: [0.18480311]\n",
      "Epoch: 441 Loss: [0.44821143]\n",
      "Epoch: 442 Loss: [0.15924023]\n",
      "Epoch: 443 Loss: [0.32485587]\n",
      "Epoch: 444 Loss: [0.18866376]\n",
      "Epoch: 445 Loss: [0.47810445]\n",
      "Epoch: 446 Loss: [0.14453449]\n",
      "Epoch: 447 Loss: [0.26027328]\n",
      "Epoch: 448 Loss: [0.16986514]\n",
      "Epoch: 449 Loss: [0.34183949]\n",
      "Epoch: 450 Loss: [0.19113931]\n",
      "Epoch: 451 Loss: [0.49953653]\n",
      "Epoch: 452 Loss: [0.13249102]\n",
      "Epoch: 453 Loss: [0.21552093]\n",
      "Epoch: 454 Loss: [0.14410782]\n",
      "Epoch: 455 Loss: [0.21889704]\n",
      "Epoch: 456 Loss: [0.14647246]\n",
      "Epoch: 457 Loss: [0.22650048]\n",
      "Epoch: 458 Loss: [0.151949]\n",
      "Epoch: 459 Loss: [0.24697499]\n",
      "Epoch: 460 Loss: [0.16543446]\n",
      "Epoch: 461 Loss: [0.30927461]\n",
      "Epoch: 462 Loss: [0.19151571]\n",
      "Epoch: 463 Loss: [0.49275132]\n",
      "Epoch: 464 Loss: [0.13277085]\n",
      "Epoch: 465 Loss: [0.20945973]\n",
      "Epoch: 466 Loss: [0.14155395]\n",
      "Epoch: 467 Loss: [0.20314092]\n",
      "Epoch: 468 Loss: [0.13683387]\n",
      "Epoch: 469 Loss: [0.18720273]\n",
      "Epoch: 470 Loss: [0.12497318]\n",
      "Epoch: 471 Loss: [0.15422923]\n",
      "Epoch: 472 Loss: [0.10029477]\n",
      "Epoch: 473 Loss: [0.10754755]\n",
      "Epoch: 474 Loss: [0.07525829]\n",
      "Epoch: 475 Loss: [0.07937776]\n",
      "Epoch: 476 Loss: [0.07041535]\n",
      "Epoch: 477 Loss: [0.07271479]\n",
      "Epoch: 478 Loss: [0.0708537]\n",
      "Epoch: 479 Loss: [0.07128767]\n",
      "Epoch: 480 Loss: [0.07074775]\n",
      "Epoch: 481 Loss: [0.0706705]\n",
      "Epoch: 482 Loss: [0.07038996]\n",
      "Epoch: 483 Loss: [0.070204]\n",
      "Epoch: 484 Loss: [0.06997794]\n",
      "Epoch: 485 Loss: [0.06977173]\n",
      "Epoch: 486 Loss: [0.06955891]\n",
      "Epoch: 487 Loss: [0.06935096]\n",
      "Epoch: 488 Loss: [0.06914296]\n",
      "Epoch: 489 Loss: [0.06893681]\n",
      "Epoch: 490 Loss: [0.06873205]\n",
      "Epoch: 491 Loss: [0.06852844]\n",
      "Epoch: 492 Loss: [0.06832624]\n",
      "Epoch: 493 Loss: [0.06812529]\n",
      "Epoch: 494 Loss: [0.0679256]\n",
      "Epoch: 495 Loss: [0.06772713]\n",
      "Epoch: 496 Loss: [0.06752996]\n",
      "Epoch: 497 Loss: [0.06733392]\n",
      "Epoch: 498 Loss: [0.06713929]\n",
      "Epoch: 499 Loss: [0.0669457]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 10\n",
    "num_batches = num_samples/batch_size\n",
    "alpha = 0.8\n",
    "eps = 1e-3\n",
    "loss1 = []\n",
    "v = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        for k in range(batch_size):\n",
    "            z1 = relu(np.matmul(W1,x_train_subs[j+k]).reshape(-1,1)+b1)\n",
    "        \n",
    "            z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "\n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j+k])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j+k].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(z2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(z1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j+k].reshape(-1,1).T)\n",
    "        v[\"W1\"] = alpha*v[\"W1\"] - eps*(W1_upd)\n",
    "        v[\"W2\"] = alpha*v[\"W2\"] - eps*(W2_upd)\n",
    "        v[\"W3\"] = alpha*v[\"W3\"] - eps*(W3_upd)\n",
    "        v[\"b1\"] = alpha*v[\"b1\"] - eps*(b1_upd)\n",
    "        v[\"b2\"] = alpha*v[\"b2\"] - eps*(b2_upd)\n",
    "        v[\"b3\"] = alpha*v[\"b3\"] - eps*(b3_upd)\n",
    "        W3 = W3 + v[\"W3\"]\n",
    "        W2 = W2 + v[\"W2\"]\n",
    "        W1 = W1 + v[\"W1\"]\n",
    "        b3 = b3 + v[\"b3\"]\n",
    "        b2 = b2 + v[\"b2\"]\n",
    "        b1 = b1 + v[\"b1\"]\n",
    "    loss1.append(loss)\n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = relu(np.matmul(W1,test_x[4]).reshape(-1,1)+b1)\n",
    "z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.94828926e-09]\n",
      " [9.99984160e-01]\n",
      " [1.58374115e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "acc =accuracy_score(y_pred=preds,y_true=true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,501)\n",
    "plt.plot(epochs,loss1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.77332557768742,82.72312053868744,73.26902840807718,57.109845731942585,46.221966489403925,37.10100865572896,30.544828571085787,26.461659094295047,24.01965873352157,22.82479291944212,22.5572567614594,22.46784211274613,21.494229817473695,19.52063464819355,17.214250335435718,14.944930900138154,13.10970574707526,11.606764800277151,10.385078065701837,9.404949885766243,8.573714712233022,7.87945725236165,7.326590779368306,6.8751372021077755,6.508880544597188,6.209309760341711,5.963314448254944,5.761144358344384,5.599951027323506,5.469565284811404,5.370115119481163,5.284919118265292,5.246316479786586,5.189444220434339,5.203266955493111,5.1589246642046165,5.226550171212909,5.176909846528059,5.367950065910637,5.173892500605522,5.740841433458212,4.873419657373369,7.287721045503081,3.37001879046641,12.451524543679561,3.188432397386777,10.197673201211236,2.7511665885707366,9.024843588632875,3.243237866580489,12.049337157963231,2.957495231776355,10.263691580937083,3.297774864967023,11.103654296215009,2.644812688512344,8.31765403172102,5.977213073624061,9.288611241272504,3.792386413760266,11.48286513322951,3.466612686091982,9.99297657249564,6.380353487679045,10.049474158706872,5.594107723086717,12.64022208338814,3.6640002296444645,10.212315605559443,8.639641142128037,9.163156668747838,8.89737890289006,10.142958495072856,9.528958897314995,10.788189837313533,9.458894788974236,12.129085018029597,11.070668166715032,11.911856580912717,13.52743963556417,13.198393558856033,14.231676595879362,14.286518582186737,14.744938883904483,14.991345035074964,15.411990466790444,15.796761311437278,15.551159270069403,17.304831673159583,17.29144706266903,16.8996650856045,16.46164392857641,13.219249614178473,11.494039867306293,10.217347634426625,9.766913649208037,8.872158060861064,8.905105290023217,8.829004125977523,9.208524324877082,8.631262322636463,9.247264610267317,8.236749653160214,8.452781285300468,8.174358877228187,8.326668011033505,8.22334912882537,8.519260655726338,8.478657726442524,8.210484380175119,8.626979823144271,8.490833888887193,8.175473149658991,8.606440782079547,8.5496630995445,8.182031147892712,8.34628984963747,8.16672159001518,8.301671228643565,8.254645333594478,8.28926508799989,8.345827167499671,8.300048577063034,8.33514767890793,8.364948746926524,8.399743118042494,8.666068032403764,8.642346377612672,8.21667432826593,8.36626214438684,8.365070117498407,8.416739997019826,8.435654383545222,8.444571281264885,8.536086320711965,8.984396005436452,8.733352840242953,8.947911215393148,8.84090458598153,9.271718493398831,9.041729620278959,9.477593211812053,9.241657569672858,11.070043596045496,12.351518948500177,9.907502736127482,9.747689085533583,10.128738606091867,10.323082853835059,10.525458700761938,10.810878253294037,11.076754038307037,11.329859528604548,11.395984702304391,11.40790143979475,11.413990929632085,11.319911509060212,11.182697846674143,11.075711760468918,10.902359440831509,10.674958148053747,10.394444368011012,10.108059060030884,9.917848413368972,9.465297874832318,9.140190310223154,8.805772781501632,8.422267115463917,8.004675298384225,7.310764389069422,6.761417233123363,6.278555501302418,5.778255423795244,5.248602874666406,4.680356507235357,4.066190947879621,3.420642413485511,2.791952988327355,2.372694430393843,2.051489948604253,1.7629611870863715,1.4810448237043898,1.2926239741563914,1.2367911911905427,1.2087923944080916,1.1897996020554311,1.1610107056642849,1.1310009455294028,1.1180878852237914,1.0923552552150495,1.0730222157352076,1.0493751375298943,1.022571870000393,1.0037997848651496,0.9928950106522112,0.9707792382950421,0.9461278968473447,0.937326931837052,0.9171774020326493,0.8939026020768538,0.8792681120823778,0.8701342405734479,0.8515634088812861,0.8303819640067869,0.8177543952857723,0.8094768086677543,0.7926924189565733,0.7736691042971836,0.762699561871179,0.7550482539885792,0.7397801429964458,0.7223409781826864,0.7129243921290699,0.7059514214101419,0.6920367113508377,0.6763445855707383,0.6681246576141752,0.6564967797051074,0.65175163771823,0.6386285119601369,0.6250971017856795,0.618019588344102,0.6074266141059137,0.603708900078773,0.5914755779718806,0.579860574157076,0.5736342340140896,0.5640347504971989,0.5611702732110451,0.5496694892940694,0.539836106048766,0.5341670807321174,0.5255668300367582,0.5192348596188319,0.5157031670545679,0.5059643143790541,0.49736269774589226,0.4925613543660371,0.4848059980765323,0.4795646986004753,0.47639129432069655,0.46763542249183326,0.4603751621516972,0.45604044688832907,0.4492375983004551,0.4446975681376386,0.4419856399094257,0.4339490579111684,0.42793674587285335,0.4238419395845841,0.4180006775616739,0.41391021009668577,0.41169629412071024,0.4041838572824034,0.3993306149592486,0.39529328189711876,0.3904483413844398,0.3865340394792629,0.38191366851789577,0.38112303073705905,0.3732299637177733,0.37042743221919094,0.36549918800023795,0.36261270681527275,0.35786659690081135,0.35792460261191555,0.34985982074620153,0.3484821847728844,0.34286731112440944,0.3415414539630502,0.3359544785746147,0.3375228536499347,0.32853481419820674,0.32920081977702825,0.32209589301834746,0.32312542317923654,0.3156910713736505,0.31980668506193066,0.30865293770620966,0.3126755261656126,0.30255886456155207,0.30752084156230336,0.29644048408507145,0.3050271327991815,0.28960237220748164,0.29927637940154345,0.2834470254694299,0.2955342337087712,0.2771611420685434,0.2944256753214404,0.27013455094153754,0.2908125536069324,0.26354372644106383,0.2914963048099089,0.2561596146737575,0.29234230762362257,0.24869279372329964,0.2934370955508257,0.2414995945387607,0.29983845852363195,0.23415697630436896,0.3085142261905272,0.22749529776404273,0.32166855050180304,0.2219516266439341,0.339887582516384,0.21796578525103158,0.3628823810277274,0.2155635619929478,0.3879863787661776,0.21399755617812757,0.40962156325237703,0.21233007216694275,0.4229158313119572,0.21028834543104585,0.42843360899878413,0.2081247395483288,0.43007816923484615,0.20601910789254013,0.4304845099405393,0.2040116813272798,0.43055003942029474,0.20210296806634526,0.43049351528040286,0.20028806676752595,0.4303588891660524,0.19856332318823697,0.4301666295926347,0.196924720864619,0.42991697997597444,0.19536783550402892,0.42961554166780036,0.19388996028827832,0.42927617751801955,0.19248772715742424,0.42889584819903165,0.1911574335014953,0.4284790725742063,0.18989684901727713,0.4280386322022033,0.18870256898496326,0.42756673855589844,0.1875726858662213,0.4270788383951247,0.18650430419874064,0.426568590635405,0.18549465131914367,0.4260402255299013,0.1845607298609223,0.4255098109343528,0.18366199830251134,0.42496046342784355,0.1828155239645362,0.42440680810630554,0.18204393487189377,0.4239029119414477,0.18129147018543315,0.4233259404421599,0.18059016525140978,0.42275732017880135,0.17993231373206012,0.42218911032799905,0.17931779021509264,0.42162537824632196,0.1787749899953993,0.42117078566234295,0.17822945885729607,0.420571488818877,0.17773675422041396,0.42003007212132076,0.17727448229720896,0.41947684785735645,0.1768496806564319,0.41894297897017185,0.17649383869033022,0.4185724934497808,0.1761476033525486,0.4181614564942497,0.17580042708311444,0.4175801536940506,0.17550816588090357,0.41711940158189637,0.1752286158323903,0.41659791253024603,0.17498637255994978,0.4161381713375778,0.17480528780651408,0.41586539754999724,0.17457654786423749,0.4152700776295093,0.17442754171704003,0.4149399103781363,0.17425800362992,0.4144229099401991,0.1741452717590346,0.4143558168102477,0.1738320999025825,0.4128969241911363,0.174004964912238,0.4137303438493288,0.1737000568926906,0.4122343436438259,0.1738866395437844,0.4129902709481045,0.1736747753678261,0.41321058747848816,0.17340440269815072,0.41024776638119276,0.17417511042697045,0.41357432861691534,0.1733432610337153,0.40912288555728893,0.17436218664997644,0.41510624965364595,0.17268643402967038,0.404888741804767,0.17537420498919049,0.41784385772055777,0.17197993046780446,0.40116950082522373,0.17628248494505258,0.4215727218239974,0.1707450988136479,0.3920579504788422,0.17873409362134812,0.43334842727622735,0.16659301195605145,0.37031930414952746,0.18319121630567672,0.4562606548022722,0.15726613357447794,0.3237694615341285,0.1848346905813037,0.45512271950284067,0.15721649419863623,0.32097760983838186,0.18568678170180608,0.4588667603037584,0.15498301595137337,0.30838419899375535,0.18480311277938,0.4482114316120495,0.1592402299381997,0.3248558727839686,0.18866376111323926,0.4781044507849627,0.14453448641836666,0.2602732841577862,0.1698651404455953,0.3418394940963227,0.19113930556649622,0.4995365251199946,0.13249102200563578,0.21552093213595577,0.14410781884502125,0.21889703873414373,0.14647245515790255,0.2265004823316027,0.1519489988587502,0.24697498812518923,0.16543446242988274,0.3092746059228496,0.1915157094074479,0.4927513165387709,0.1327708534770759,0.20945972656854495,0.14155395263465498,0.20314091738349702,0.13683386807967715,0.18720272678591338,0.12497318385683986,0.1542292295591202,0.10029476506465365,0.10754754557992265,0.07525828842573336,0.07937776409851972,0.07041535274305659,0.07271478510932237,0.07085369768063404,0.07128767181923482,0.07074774601599736,0.07067049816085927,0.0703899637809479,0.0702039989329577,0.06997794349071854,0.06977172662849244,0.06955890644732318,0.06935096266596458,0.06914296146970095,0.0689368101043509,0.06873204994201947,0.06852843503890195,0.0683262437391609,0.06812529217918524,0.06792560044670022,0.06772712794681801,0.06752996311488142,0.06733391883164541,0.06713928925353388,0.06694569715928099,"
     ]
    }
   ],
   "source": [
    "for elem in np.array(loss1):\n",
    "    print(str(elem[0])+',' , end = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
