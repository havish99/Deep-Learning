{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/havish/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [196.3660917]\n",
      "Epoch: 1 Loss: [142.30286418]\n",
      "Epoch: 2 Loss: [117.70130564]\n",
      "Epoch: 3 Loss: [94.695696]\n",
      "Epoch: 4 Loss: [83.48210516]\n",
      "Epoch: 5 Loss: [77.08913646]\n",
      "Epoch: 6 Loss: [72.27089668]\n",
      "Epoch: 7 Loss: [67.8523419]\n",
      "Epoch: 8 Loss: [63.19929308]\n",
      "Epoch: 9 Loss: [59.24579921]\n",
      "Epoch: 10 Loss: [56.22063291]\n",
      "Epoch: 11 Loss: [53.58032009]\n",
      "Epoch: 12 Loss: [51.19732772]\n",
      "Epoch: 13 Loss: [48.87646835]\n",
      "Epoch: 14 Loss: [46.61476288]\n",
      "Epoch: 15 Loss: [44.64194163]\n",
      "Epoch: 16 Loss: [42.87166401]\n",
      "Epoch: 17 Loss: [41.23069018]\n",
      "Epoch: 18 Loss: [39.66943928]\n",
      "Epoch: 19 Loss: [38.21866288]\n",
      "Epoch: 20 Loss: [36.83647227]\n",
      "Epoch: 21 Loss: [35.52305233]\n",
      "Epoch: 22 Loss: [34.27130722]\n",
      "Epoch: 23 Loss: [33.09430567]\n",
      "Epoch: 24 Loss: [31.96875355]\n",
      "Epoch: 25 Loss: [30.85356552]\n",
      "Epoch: 26 Loss: [29.84263745]\n",
      "Epoch: 27 Loss: [28.85856915]\n",
      "Epoch: 28 Loss: [27.93310865]\n",
      "Epoch: 29 Loss: [27.03946167]\n",
      "Epoch: 30 Loss: [26.1923283]\n",
      "Epoch: 31 Loss: [25.37343731]\n",
      "Epoch: 32 Loss: [24.6006751]\n",
      "Epoch: 33 Loss: [23.85226816]\n",
      "Epoch: 34 Loss: [23.14464469]\n",
      "Epoch: 35 Loss: [22.49672418]\n",
      "Epoch: 36 Loss: [21.87104319]\n",
      "Epoch: 37 Loss: [21.28458372]\n",
      "Epoch: 38 Loss: [20.75077096]\n",
      "Epoch: 39 Loss: [20.2022271]\n",
      "Epoch: 40 Loss: [19.72329368]\n",
      "Epoch: 41 Loss: [19.25141088]\n",
      "Epoch: 42 Loss: [18.82760896]\n",
      "Epoch: 43 Loss: [18.40153472]\n",
      "Epoch: 44 Loss: [18.02169149]\n",
      "Epoch: 45 Loss: [17.6350596]\n",
      "Epoch: 46 Loss: [17.27967773]\n",
      "Epoch: 47 Loss: [16.94585271]\n",
      "Epoch: 48 Loss: [16.62937539]\n",
      "Epoch: 49 Loss: [16.32676669]\n",
      "Epoch: 50 Loss: [16.04356399]\n",
      "Epoch: 51 Loss: [15.77306252]\n",
      "Epoch: 52 Loss: [15.51635585]\n",
      "Epoch: 53 Loss: [15.26226242]\n",
      "Epoch: 54 Loss: [15.02877812]\n",
      "Epoch: 55 Loss: [14.81028292]\n",
      "Epoch: 56 Loss: [14.60497691]\n",
      "Epoch: 57 Loss: [14.41438181]\n",
      "Epoch: 58 Loss: [14.22841786]\n",
      "Epoch: 59 Loss: [14.0513724]\n",
      "Epoch: 60 Loss: [13.88282169]\n",
      "Epoch: 61 Loss: [13.72671753]\n",
      "Epoch: 62 Loss: [13.57317381]\n",
      "Epoch: 63 Loss: [13.42603885]\n",
      "Epoch: 64 Loss: [13.28627109]\n",
      "Epoch: 65 Loss: [13.15315632]\n",
      "Epoch: 66 Loss: [13.02743351]\n",
      "Epoch: 67 Loss: [12.9062273]\n",
      "Epoch: 68 Loss: [12.78817773]\n",
      "Epoch: 69 Loss: [12.67710945]\n",
      "Epoch: 70 Loss: [12.57138171]\n",
      "Epoch: 71 Loss: [12.47007105]\n",
      "Epoch: 72 Loss: [12.37366694]\n",
      "Epoch: 73 Loss: [12.28340477]\n",
      "Epoch: 74 Loss: [12.19528148]\n",
      "Epoch: 75 Loss: [12.1093426]\n",
      "Epoch: 76 Loss: [12.03059803]\n",
      "Epoch: 77 Loss: [11.95366613]\n",
      "Epoch: 78 Loss: [11.87959705]\n",
      "Epoch: 79 Loss: [11.81057232]\n",
      "Epoch: 80 Loss: [11.74460629]\n",
      "Epoch: 81 Loss: [11.68061399]\n",
      "Epoch: 82 Loss: [11.61890538]\n",
      "Epoch: 83 Loss: [11.55976401]\n",
      "Epoch: 84 Loss: [11.50233025]\n",
      "Epoch: 85 Loss: [11.44803515]\n",
      "Epoch: 86 Loss: [11.39599113]\n",
      "Epoch: 87 Loss: [11.34829357]\n",
      "Epoch: 88 Loss: [11.29123593]\n",
      "Epoch: 89 Loss: [11.24916801]\n",
      "Epoch: 90 Loss: [11.20450542]\n",
      "Epoch: 91 Loss: [11.16477524]\n",
      "Epoch: 92 Loss: [11.12271979]\n",
      "Epoch: 93 Loss: [11.08323587]\n",
      "Epoch: 94 Loss: [11.04840908]\n",
      "Epoch: 95 Loss: [11.01348804]\n",
      "Epoch: 96 Loss: [10.97970525]\n",
      "Epoch: 97 Loss: [10.94538867]\n",
      "Epoch: 98 Loss: [10.91406717]\n",
      "Epoch: 99 Loss: [10.88401149]\n",
      "Epoch: 100 Loss: [10.85590955]\n",
      "Epoch: 101 Loss: [10.82740736]\n",
      "Epoch: 102 Loss: [10.80170293]\n",
      "Epoch: 103 Loss: [10.77627188]\n",
      "Epoch: 104 Loss: [10.75152592]\n",
      "Epoch: 105 Loss: [10.72903765]\n",
      "Epoch: 106 Loss: [10.70611305]\n",
      "Epoch: 107 Loss: [10.6844855]\n",
      "Epoch: 108 Loss: [10.66332783]\n",
      "Epoch: 109 Loss: [10.64338549]\n",
      "Epoch: 110 Loss: [10.62385699]\n",
      "Epoch: 111 Loss: [10.6054534]\n",
      "Epoch: 112 Loss: [10.587414]\n",
      "Epoch: 113 Loss: [10.57044396]\n",
      "Epoch: 114 Loss: [10.55416535]\n",
      "Epoch: 115 Loss: [10.53809535]\n",
      "Epoch: 116 Loss: [10.5228696]\n",
      "Epoch: 117 Loss: [10.50823436]\n",
      "Epoch: 118 Loss: [10.49417866]\n",
      "Epoch: 119 Loss: [10.48068]\n",
      "Epoch: 120 Loss: [10.46770425]\n",
      "Epoch: 121 Loss: [10.4543764]\n",
      "Epoch: 122 Loss: [10.44261065]\n",
      "Epoch: 123 Loss: [10.43111766]\n",
      "Epoch: 124 Loss: [10.41987892]\n",
      "Epoch: 125 Loss: [10.40739031]\n",
      "Epoch: 126 Loss: [10.39672926]\n",
      "Epoch: 127 Loss: [10.38713806]\n",
      "Epoch: 128 Loss: [10.37777947]\n",
      "Epoch: 129 Loss: [10.36488195]\n",
      "Epoch: 130 Loss: [10.35726785]\n",
      "Epoch: 131 Loss: [10.34879601]\n",
      "Epoch: 132 Loss: [10.34083578]\n",
      "Epoch: 133 Loss: [10.3331889]\n",
      "Epoch: 134 Loss: [10.32579144]\n",
      "Epoch: 135 Loss: [10.31873937]\n",
      "Epoch: 136 Loss: [10.31197592]\n",
      "Epoch: 137 Loss: [10.30177747]\n",
      "Epoch: 138 Loss: [10.29551884]\n",
      "Epoch: 139 Loss: [10.2895083]\n",
      "Epoch: 140 Loss: [10.28373663]\n",
      "Epoch: 141 Loss: [10.27818622]\n",
      "Epoch: 142 Loss: [10.27277996]\n",
      "Epoch: 143 Loss: [10.26721036]\n",
      "Epoch: 144 Loss: [10.26228898]\n",
      "Epoch: 145 Loss: [10.25756778]\n",
      "Epoch: 146 Loss: [10.2531113]\n",
      "Epoch: 147 Loss: [10.24903778]\n",
      "Epoch: 148 Loss: [10.24520911]\n",
      "Epoch: 149 Loss: [10.24107462]\n",
      "Epoch: 150 Loss: [10.23716988]\n",
      "Epoch: 151 Loss: [10.23340986]\n",
      "Epoch: 152 Loss: [10.22980174]\n",
      "Epoch: 153 Loss: [10.22622618]\n",
      "Epoch: 154 Loss: [10.22289917]\n",
      "Epoch: 155 Loss: [10.21923325]\n",
      "Epoch: 156 Loss: [10.21764167]\n",
      "Epoch: 157 Loss: [10.21467225]\n",
      "Epoch: 158 Loss: [10.21181887]\n",
      "Epoch: 159 Loss: [10.209077]\n",
      "Epoch: 160 Loss: [10.20643585]\n",
      "Epoch: 161 Loss: [10.20389886]\n",
      "Epoch: 162 Loss: [10.201454]\n",
      "Epoch: 163 Loss: [10.19910256]\n",
      "Epoch: 164 Loss: [10.1968407]\n",
      "Epoch: 165 Loss: [10.19465903]\n",
      "Epoch: 166 Loss: [10.19218296]\n",
      "Epoch: 167 Loss: [10.19014471]\n",
      "Epoch: 168 Loss: [10.18818443]\n",
      "Epoch: 169 Loss: [10.1862926]\n",
      "Epoch: 170 Loss: [10.18447122]\n",
      "Epoch: 171 Loss: [10.18270901]\n",
      "Epoch: 172 Loss: [10.1810072]\n",
      "Epoch: 173 Loss: [10.1793678]\n",
      "Epoch: 174 Loss: [10.17777906]\n",
      "Epoch: 175 Loss: [10.17624024]\n",
      "Epoch: 176 Loss: [10.174754]\n",
      "Epoch: 177 Loss: [10.17331715]\n",
      "Epoch: 178 Loss: [10.17192073]\n",
      "Epoch: 179 Loss: [10.17056489]\n",
      "Epoch: 180 Loss: [10.16945536]\n",
      "Epoch: 181 Loss: [10.16812833]\n",
      "Epoch: 182 Loss: [10.16688457]\n",
      "Epoch: 183 Loss: [10.16576323]\n",
      "Epoch: 184 Loss: [10.16456316]\n",
      "Epoch: 185 Loss: [10.16339243]\n",
      "Epoch: 186 Loss: [10.16224941]\n",
      "Epoch: 187 Loss: [10.16131379]\n",
      "Epoch: 188 Loss: [10.16022087]\n",
      "Epoch: 189 Loss: [10.15915005]\n",
      "Epoch: 190 Loss: [10.15809926]\n",
      "Epoch: 191 Loss: [10.1570441]\n",
      "Epoch: 192 Loss: [10.15603051]\n",
      "Epoch: 193 Loss: [10.1551217]\n",
      "Epoch: 194 Loss: [10.15408807]\n",
      "Epoch: 195 Loss: [10.15310711]\n",
      "Epoch: 196 Loss: [10.15213803]\n",
      "Epoch: 197 Loss: [10.15118005]\n",
      "Epoch: 198 Loss: [10.15019829]\n",
      "Epoch: 199 Loss: [10.14924904]\n",
      "Epoch: 200 Loss: [10.14830652]\n",
      "Epoch: 201 Loss: [10.14739477]\n",
      "Epoch: 202 Loss: [10.14644786]\n",
      "Epoch: 203 Loss: [10.14551612]\n",
      "Epoch: 204 Loss: [10.14458675]\n",
      "Epoch: 205 Loss: [10.14365782]\n",
      "Epoch: 206 Loss: [10.14256407]\n",
      "Epoch: 207 Loss: [10.141557]\n",
      "Epoch: 208 Loss: [10.14063771]\n",
      "Epoch: 209 Loss: [10.13971402]\n",
      "Epoch: 210 Loss: [10.13878583]\n",
      "Epoch: 211 Loss: [10.13785236]\n",
      "Epoch: 212 Loss: [10.13691282]\n",
      "Epoch: 213 Loss: [10.1359]\n",
      "Epoch: 214 Loss: [10.13501177]\n",
      "Epoch: 215 Loss: [10.13405728]\n",
      "Epoch: 216 Loss: [10.13309412]\n",
      "Epoch: 217 Loss: [10.13225462]\n",
      "Epoch: 218 Loss: [10.13125056]\n",
      "Epoch: 219 Loss: [10.13023746]\n",
      "Epoch: 220 Loss: [10.12921422]\n",
      "Epoch: 221 Loss: [10.12818024]\n",
      "Epoch: 222 Loss: [10.12713496]\n",
      "Epoch: 223 Loss: [10.12607783]\n",
      "Epoch: 224 Loss: [10.12500834]\n",
      "Epoch: 225 Loss: [10.12392598]\n",
      "Epoch: 226 Loss: [10.12283029]\n",
      "Epoch: 227 Loss: [10.12172081]\n",
      "Epoch: 228 Loss: [10.12059711]\n",
      "Epoch: 229 Loss: [10.11945877]\n",
      "Epoch: 230 Loss: [10.11830505]\n",
      "Epoch: 231 Loss: [10.11713793]\n",
      "Epoch: 232 Loss: [10.11595351]\n",
      "Epoch: 233 Loss: [10.11471931]\n",
      "Epoch: 234 Loss: [10.11350549]\n",
      "Epoch: 235 Loss: [10.11227492]\n",
      "Epoch: 236 Loss: [10.11102732]\n",
      "Epoch: 237 Loss: [10.10976243]\n",
      "Epoch: 238 Loss: [10.10848015]\n",
      "Epoch: 239 Loss: [10.10717942]\n",
      "Epoch: 240 Loss: [10.10586117]\n",
      "Epoch: 241 Loss: [10.1045288]\n",
      "Epoch: 242 Loss: [10.10289872]\n",
      "Epoch: 243 Loss: [10.10142733]\n",
      "Epoch: 244 Loss: [10.10008495]\n",
      "Epoch: 245 Loss: [10.09867876]\n",
      "Epoch: 246 Loss: [10.09725755]\n",
      "Epoch: 247 Loss: [10.09581795]\n",
      "Epoch: 248 Loss: [10.0943598]\n",
      "Epoch: 249 Loss: [10.09288297]\n",
      "Epoch: 250 Loss: [10.09138733]\n",
      "Epoch: 251 Loss: [10.08924497]\n",
      "Epoch: 252 Loss: [10.08773152]\n",
      "Epoch: 253 Loss: [10.08619891]\n",
      "Epoch: 254 Loss: [10.08464563]\n",
      "Epoch: 255 Loss: [10.08307345]\n",
      "Epoch: 256 Loss: [10.08148165]\n",
      "Epoch: 257 Loss: [10.07982669]\n",
      "Epoch: 258 Loss: [10.07819882]\n",
      "Epoch: 259 Loss: [10.07655124]\n",
      "Epoch: 260 Loss: [10.07488396]\n",
      "Epoch: 261 Loss: [10.07319746]\n",
      "Epoch: 262 Loss: [10.07148996]\n",
      "Epoch: 263 Loss: [10.06976365]\n",
      "Epoch: 264 Loss: [10.06801776]\n",
      "Epoch: 265 Loss: [10.06625233]\n",
      "Epoch: 266 Loss: [10.06446793]\n",
      "Epoch: 267 Loss: [10.06266265]\n",
      "Epoch: 268 Loss: [10.06082212]\n",
      "Epoch: 269 Loss: [10.05898034]\n",
      "Epoch: 270 Loss: [10.0571199]\n",
      "Epoch: 271 Loss: [10.05523878]\n",
      "Epoch: 272 Loss: [10.05333953]\n",
      "Epoch: 273 Loss: [10.0514723]\n",
      "Epoch: 274 Loss: [10.04961597]\n",
      "Epoch: 275 Loss: [10.0476584]\n",
      "Epoch: 276 Loss: [10.04568289]\n",
      "Epoch: 277 Loss: [10.04369015]\n",
      "Epoch: 278 Loss: [10.04167794]\n",
      "Epoch: 279 Loss: [10.03964901]\n",
      "Epoch: 280 Loss: [10.0375023]\n",
      "Epoch: 281 Loss: [10.03540461]\n",
      "Epoch: 282 Loss: [10.03351121]\n",
      "Epoch: 283 Loss: [10.03117418]\n",
      "Epoch: 284 Loss: [10.02895991]\n",
      "Epoch: 285 Loss: [10.02677835]\n",
      "Epoch: 286 Loss: [10.02460778]\n",
      "Epoch: 287 Loss: [10.02247328]\n",
      "Epoch: 288 Loss: [10.02007742]\n",
      "Epoch: 289 Loss: [10.01787004]\n",
      "Epoch: 290 Loss: [10.01564931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 291 Loss: [10.01341534]\n",
      "Epoch: 292 Loss: [10.01116714]\n",
      "Epoch: 293 Loss: [10.00890525]\n",
      "Epoch: 294 Loss: [10.0063071]\n",
      "Epoch: 295 Loss: [10.00402564]\n",
      "Epoch: 296 Loss: [10.00171637]\n",
      "Epoch: 297 Loss: [9.99939389]\n",
      "Epoch: 298 Loss: [9.99705831]\n",
      "Epoch: 299 Loss: [9.99470972]\n",
      "Epoch: 300 Loss: [9.99226563]\n",
      "Epoch: 301 Loss: [9.98989619]\n",
      "Epoch: 302 Loss: [9.98751628]\n",
      "Epoch: 303 Loss: [9.98512373]\n",
      "Epoch: 304 Loss: [9.98271866]\n",
      "Epoch: 305 Loss: [9.9803012]\n",
      "Epoch: 306 Loss: [9.97787149]\n",
      "Epoch: 307 Loss: [9.97542966]\n",
      "Epoch: 308 Loss: [9.97297584]\n",
      "Epoch: 309 Loss: [9.97051019]\n",
      "Epoch: 310 Loss: [9.96803285]\n",
      "Epoch: 311 Loss: [9.96554395]\n",
      "Epoch: 312 Loss: [9.96304365]\n",
      "Epoch: 313 Loss: [9.96053209]\n",
      "Epoch: 314 Loss: [9.95800943]\n",
      "Epoch: 315 Loss: [9.95547581]\n",
      "Epoch: 316 Loss: [9.9529314]\n",
      "Epoch: 317 Loss: [9.95037634]\n",
      "Epoch: 318 Loss: [9.94781079]\n",
      "Epoch: 319 Loss: [9.94523491]\n",
      "Epoch: 320 Loss: [9.94264885]\n",
      "Epoch: 321 Loss: [9.94005278]\n",
      "Epoch: 322 Loss: [9.93744685]\n",
      "Epoch: 323 Loss: [9.93483123]\n",
      "Epoch: 324 Loss: [9.93333089]\n",
      "Epoch: 325 Loss: [9.93011824]\n",
      "Epoch: 326 Loss: [9.92741242]\n",
      "Epoch: 327 Loss: [9.92475985]\n",
      "Epoch: 328 Loss: [9.92330756]\n",
      "Epoch: 329 Loss: [9.91948359]\n",
      "Epoch: 330 Loss: [9.91673981]\n",
      "Epoch: 331 Loss: [9.91405172]\n",
      "Epoch: 332 Loss: [9.91255182]\n",
      "Epoch: 333 Loss: [9.90870994]\n",
      "Epoch: 334 Loss: [9.90535409]\n",
      "Epoch: 335 Loss: [9.90263151]\n",
      "Epoch: 336 Loss: [9.89992459]\n",
      "Epoch: 337 Loss: [9.89720852]\n",
      "Epoch: 338 Loss: [9.89585991]\n",
      "Epoch: 339 Loss: [9.89199527]\n",
      "Epoch: 340 Loss: [9.88927134]\n",
      "Epoch: 341 Loss: [9.88654306]\n",
      "Epoch: 342 Loss: [9.88367235]\n",
      "Epoch: 343 Loss: [9.88163753]\n",
      "Epoch: 344 Loss: [9.87970853]\n",
      "Epoch: 345 Loss: [9.87885988]\n",
      "Epoch: 346 Loss: [9.87615377]\n",
      "Epoch: 347 Loss: [9.87345845]\n",
      "Epoch: 348 Loss: [9.87066843]\n",
      "Epoch: 349 Loss: [9.8680483]\n",
      "Epoch: 350 Loss: [9.86458033]\n",
      "Epoch: 351 Loss: [9.86172553]\n",
      "Epoch: 352 Loss: [9.85893452]\n",
      "Epoch: 353 Loss: [9.85622366]\n",
      "Epoch: 354 Loss: [9.85360541]\n",
      "Epoch: 355 Loss: [9.85080845]\n",
      "Epoch: 356 Loss: [9.84807841]\n",
      "Epoch: 357 Loss: [9.84535619]\n",
      "Epoch: 358 Loss: [9.84261899]\n",
      "Epoch: 359 Loss: [9.8399778]\n",
      "Epoch: 360 Loss: [9.83714025]\n",
      "Epoch: 361 Loss: [9.83500357]\n",
      "Epoch: 362 Loss: [9.83227823]\n",
      "Epoch: 363 Loss: [9.82950364]\n",
      "Epoch: 364 Loss: [9.82682942]\n",
      "Epoch: 365 Loss: [9.82396014]\n",
      "Epoch: 366 Loss: [9.82117317]\n",
      "Epoch: 367 Loss: [9.81847953]\n",
      "Epoch: 368 Loss: [9.81560271]\n",
      "Epoch: 369 Loss: [9.81280546]\n",
      "Epoch: 370 Loss: [9.81001]\n",
      "Epoch: 371 Loss: [9.8072524]\n",
      "Epoch: 372 Loss: [9.80430575]\n",
      "Epoch: 373 Loss: [9.8015615]\n",
      "Epoch: 374 Loss: [9.79855004]\n",
      "Epoch: 375 Loss: [9.79575865]\n",
      "Epoch: 376 Loss: [9.79297216]\n",
      "Epoch: 377 Loss: [9.79019259]\n",
      "Epoch: 378 Loss: [9.78740602]\n",
      "Epoch: 379 Loss: [9.7846258]\n",
      "Epoch: 380 Loss: [9.78183856]\n",
      "Epoch: 381 Loss: [9.77889453]\n",
      "Epoch: 382 Loss: [9.77610706]\n",
      "Epoch: 383 Loss: [9.77331699]\n",
      "Epoch: 384 Loss: [9.77052473]\n",
      "Epoch: 385 Loss: [9.7677304]\n",
      "Epoch: 386 Loss: [9.7645779]\n",
      "Epoch: 387 Loss: [9.76176221]\n",
      "Epoch: 388 Loss: [9.75896417]\n",
      "Epoch: 389 Loss: [9.75610386]\n",
      "Epoch: 390 Loss: [9.75330498]\n",
      "Epoch: 391 Loss: [9.7505092]\n",
      "Epoch: 392 Loss: [9.74861758]\n",
      "Epoch: 393 Loss: [9.74576506]\n",
      "Epoch: 394 Loss: [9.74367469]\n",
      "Epoch: 395 Loss: [9.74017717]\n",
      "Epoch: 396 Loss: [9.73732208]\n",
      "Epoch: 397 Loss: [9.73522898]\n",
      "Epoch: 398 Loss: [9.73173126]\n",
      "Epoch: 399 Loss: [9.72887642]\n",
      "Epoch: 400 Loss: [9.72605653]\n",
      "Epoch: 401 Loss: [9.72396363]\n",
      "Epoch: 402 Loss: [9.72046622]\n",
      "Epoch: 403 Loss: [9.71760623]\n",
      "Epoch: 404 Loss: [9.7155147]\n",
      "Epoch: 405 Loss: [9.71201497]\n",
      "Epoch: 406 Loss: [9.70915972]\n",
      "Epoch: 407 Loss: [9.70634122]\n",
      "Epoch: 408 Loss: [9.70425279]\n",
      "Epoch: 409 Loss: [9.70075596]\n",
      "Epoch: 410 Loss: [9.69789834]\n",
      "Epoch: 411 Loss: [9.69569714]\n",
      "Epoch: 412 Loss: [9.69231199]\n",
      "Epoch: 413 Loss: [9.68946413]\n",
      "Epoch: 414 Loss: [9.68726501]\n",
      "Epoch: 415 Loss: [9.68388423]\n",
      "Epoch: 416 Loss: [9.68067411]\n",
      "Epoch: 417 Loss: [9.6784542]\n",
      "Epoch: 418 Loss: [9.67496754]\n",
      "Epoch: 419 Loss: [9.67212637]\n",
      "Epoch: 420 Loss: [9.6700547]\n",
      "Epoch: 421 Loss: [9.6665781]\n",
      "Epoch: 422 Loss: [9.66373944]\n",
      "Epoch: 423 Loss: [9.66167512]\n",
      "Epoch: 424 Loss: [9.65820393]\n",
      "Epoch: 425 Loss: [9.65549313]\n",
      "Epoch: 426 Loss: [9.65259232]\n",
      "Epoch: 427 Loss: [9.65053556]\n",
      "Epoch: 428 Loss: [9.64706118]\n",
      "Epoch: 429 Loss: [9.6447003]\n",
      "Epoch: 430 Loss: [9.64134859]\n",
      "Epoch: 431 Loss: [9.63855591]\n",
      "Epoch: 432 Loss: [9.63639622]\n",
      "Epoch: 433 Loss: [9.63305981]\n",
      "Epoch: 434 Loss: [9.63027316]\n",
      "Epoch: 435 Loss: [9.62811934]\n",
      "Epoch: 436 Loss: [9.62479017]\n",
      "Epoch: 437 Loss: [9.62258018]\n",
      "Epoch: 438 Loss: [9.61962017]\n",
      "Epoch: 439 Loss: [9.61685721]\n",
      "Epoch: 440 Loss: [9.61411095]\n",
      "Epoch: 441 Loss: [9.61136733]\n",
      "Epoch: 442 Loss: [9.60862642]\n",
      "Epoch: 443 Loss: [9.60588827]\n",
      "Epoch: 444 Loss: [9.60315292]\n",
      "Epoch: 445 Loss: [9.60042043]\n",
      "Epoch: 446 Loss: [9.59769084]\n",
      "Epoch: 447 Loss: [9.59483445]\n",
      "Epoch: 448 Loss: [9.59223335]\n",
      "Epoch: 449 Loss: [9.58952122]\n",
      "Epoch: 450 Loss: [9.58680358]\n",
      "Epoch: 451 Loss: [9.58408907]\n",
      "Epoch: 452 Loss: [9.58137774]\n",
      "Epoch: 453 Loss: [9.57866962]\n",
      "Epoch: 454 Loss: [9.57596475]\n",
      "Epoch: 455 Loss: [9.57326317]\n",
      "Epoch: 456 Loss: [9.57056491]\n",
      "Epoch: 457 Loss: [9.56787002]\n",
      "Epoch: 458 Loss: [9.56504417]\n",
      "Epoch: 459 Loss: [9.56248431]\n",
      "Epoch: 460 Loss: [9.55980671]\n",
      "Epoch: 461 Loss: [9.55712672]\n",
      "Epoch: 462 Loss: [9.5544486]\n",
      "Epoch: 463 Loss: [9.55177394]\n",
      "Epoch: 464 Loss: [9.54910616]\n",
      "Epoch: 465 Loss: [9.5491169]\n",
      "Epoch: 466 Loss: [9.54645258]\n",
      "Epoch: 467 Loss: [9.54378963]\n",
      "Epoch: 468 Loss: [9.54113256]\n",
      "Epoch: 469 Loss: [9.53847671]\n",
      "Epoch: 470 Loss: [9.53582369]\n",
      "Epoch: 471 Loss: [9.53317367]\n",
      "Epoch: 472 Loss: [9.5305267]\n",
      "Epoch: 473 Loss: [9.52872634]\n",
      "Epoch: 474 Loss: [9.52541035]\n",
      "Epoch: 475 Loss: [9.52338937]\n",
      "Epoch: 476 Loss: [9.52076322]\n",
      "Epoch: 477 Loss: [9.51820558]\n",
      "Epoch: 478 Loss: [9.51576025]\n",
      "Epoch: 479 Loss: [9.51387135]\n",
      "Epoch: 480 Loss: [9.5105809]\n",
      "Epoch: 481 Loss: [9.50793079]\n",
      "Epoch: 482 Loss: [9.5053298]\n",
      "Epoch: 483 Loss: [9.50273256]\n",
      "Epoch: 484 Loss: [9.50086242]\n",
      "Epoch: 485 Loss: [9.49759326]\n",
      "Epoch: 486 Loss: [9.49496072]\n",
      "Epoch: 487 Loss: [9.49237921]\n",
      "Epoch: 488 Loss: [9.48980325]\n",
      "Epoch: 489 Loss: [9.48722755]\n",
      "Epoch: 490 Loss: [9.48538382]\n",
      "Epoch: 491 Loss: [9.48213518]\n",
      "Epoch: 492 Loss: [9.47952664]\n",
      "Epoch: 493 Loss: [9.4769708]\n",
      "Epoch: 494 Loss: [9.474417]\n",
      "Epoch: 495 Loss: [9.47250109]\n",
      "Epoch: 496 Loss: [9.46926962]\n",
      "Epoch: 497 Loss: [9.46676451]\n",
      "Epoch: 498 Loss: [9.46422505]\n",
      "Epoch: 499 Loss: [9.46169345]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 10\n",
    "num_batches = num_samples/batch_size\n",
    "lr = 1e-3\n",
    "loss1 = []\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        for k in range(batch_size):\n",
    "            x1 = np.matmul(W1,x_train_subs[j*batch_size+k]).reshape(-1,1)+b1\n",
    "            z1 = relu(x1)\n",
    "        \n",
    "            x2 = np.matmul(W2,z1).reshape(-1,1)+b2\n",
    "            z2 = relu(x2)\n",
    "            \n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j*batch_size+k])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j*batch_size+k].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(x2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(x1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j*batch_size+k].reshape(-1,1).T)\n",
    "        W3 = W3 - lr*W3_upd\n",
    "        W2 = W2 - lr*W2_upd\n",
    "        W1 = W1 - lr*W1_upd\n",
    "        b3 = b3 - lr*b3_upd\n",
    "        b2 = b2 - lr*b2_upd\n",
    "        b1 = b1 - lr*b1_upd\n",
    "    loss1.append(loss)\n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = relu(np.matmul(W1,test_x[4]).reshape(-1,1)+b1)\n",
    "z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.90318860e-05]\n",
      " [9.98273512e-01]\n",
      " [1.67745639e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 1, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "acc =accuracy_score(y_pred=preds,y_true=true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGuhJREFUeJzt3X2QHPWd3/H3Z2Yf9Ige0CKEHpAAYU5wIGBLlgvsYPxwwNlgzi5iyjE4RyI7B1e2y5UryF3ZvqScXJwAji8JFzkQsEOwsWUO4pAzMiYGXx2YFchCQggJEEZCSIse0LNWu/vNH9O7aq1mZlc7szs7PZ9X1TDdv+7p/vbu8pnWr58UEZiZWXblal2AmZmNLAe9mVnGOejNzDLOQW9mlnEOejOzjHPQm5llnIPezCzjHPRmZhnnoDczy7imWhcAMGPGjJg/f36tyzAzqyurVq16NyLaBptvTAT9/Pnz6ejoqHUZZmZ1RdKbQ5nPXTdmZhk3aNBLmivpKUkvS1on6ctJ+3RJKyVtTN6nJe2S9F1JmyStkXTJSG+EmZmVNpQ9+m7gaxGxCFgK3CppEXA78GRELASeTMYBrgYWJq9lwD1Vr9rMzIZs0KCPiG0R8UIyvA9YD8wGrgMeSGZ7APhUMnwd8P0oeBaYKmlW1Ss3M7MhOak+eknzgYuB54CZEbEtmfQOMDMZng28lfrYlqRt4LKWSeqQ1NHZ2XmSZZuZ2VANOeglTQJWAF+JiL3paVF4eslJPcEkIpZHRHtEtLe1DXp2kJmZDdOQgl5SM4WQfzAifpo0b+/rkknedyTtW4G5qY/PSdrMzKwGhnLWjYB7gfURcVdq0mPAzcnwzcCjqfabkrNvlgLvpbp4qmrbe4e484kNvN65fyQWb2aWCUPZo78M+DxwpaTVyesa4K+Aj0naCHw0GQd4HHgd2AR8D/iT6pddsGPvEf76l5t4490DI7UKM7O6N+iVsRHxa0AlJn+kyPwB3FphXUOSzxXK6vXzzc3MSqrrK2OVfP30OOnNzEqq66A/tkfvoDczK6W+gz7ZpfcevZlZaXUd9JL36M3MBlPXQe+uGzOzwdV30Pd33dS4EDOzMayugz6XVO89ejOz0uo76Pv66H0w1syspLoO+r4++h7v0ZuZlVTXQe89ejOzwdV10PsWCGZmg6vroM/5FghmZoOq76D3efRmZoOq66D3LRDMzAZX10HffzDWOW9mVlJ9B70vmDIzG1RdB727bszMBjeUZ8beJ2mHpLWpth+lHiu4WdLqpH2+pEOpaX8zksX7pmZmZoMb9FGCwP3Afwa+39cQEf+4b1jSncB7qflfi4jF1SqwHPmCKTOzQQ3lmbFPS5pfbJoKSXsDcGV1yxq6fE6+BYKZWRmV9tF/ENgeERtTbQskvSjpV5I+WOHyB5WXfJtiM7MyhtJ1U86NwEOp8W3AvIjYKelS4G8lnR8Rewd+UNIyYBnAvHnzhl1ALgfhPXozs5KGvUcvqQn4I+BHfW0RcSQidibDq4DXgHOLfT4ilkdEe0S0t7W1DbcMcpLPujEzK6OSrpuPAq9ExJa+BkltkvLJ8FnAQuD1ykosLy/30ZuZlTOU0ysfAv4BeJ+kLZJuSSZ9luO7bQA+BKxJTrf8CfCliNhVzYIHyuXks27MzMoYylk3N5Zo/0KRthXAisrLGrqcfAsEM7Ny6vrKWPDplWZmg6n7oM/JXTdmZuXUfdDnc/ItEMzMyqj7oM/5gikzs7LqP+hzvqmZmVk5dR/0eV8wZWZWVt0Hfc599GZmZdV/0MtBb2ZWTt0HvbtuzMzKq/ugz+V81o2ZWTn1H/TybYrNzMqp+6D3LRDMzMqr+6D3/ejNzMqr+6DP54R36M3MSqv7oM8J79GbmZWRgaB3H72ZWTl1H/R5P2HKzKysbAS99+jNzEoayjNj75O0Q9LaVNs3JW2VtDp5XZOadoekTZI2SPqDkSo8tT56nPNmZiUNZY/+fuCqIu13R8Ti5PU4gKRFFB4afn7ymf8qKV+tYovJC3fdmJmVMWjQR8TTwK4hLu864IcRcSQi3gA2AUsqqG9Q+ZzPozczK6eSPvrbJK1JunamJW2zgbdS82xJ2k4gaZmkDkkdnZ2dwy5CvnulmVlZww36e4CzgcXANuDOk11ARCyPiPaIaG9raxtmGYW7VzrozcxKG1bQR8T2iOiJiF7gexzrntkKzE3NOidpGzHuujEzK29YQS9pVmr0eqDvjJzHgM9KapW0AFgI/KayEsvL+RYIZmZlNQ02g6SHgCuAGZK2AN8ArpC0GAhgM/BFgIhYJ+lh4GWgG7g1InpGpvSCnPCVsWZmZQwa9BFxY5Hme8vM/y3gW5UUdTL8hCkzs/Lq/srYnG+BYGZWVt0HfeGsm1pXYWY2dtV90OdyottJb2ZWUt0HfUtedPf66eBmZqXUfdA353N0dTvozcxKqf+gb8pxtMdBb2ZWSt0HfUs+x9GeIHwuvZlZUfUf9E2FTTjqm9KbmRVV90HfnBeAu2/MzErIQNAXNsEHZM3Miqv7oD/WdeOgNzMrpu6Dvn+P3kFvZlZU3Qd9S94HY83Myqn7oG/Ou+vGzKycDAR94awbH4w1Myuu7oO+72Cs++jNzIqr/6Dv67rxHr2ZWVGDBr2k+yTtkLQ21fYfJL0iaY2kRyRNTdrnSzokaXXy+puRLB4K97oBH4w1MytlKHv09wNXDWhbCVwQERcCrwJ3pKa9FhGLk9eXqlNmaT4Ya2ZW3qBBHxFPA7sGtD0REd3J6LPAnBGobUj6DsYecdeNmVlR1eij/2Pg/6bGF0h6UdKvJH2wCssvq9VXxpqZldVUyYcl/TnQDTyYNG0D5kXETkmXAn8r6fyI2Fvks8uAZQDz5s0bdg3uujEzK2/Ye/SSvgB8AvhcJDeDj4gjEbEzGV4FvAacW+zzEbE8Itojor2trW24ZTjozcwGMaygl3QV8GfAtRFxMNXeJimfDJ8FLARer0ahpRy7143PujEzK2bQrhtJDwFXADMkbQG+QeEsm1ZgpSSAZ5MzbD4E/GtJR4Fe4EsRsavogquk/4IpH4w1Mytq0KCPiBuLNN9bYt4VwIpKizoZLe66MTMrq+6vjO1/wpT36M3Miqr7oM/nhOQ9ejOzUuo+6CXRnM/5YKyZWQl1H/QArfmcD8aamZWQiaBvbsq568bMrIRsBH1eDnozsxIyEvQ5P3jEzKyETAR9Sz7n+9GbmZWQjaBvytHV3VPrMszMxqRMBH2z9+jNzErKSND7YKyZWSkZCXqfR29mVkomgr7F59GbmZWUjaD36ZVmZiVlIuib8zmOdvtgrJlZMdkIenfdmJmVlI2gz8tdN2ZmJWQi6Fu9R29mVtKQgl7SfZJ2SFqbapsuaaWkjcn7tKRdkr4raZOkNZIuGani+/j0SjOz0oa6R38/cNWAttuBJyNiIfBkMg5wNbAweS0D7qm8zPJ8ZayZWWlDCvqIeBrYNaD5OuCBZPgB4FOp9u9HwbPAVEmzqlFsKb57pZlZaZX00c+MiG3J8DvAzGR4NvBWar4tSdtxJC2T1CGpo7Ozs4IyoCW5BUKE9+rNzAaqysHYKCTsSaVsRCyPiPaIaG9ra6to/S1NOSKgp9dBb2Y2UCVBv72vSyZ535G0bwXmpuabk7SNmOZ8YTPcfWNmdqJKgv4x4OZk+Gbg0VT7TcnZN0uB91JdPCOiL+h9dayZ2YmahjKTpIeAK4AZkrYA3wD+CnhY0i3Am8ANyeyPA9cAm4CDwD+tcs0naG7yHr2ZWSlDCvqIuLHEpI8UmTeAWysp6mS1uuvGzKykTFwZO64lD8Chru4aV2JmNvZkIugntRaCfv8RPzfWzGygjAR9MwAHjniP3sxsoEwE/cT+PXoHvZnZQJkI+kmthWPK+w876M3MBspE0E9Mgv6AD8aamZ0gE0Hfv0fvrhszsxNkIuhbm3Lkc/LBWDOzIjIR9JKY2JLngE+vNDM7QSaCHgrdN/t8MNbM7ATZCfpxTe66MTMrIjtB39rkg7FmZkVkJuinT2xh54GuWpdhZjbmZCboZ0xq5d39R2pdhpnZmJOpoN+5/4gfJ2hmNkCGgr6F3oDdB919Y2aWlp2gn9wK4O4bM7MBhvSEqWIkvQ/4UarpLODrwFTgnwOdSfu/iojHh13hELVNSoJ+XxecPtJrMzOrH8MO+ojYACwGkJQHtgKPUHhG7N0R8R+rUuEQtSV79Dv2HR7N1ZqZjXnV6rr5CPBaRLxZpeWdtDOmjgdg6+5DtSrBzGxMqlbQfxZ4KDV+m6Q1ku6TNK1K6yhrXHOetsmtbHHQm5kdp+Kgl9QCXAv8OGm6BzibQrfONuDOEp9bJqlDUkdnZ2exWU7anGnj2bLnYFWWZWaWFdXYo78aeCEitgNExPaI6ImIXuB7wJJiH4qI5RHRHhHtbW1tVSgD5kyb4D16M7MBqhH0N5LqtpE0KzXtemBtFdYxJHOmjeftPYd80ZSZWcqwz7oBkDQR+BjwxVTztyUtBgLYPGDaiJozbTxHe4Id+w4za8r40VqtmdmYVlHQR8QB4NQBbZ+vqKIKzE7OvNmy+5CD3swskZkrY6HQRw+wZbcPyJqZ9clY0Cd79Lt8QNbMrE+mgn5cc54Zk3wuvZlZWqaCHgp79Vv3OOjNzPpkMujdR29mdkwGg34CW/ccotfn0puZAZkM+sK59Nt9F0szMyCDQT93euEUy9/tdPeNmRlkMOgXnDoRgDfePVDjSszMxobMBf3saeNpzos3djrozcwgg0Gfz4l50yew2Xv0ZmZABoMe4Oy2SWzcvr/WZZiZjQmZDPrfnz2F1989wN7DR2tdiplZzWUz6OdMAWDt1vdqXImZWe1lM+hnF4L+pS0OejOzTAb9qZNamT11PGu8R29mls2gB7hwzhTv0ZuZUYWgl7RZ0kuSVkvqSNqmS1opaWPyPq3yUk/ORXOn8rtdB+ncd2S0V21mNqZUa4/+wxGxOCLak/HbgScjYiHwZDI+qt6/YDoAv3lj12iv2sxsTBmprpvrgAeS4QeAT43Qekq6YPYUJrbkefb1naO9ajOzMaUaQR/AE5JWSVqWtM2MiG3J8DvAzCqs56Q053NcOn86z73hoDezxlaNoL88Ii4BrgZulfSh9MSICApfBseRtExSh6SOzs7OKpRxoqVnTefV7ft5d7/76c2scVUc9BGxNXnfATwCLAG2S5oFkLzvKPK55RHRHhHtbW1tlZZR1PsXnAq4n97MGltFQS9poqTJfcPAx4G1wGPAzclsNwOPVrKe4bpwzhTGN+d5zv30ZtbAmir8/EzgEUl9y/pfEfF3kp4HHpZ0C/AmcEOF6xmW5nyO9vnT+PvXHPRm1rgqCvqIeB24qEj7TuAjlSy7Wq5432n8m5+9zJs7D3Bm8lASM7NGktkrY/t8fFHhhJ+VL2+vcSVmZrWR+aCfO30C550+mScc9GbWoDIf9FDYq+/YvMu3QzCzhtQQQX/t4jPoDXh09dZal2JmNuoaIujPOW0yF82dyk9Wbal1KWZmo64hgh7gM5fM5pV39rHubd+62MwaS8ME/ScvOoOWfI4Vq9x9Y2aNpWGCfuqEFj666DQeeXELh4/21LocM7NR0zBBD/D5pfPZffCoD8qaWUNpqKBfetZ0fm/WKdz3680UbqppZpZ9DRX0kvjjy+azYfs+/n6T739jZo2hoYIeCufUz5jUyvJnXq91KWZmo6Lhgr61Kc8tly/g6Vc7WfXm7lqXY2Y24hou6AFu+sCZnDqxhe/84tVal2JmNuIaMugntjbxxX90Fs9sfJfnN/vpU2aWbQ0Z9FA41bJtcivf+j/r6e31GThmll0NG/TjW/LcftV5rH5rDz95wffAMbPsatigB7j+4tlceuY0vv13r/DeoaO1LsfMbEQMO+glzZX0lKSXJa2T9OWk/ZuStkpanbyuqV651ZXLib+89nx2Heji3z2+vtblmJmNiEr26LuBr0XEImApcKukRcm0uyNicfJ6vOIqR9AFs6ew7ENn88Pn3+KpDTtqXY6ZWdUNO+gjYltEvJAM7wPWA7OrVdho+urHFnLuzEncvmINuw901bocM7OqqkofvaT5wMXAc0nTbZLWSLpP0rQSn1kmqUNSR2dnZzXKGLbWpjx33bCY3QeO8tWHV/ssHDPLlIqDXtIkYAXwlYjYC9wDnA0sBrYBdxb7XEQsj4j2iGhva2urtIyKXTB7Cl//5CL+34ZOvvvLjbUux8ysaioKeknNFEL+wYj4KUBEbI+InojoBb4HLKm8zNHxuffP448umc13frGRn/qUSzPLiKbhflCSgHuB9RFxV6p9VkRsS0avB9ZWVuLokcS/vf73eee9w/zLn6xhUmsTHz//9FqXZWZWkUr26C8DPg9cOeBUym9LeknSGuDDwFerUehoGdecZ/lN7Vwwewp/8uALrPADxc2szg17jz4ifg2oyKQxfTrlUExqbeIHtyzhX/zPVXztx7/l7T2HuO3Kcyj8I8bMrL409JWx5Zwyrpn/8YUlXH/xbO5c+SrLfrCKnfuP1LosM7OT5qAvo6Upx103XMRf/OHv8asNnfzBd57h5+ve8WMIzayuOOgHIYl/9sGzeOxPL2PGpBa++INV/JN7n+Plt/fWujQzsyFx0A/Reaefwv/+08v55icXse7tvfzhXz/Dl36wihd/56dUmdnYprHQDdHe3h4dHR21LmPI9hzs4r8/8wbf/4fN7D3czUVzpvDpS+fwiQvPYPrEllqXZ2YNQtKqiGgfdD4H/fDtP9LNw8+/xY9XbWH9tr3kc+LSedO44rw2rjj3NM47fTK5nM/UMbOR4aAfZS+/vZfHX9rGUxt2sC7pv588romL502j/cxpXDhnCufOnMysKeN8mqaZVYWDvoa27z3Mrze+y6rf7eaFN3ezYfs++n7Mk1ubOGfmJM5pm8QZU8cze+p4zpg6njOmjuOMqeMZ15yvbfFmVjeGGvTDvmDKSpt5yjg+fekcPn3pHAD2Hj7KK9v28er2fWzcvo9Xt+/nV6920rn/CAO/Zye3NjF9UgvTJrQwfWLfezOTxzUzsbWJiS15JvS9tzQxqbWJCa15JrTkacnnaGnK0ZzP0ZLPudvIzAAH/ag4ZVwzSxZMZ8mC6ce1d3X3sn3vYbbuOcTbew6x7b3DdO47wu6DXew60MWOfYd5Zdtedh7o4kh370mvtymn/uBvzudobcrRnC+05XM58jnIS0ginxM5Qa5/WOSStnx6OJfMn8zX/1WivjchHWtSqp1kXAM+JB27xFolllOYduyLq/x8x+ZV8p+++Y6bJ90mnVBv37T08op9Nj1+bB6lpiXj6TqPm168NtKfPW7bBmz3wOkDamPAzyiX1JJeT65YPScsqzCcSy2zrz13Qk2p9RRZRnq7+uZL/+6OqzP1O8ylpvcvO3f88vrWPfDncdw2Nlj3qYO+hlqacsydPoG50ycMOm9Xdy+Huno40NXNwa5uDhwpDB840sPBrm4OdvXQ1d3L0Z5ejiTvXan3rp44bry7N+iNwqunb7gXeiI42tNLT2/QExD906G3N+iJvnkLwwARnPAvk4gg+oeTdyI1nJ4WA+aj/6K09HwD24j09Eh99vj1Rf+8qXpTNY6B3kurkWJfHCRfZmW/oMSAL6kT2/rmBcjlSi/vw+87jb/4xKKi9VWLg75OtDQVumWmTGiudSmZFVH6iyAGfhnF8V8kJ3zRDDI9km+pYsvvX3dqXYXPRokvqYGfTc1bovbe/s+ntrHIcG+cWGf68xzXdmJdvUHRn9XAGksu77gv66Sm3vTvpcjyUtvIcdOT9QxcXhz/ub7fTbH2E35OqW2kyPacsLwidc+aOv6k/k6Hw0FvllDqn/tJS81qMasmXxlrZpZxDnozs4xz0JuZZZyD3sws40Ys6CVdJWmDpE2Sbh+p9ZiZWXkjEvSS8sB/Aa4GFgE3ShrZE0XNzKyokdqjXwJsiojXI6IL+CFw3Qity8zMyhipoJ8NvJUa35K0mZnZKKvZBVOSlgHLktH9kjYMc1EzgHerU1Xd8DY3Bm9zY6hkm88cykwjFfRbgbmp8TlJW7+IWA4sr3RFkjqGcpvOLPE2NwZvc2MYjW0eqa6b54GFkhZIagE+Czw2QusyM7MyRmSPPiK6Jd0G/BzIA/dFxLqRWJeZmZU3Yn30EfE48PhILT+l4u6fOuRtbgze5sYw4ts8Jh4laGZmI8e3QDAzy7i6Dfos32JB0n2Sdkham2qbLmmlpI3J+7SkXZK+m/wc1ki6pHaVD4+kuZKekvSypHWSvpy0Z3mbx0n6jaTfJtv8l0n7AknPJdv2o+RkBiS1JuObkunza1l/JSTlJb0o6WfJeKa3WdJmSS9JWi2pI2kb1b/tugz6BrjFwv3AVQPabgeejIiFwJPJOBR+BguT1zLgnlGqsZq6ga9FxCJgKXBr8vvM8jYfAa6MiIuAxcBVkpYC/x64OyLOAXYDtyTz3wLsTtrvTuarV18G1qfGG2GbPxwRi1OnUY7u33bhcVb19QI+APw8NX4HcEet66ryNs4H1qbGNwCzkuFZwIZk+L8BNxabr15fwKPAxxplm4EJwAvA+ylcONOUtPf/nVM4g+0DyXBTMp9qXfswtnUOhWC7EvgZhcd4ZX2bNwMzBrSN6t92Xe7R05i3WJgZEduS4XeAmclwpn4WyT/PLwaeI+PbnHRhrAZ2ACuB14A9EdGdzJLerv5tTqa/B5w6uhVXxXeAPwN6k/FTyf42B/CEpFXJHQFglP+2/czYOhQRISlzp0tJmgSsAL4SEXt17OGtmdzmiOgBFkuaCjwCnFfjkkaUpE8AOyJilaQral3PKLo8IrZKOg1YKemV9MTR+Nuu1z36QW+xkEHbJc0CSN53JO2Z+FlIaqYQ8g9GxE+T5kxvc5+I2AM8RaHbYqqkvh2w9Hb1b3MyfQqwc5RLrdRlwLWSNlO4o+2VwH8i29tMRGxN3ndQ+EJfwij/bddr0DfiLRYeA25Ohm+m0I/d135TcrR+KfBe6p+EdUGFXfd7gfURcVdqUpa3uS3Zk0fSeArHJNZTCPzPJLMN3Oa+n8VngF9G0olbLyLijoiYExHzKfw/+8uI+BwZ3mZJEyVN7hsGPg6sZbT/tmt9oKKCAxzXAK9S6Nf881rXU+VtewjYBhyl0Ed3C4W+ySeBjcAvgOnJvKJwBtJrwEtAe63rH8b2Xk6hH3MNsDp5XZPxbb4QeDHZ5rXA15P2s4DfAJuAHwOtSfu4ZHxTMv2sWm9Dhdt/BfCzrG9zsm2/TV7r+rJqtP+2fWWsmVnG1WvXjZmZDZGD3sws4xz0ZmYZ56A3M8s4B72ZWcY56M3MMs5Bb2aWcQ56M7OM+//B7SjKZKsudAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,501)\n",
    "plt.plot(epochs,loss1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196.3660916954209,142.30286417767098,117.70130563525572,94.69569600231195,83.48210515565967,77.08913646395361,72.27089667791495,67.85234189643197,63.19929308490474,59.24579920973699,56.220632911636095,53.58032008762,51.197327722578095,48.876468352319485,46.614762876094666,44.64194162720479,42.87166401337471,41.23069018447052,39.669439276829394,38.21866288493695,36.83647227204972,35.52305232799775,34.27130721664939,33.094305669826916,31.968753549907703,30.853565517693198,29.842637451607718,28.858569153303595,27.933108650631972,27.039461674155298,26.192328297727705,25.373437308002504,24.600675101829243,23.852268159390025,23.144644692791754,22.496724177681685,21.87104319384201,21.28458372142161,20.7507709569495,20.20222710481518,19.723293676242967,19.25141088291232,18.827608959832887,18.401534719685973,18.021691485304153,17.63505960449034,17.279677730963684,16.9458527080261,16.629375390277833,16.32676669483387,16.043563994140413,15.773062516661474,15.516355849395042,15.26226241596077,15.028778123120393,14.810282918129438,14.6049769132443,14.414381813093492,14.228417855583729,14.051372403638595,13.882821693658354,13.726717526139169,13.573173805160993,13.426038846469792,13.286271087120397,13.15315632158569,13.027433511756934,12.906227304776724,12.788177727169526,12.677109448075866,12.571381707776958,12.470071050004343,12.373666941015342,12.283404774346558,12.195281475146752,12.109342599965109,12.030598032498897,11.95366612591772,11.87959705119209,11.810572323229158,11.744606286051171,11.68061398760011,11.618905377896873,11.559764006231608,11.502330249856797,11.44803514704668,11.395991129324749,11.34829356552641,11.291235928350485,11.249168010101538,11.20450541524661,11.164775244208702,11.122719786960483,11.083235866273421,11.048409077780061,11.013488042870705,10.979705247355986,10.9453886732639,10.914067169621902,10.884011494797203,10.855909554678174,10.827407357970737,10.801702928850643,10.776271884484817,10.75152592490033,10.729037652714881,10.706113052432308,10.684485499472734,10.663327831475685,10.643385488687064,10.623856987496536,10.605453396624096,10.587413996490158,10.570443959830845,10.554165345027531,10.538095353276168,10.522869603344198,10.508234355999269,10.494178656999527,10.480680001707535,10.467704253249755,10.45437640084367,10.44261064538479,10.43111766323272,10.419878922658109,10.407390306654744,10.3967292645377,10.387138057110802,10.377779465862387,10.364881946145466,10.357267848393656,10.348796008632254,10.34083578027753,10.33318889768303,10.32579143926369,10.318739371390015,10.311975915478339,10.301777465656086,10.295518844172525,10.28950830296847,10.283736627166068,10.278186218136346,10.272779959936642,10.267210359270978,10.262288975559294,10.257567775011085,10.253111295057149,10.249037783792716,10.245209107485623,10.241074620324568,10.237169880517628,10.233409863496851,10.229801737256572,10.226226182300875,10.222899171626747,10.219233247063814,10.217641671788172,10.214672247491142,10.211818869267143,10.209076997394396,10.20643585423491,10.20389886019656,10.201454000609592,10.199102555916953,10.196840698855235,10.194659027440158,10.19218296206602,10.190144708220226,10.188184429020305,10.18629259749124,10.184471221867701,10.182709011864704,10.18100720371163,10.179367801222842,10.177779057503555,10.176240242590382,10.17475399771743,10.1733171521785,10.171920727871047,10.170564891372981,10.169455358142892,10.168128326808118,10.166884566152028,10.165763234262673,10.164563158934076,10.163392425007403,10.162249410021204,10.161313793342448,10.160220874639949,10.159150046636448,10.158099261015643,10.157044096863144,10.156030514224462,10.155121702236238,10.154088066590075,10.153107110554368,10.15213803204349,10.15118005414646,10.15019829337272,10.149249035344889,10.14830652184864,10.147394769986235,10.146447859511733,10.1455161168613,10.144586751932536,10.143657824536891,10.142564069820699,10.141557004455906,10.140637706713333,10.139714016262445,10.138785834569894,10.137852358872767,10.136912815487667,10.13589999573361,10.135011768521386,10.134057282932982,10.133094124446924,10.132254618625103,10.131250561595586,10.130237464474435,10.129214222774623,10.128180244368066,10.127134962173594,10.126077833430791,10.12500833900378,10.12392598270967,10.122830290670537,10.12172081068766,10.120597111636817,10.11945877400829,10.118305047171111,10.117137934131819,10.115953510518942,10.114719313965944,10.11350548660699,10.11227491669205,10.111027318733711,10.109762430115845,10.108480148281801,10.107179417505622,10.105861168095414,10.104528797444212,10.102898720983143,10.101427328658348,10.100084951215756,10.09867875875001,10.097257551081167,10.095817946037636,10.094359796402474,10.092882965911006,10.091387331699147,10.089244972333274,10.087731517192974,10.086198908114637,10.084645633037532,10.083073452466381,10.081481646512097,10.07982668702653,10.078198816687918,10.07655123722421,10.074883959249332,10.073197456121589,10.07148996252069,10.069763649369877,10.06801775784332,10.066252327810464,10.064467930191887,10.062662652343413,10.060822117383507,10.058980342516262,10.057119902695907,10.055238778702767,10.053339527475307,10.05147230129323,10.049615973308851,10.047658395802449,10.045682889795772,10.043690146070647,10.041677936601847,10.039649011488585,10.037502298319092,10.03540460974043,10.033511206972914,10.031174176950447,10.028959908726701,10.026778345385198,10.024607781490573,10.022473277609464,10.020077423759073,10.017870038444435,10.015649308509346,10.013415338648935,10.011167142867706,10.00890525304517,10.006307099784342,10.004025636073896,10.001716368617721,9.999393894842713,9.997058310935582,9.994709718191835,9.992265626913717,9.98989619362748,9.987516283956209,9.985123729681733,9.982718658039715,9.98030119925794,9.977871486439422,9.975429655390206,9.972975844453646,9.970510194350975,9.968032848027724,9.965543950505955,9.963043648741857,9.960532091488536,9.958009429163864,9.955475813723098,9.952931398536105,9.95037633826899,9.947810788769988,9.94523490695946,9.942648850723812,9.940052778813266,9.937446850743168,9.934831226698948,9.933330887067061,9.930118237067271,9.927412419263085,9.92475984917461,9.923307559070867,9.919483591822942,9.916739811786586,9.91405172055917,9.912551817724832,9.908709938958749,9.90535409024436,9.90263150930408,9.899924593636712,9.89720851777096,9.895859909264034,9.891995269913838,9.889271340900487,9.886543062411011,9.883672349589625,9.88163752768789,9.879708525462094,9.87885988413467,9.876153772986346,9.873458451390743,9.87066843498344,9.868048295088222,9.864580328154531,9.861725529985751,9.85893452403523,9.856223661209972,9.853605407462558,9.85080844916547,9.8480784083181,9.845356191360946,9.842618993396565,9.839977799188274,9.837140249955926,9.835003565989565,9.83227822644992,9.82950363669705,9.826829419517004,9.823960136586809,9.821173171014665,9.818479527462774,9.815602709892028,9.812805462629214,9.810009997544281,9.80725240208412,9.804305747082095,9.801561498351006,9.79855003805247,9.795758646641017,9.792972155127282,9.790192587605194,9.78740602285277,9.784625797467747,9.781838559423278,9.778894532907842,9.77610706223789,9.773316994028358,9.770524727618875,9.767730400477856,9.76457790299284,9.761762210765982,9.758964174807094,9.756103860700764,9.753304975165902,9.750509203904825,9.748617578485838,9.745765056517671,9.743674688845651,9.74017716909019,9.737322077407065,9.735228982869833,9.731731264173042,9.72887642096454,9.726056529601042,9.723963633679029,9.720466215688138,9.717606230796909,9.71551469802091,9.71201496585399,9.70915971942234,9.706341217706035,9.704252792745343,9.700755961773304,9.697898338508255,9.695697138219087,9.69231198600102,9.689464134562924,9.687265006547982,9.68388422826562,9.680674112666964,9.678454200459953,9.674967537841049,9.672126370753002,9.670054700121863,9.66657810095624,9.663739444056453,9.661675122126299,9.658203932496493,9.655493126764553,9.652592323339777,9.650535564477067,9.647061184724988,9.644700295369267,9.64134858505072,9.638555909894173,9.636396219255433,9.633059807468184,9.630273155984327,9.62811934215313,9.624790173841687,9.622580179460686,9.619620167851075,9.61685721397733,9.614110946311325,9.611367330698807,9.608626420126827,9.605888266076601,9.603152918804708,9.60042042736702,9.597690839642361,9.594834448627882,9.592233348649065,9.589521215142868,9.58680357816768,9.584089072364762,9.58137773870386,9.578669617262179,9.575964747117773,9.57326316636888,9.570564912152525,9.56787002066312,9.565044169850035,9.562484313068703,9.559806706235424,9.557126718710162,9.554448597844997,9.55177394397437,9.549106161638871,9.549116898987897,9.546452584394261,9.543789631974796,9.54113255578869,9.538476705360871,9.535823692634093,9.533173672974229,9.53052670330881,9.528726335586546,9.5254103487968,9.523389366255264,9.52076322054402,9.518205576939968,9.515760245010954,9.513871352056519,9.510580901079232,9.507930789253898,9.505329800678393,9.502732555855395,9.500862419114123,9.497593264997564,9.494960719194426,9.492379208484095,9.489803251848041,9.48722755075264,9.485383821313066,9.482135181429753,9.479526637852224,9.476970801079032,9.474416995681073,9.472501086128515,9.469269620808891,9.466764505878508,9.464225045376146,9.461693452376277,"
     ]
    }
   ],
   "source": [
    "for elem in np.array(loss1):\n",
    "    print(str(elem[0])+',' , end = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
