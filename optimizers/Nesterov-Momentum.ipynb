{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/havish/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [149.31813844]\n",
      "Epoch: 1 Loss: [79.4953995]\n",
      "Epoch: 2 Loss: [55.03386392]\n",
      "Epoch: 3 Loss: [43.29602324]\n",
      "Epoch: 4 Loss: [35.89935253]\n",
      "Epoch: 5 Loss: [30.42329365]\n",
      "Epoch: 6 Loss: [26.61212873]\n",
      "Epoch: 7 Loss: [23.61624474]\n",
      "Epoch: 8 Loss: [20.92817856]\n",
      "Epoch: 9 Loss: [18.89686713]\n",
      "Epoch: 10 Loss: [17.4601471]\n",
      "Epoch: 11 Loss: [16.7060665]\n",
      "Epoch: 12 Loss: [16.65663074]\n",
      "Epoch: 13 Loss: [17.01932292]\n",
      "Epoch: 14 Loss: [17.3201812]\n",
      "Epoch: 15 Loss: [17.34886525]\n",
      "Epoch: 16 Loss: [17.19175278]\n",
      "Epoch: 17 Loss: [17.03936046]\n",
      "Epoch: 18 Loss: [16.95252009]\n",
      "Epoch: 19 Loss: [16.55621493]\n",
      "Epoch: 20 Loss: [16.37318168]\n",
      "Epoch: 21 Loss: [16.53026483]\n",
      "Epoch: 22 Loss: [16.5047715]\n",
      "Epoch: 23 Loss: [16.72376626]\n",
      "Epoch: 24 Loss: [16.93312372]\n",
      "Epoch: 25 Loss: [17.09006778]\n",
      "Epoch: 26 Loss: [17.37781817]\n",
      "Epoch: 27 Loss: [18.54040227]\n",
      "Epoch: 28 Loss: [20.85927694]\n",
      "Epoch: 29 Loss: [21.36685891]\n",
      "Epoch: 30 Loss: [23.88629469]\n",
      "Epoch: 31 Loss: [22.37920357]\n",
      "Epoch: 32 Loss: [21.76182661]\n",
      "Epoch: 33 Loss: [19.26395112]\n",
      "Epoch: 34 Loss: [16.27651753]\n",
      "Epoch: 35 Loss: [14.2568839]\n",
      "Epoch: 36 Loss: [14.70771785]\n",
      "Epoch: 37 Loss: [14.21281967]\n",
      "Epoch: 38 Loss: [14.11084709]\n",
      "Epoch: 39 Loss: [14.53670651]\n",
      "Epoch: 40 Loss: [13.79234873]\n",
      "Epoch: 41 Loss: [13.70689269]\n",
      "Epoch: 42 Loss: [13.94414045]\n",
      "Epoch: 43 Loss: [13.44597322]\n",
      "Epoch: 44 Loss: [13.51971801]\n",
      "Epoch: 45 Loss: [13.44212743]\n",
      "Epoch: 46 Loss: [13.29609648]\n",
      "Epoch: 47 Loss: [13.33145155]\n",
      "Epoch: 48 Loss: [13.52504135]\n",
      "Epoch: 49 Loss: [13.18749063]\n",
      "Epoch: 50 Loss: [13.2925833]\n",
      "Epoch: 51 Loss: [13.13715327]\n",
      "Epoch: 52 Loss: [13.22842761]\n",
      "Epoch: 53 Loss: [13.1217451]\n",
      "Epoch: 54 Loss: [13.02753111]\n",
      "Epoch: 55 Loss: [13.11761416]\n",
      "Epoch: 56 Loss: [12.94864203]\n",
      "Epoch: 57 Loss: [13.06503232]\n",
      "Epoch: 58 Loss: [12.88631593]\n",
      "Epoch: 59 Loss: [12.99667513]\n",
      "Epoch: 60 Loss: [12.79312834]\n",
      "Epoch: 61 Loss: [12.95495913]\n",
      "Epoch: 62 Loss: [12.77155138]\n",
      "Epoch: 63 Loss: [12.80996729]\n",
      "Epoch: 64 Loss: [12.73291835]\n",
      "Epoch: 65 Loss: [12.86977466]\n",
      "Epoch: 66 Loss: [12.69981623]\n",
      "Epoch: 67 Loss: [12.75060908]\n",
      "Epoch: 68 Loss: [12.77838291]\n",
      "Epoch: 69 Loss: [12.74360572]\n",
      "Epoch: 70 Loss: [12.61443694]\n",
      "Epoch: 71 Loss: [12.7954424]\n",
      "Epoch: 72 Loss: [12.60120315]\n",
      "Epoch: 73 Loss: [12.69919485]\n",
      "Epoch: 74 Loss: [12.72212855]\n",
      "Epoch: 75 Loss: [12.61752583]\n",
      "Epoch: 76 Loss: [12.7100727]\n",
      "Epoch: 77 Loss: [12.65035681]\n",
      "Epoch: 78 Loss: [12.61812345]\n",
      "Epoch: 79 Loss: [12.79856199]\n",
      "Epoch: 80 Loss: [12.51434182]\n",
      "Epoch: 81 Loss: [12.80914389]\n",
      "Epoch: 82 Loss: [12.52754381]\n",
      "Epoch: 83 Loss: [12.70180971]\n",
      "Epoch: 84 Loss: [12.72788342]\n",
      "Epoch: 85 Loss: [12.51674178]\n",
      "Epoch: 86 Loss: [12.76170603]\n",
      "Epoch: 87 Loss: [12.69137579]\n",
      "Epoch: 88 Loss: [12.53071929]\n",
      "Epoch: 89 Loss: [12.76715938]\n",
      "Epoch: 90 Loss: [12.47213453]\n",
      "Epoch: 91 Loss: [12.76025736]\n",
      "Epoch: 92 Loss: [12.66393632]\n",
      "Epoch: 93 Loss: [12.52278659]\n",
      "Epoch: 94 Loss: [12.73684894]\n",
      "Epoch: 95 Loss: [12.63864681]\n",
      "Epoch: 96 Loss: [12.58860892]\n",
      "Epoch: 97 Loss: [12.67697245]\n",
      "Epoch: 98 Loss: [12.54491132]\n",
      "Epoch: 99 Loss: [12.71500385]\n",
      "Epoch: 100 Loss: [12.64429416]\n",
      "Epoch: 101 Loss: [12.5692443]\n",
      "Epoch: 102 Loss: [12.63479681]\n",
      "Epoch: 103 Loss: [12.82483546]\n",
      "Epoch: 104 Loss: [12.28243928]\n",
      "Epoch: 105 Loss: [13.03749819]\n",
      "Epoch: 106 Loss: [12.31608545]\n",
      "Epoch: 107 Loss: [12.82154986]\n",
      "Epoch: 108 Loss: [12.35105312]\n",
      "Epoch: 109 Loss: [12.79917321]\n",
      "Epoch: 110 Loss: [12.36638735]\n",
      "Epoch: 111 Loss: [13.01485576]\n",
      "Epoch: 112 Loss: [12.16775455]\n",
      "Epoch: 113 Loss: [13.04325449]\n",
      "Epoch: 114 Loss: [12.1600836]\n",
      "Epoch: 115 Loss: [13.05858071]\n",
      "Epoch: 116 Loss: [12.21538144]\n",
      "Epoch: 117 Loss: [12.88787839]\n",
      "Epoch: 118 Loss: [12.50257183]\n",
      "Epoch: 119 Loss: [12.86314602]\n",
      "Epoch: 120 Loss: [12.58728361]\n",
      "Epoch: 121 Loss: [12.82093756]\n",
      "Epoch: 122 Loss: [12.74367416]\n",
      "Epoch: 123 Loss: [12.55655739]\n",
      "Epoch: 124 Loss: [12.89126506]\n",
      "Epoch: 125 Loss: [12.88552143]\n",
      "Epoch: 126 Loss: [12.66705335]\n",
      "Epoch: 127 Loss: [12.9265823]\n",
      "Epoch: 128 Loss: [12.80078776]\n",
      "Epoch: 129 Loss: [12.83217054]\n",
      "Epoch: 130 Loss: [12.96494064]\n",
      "Epoch: 131 Loss: [12.57105103]\n",
      "Epoch: 132 Loss: [12.99688607]\n",
      "Epoch: 133 Loss: [12.87935633]\n",
      "Epoch: 134 Loss: [12.93534537]\n",
      "Epoch: 135 Loss: [12.93932715]\n",
      "Epoch: 136 Loss: [12.95682101]\n",
      "Epoch: 137 Loss: [12.97270082]\n",
      "Epoch: 138 Loss: [12.98999117]\n",
      "Epoch: 139 Loss: [13.00889412]\n",
      "Epoch: 140 Loss: [13.00376455]\n",
      "Epoch: 141 Loss: [12.83374048]\n",
      "Epoch: 142 Loss: [13.11343201]\n",
      "Epoch: 143 Loss: [13.11110349]\n",
      "Epoch: 144 Loss: [13.11980789]\n",
      "Epoch: 145 Loss: [13.13680028]\n",
      "Epoch: 146 Loss: [13.18789763]\n",
      "Epoch: 147 Loss: [13.21310297]\n",
      "Epoch: 148 Loss: [13.21886736]\n",
      "Epoch: 149 Loss: [13.0156961]\n",
      "Epoch: 150 Loss: [13.35008697]\n",
      "Epoch: 151 Loss: [13.36810306]\n",
      "Epoch: 152 Loss: [13.40687492]\n",
      "Epoch: 153 Loss: [13.43587643]\n",
      "Epoch: 154 Loss: [13.49104799]\n",
      "Epoch: 155 Loss: [13.69814965]\n",
      "Epoch: 156 Loss: [13.51638523]\n",
      "Epoch: 157 Loss: [13.57657617]\n",
      "Epoch: 158 Loss: [13.64572689]\n",
      "Epoch: 159 Loss: [13.68817763]\n",
      "Epoch: 160 Loss: [13.710591]\n",
      "Epoch: 161 Loss: [13.71341788]\n",
      "Epoch: 162 Loss: [12.92329522]\n",
      "Epoch: 163 Loss: [12.75158971]\n",
      "Epoch: 164 Loss: [12.56645568]\n",
      "Epoch: 165 Loss: [12.52620113]\n",
      "Epoch: 166 Loss: [12.54072499]\n",
      "Epoch: 167 Loss: [12.55661291]\n",
      "Epoch: 168 Loss: [12.45557997]\n",
      "Epoch: 169 Loss: [12.62574731]\n",
      "Epoch: 170 Loss: [12.47111811]\n",
      "Epoch: 171 Loss: [12.63006648]\n",
      "Epoch: 172 Loss: [12.47104844]\n",
      "Epoch: 173 Loss: [12.80354823]\n",
      "Epoch: 174 Loss: [12.3393835]\n",
      "Epoch: 175 Loss: [12.59087797]\n",
      "Epoch: 176 Loss: [12.54495671]\n",
      "Epoch: 177 Loss: [12.70588876]\n",
      "Epoch: 178 Loss: [12.42511416]\n",
      "Epoch: 179 Loss: [12.59126606]\n",
      "Epoch: 180 Loss: [12.35325678]\n",
      "Epoch: 181 Loss: [12.39885402]\n",
      "Epoch: 182 Loss: [12.43711153]\n",
      "Epoch: 183 Loss: [12.43247382]\n",
      "Epoch: 184 Loss: [12.44405444]\n",
      "Epoch: 185 Loss: [12.28702917]\n",
      "Epoch: 186 Loss: [12.57802212]\n",
      "Epoch: 187 Loss: [12.27101112]\n",
      "Epoch: 188 Loss: [12.35475626]\n",
      "Epoch: 189 Loss: [12.69484682]\n",
      "Epoch: 190 Loss: [12.28032727]\n",
      "Epoch: 191 Loss: [12.4882174]\n",
      "Epoch: 192 Loss: [12.25875634]\n",
      "Epoch: 193 Loss: [12.35756021]\n",
      "Epoch: 194 Loss: [12.18336203]\n",
      "Epoch: 195 Loss: [12.39312826]\n",
      "Epoch: 196 Loss: [12.2730024]\n",
      "Epoch: 197 Loss: [12.20570405]\n",
      "Epoch: 198 Loss: [12.19208148]\n",
      "Epoch: 199 Loss: [12.47944416]\n",
      "Epoch: 200 Loss: [12.2012513]\n",
      "Epoch: 201 Loss: [12.37551621]\n",
      "Epoch: 202 Loss: [12.10510192]\n",
      "Epoch: 203 Loss: [12.31290943]\n",
      "Epoch: 204 Loss: [12.20882736]\n",
      "Epoch: 205 Loss: [12.15345812]\n",
      "Epoch: 206 Loss: [12.11339765]\n",
      "Epoch: 207 Loss: [12.39358263]\n",
      "Epoch: 208 Loss: [12.15615992]\n",
      "Epoch: 209 Loss: [12.15520677]\n",
      "Epoch: 210 Loss: [12.24663096]\n",
      "Epoch: 211 Loss: [12.10484331]\n",
      "Epoch: 212 Loss: [12.21423025]\n",
      "Epoch: 213 Loss: [12.10677986]\n",
      "Epoch: 214 Loss: [12.06288899]\n",
      "Epoch: 215 Loss: [11.93626181]\n",
      "Epoch: 216 Loss: [12.2211496]\n",
      "Epoch: 217 Loss: [12.06843152]\n",
      "Epoch: 218 Loss: [12.15198505]\n",
      "Epoch: 219 Loss: [12.04970267]\n",
      "Epoch: 220 Loss: [11.97744684]\n",
      "Epoch: 221 Loss: [11.87394309]\n",
      "Epoch: 222 Loss: [12.13162163]\n",
      "Epoch: 223 Loss: [12.05980583]\n",
      "Epoch: 224 Loss: [12.11473957]\n",
      "Epoch: 225 Loss: [12.05195202]\n",
      "Epoch: 226 Loss: [11.87477936]\n",
      "Epoch: 227 Loss: [11.82964566]\n",
      "Epoch: 228 Loss: [11.82016291]\n",
      "Epoch: 229 Loss: [11.79794523]\n",
      "Epoch: 230 Loss: [12.02712726]\n",
      "Epoch: 231 Loss: [11.99611151]\n",
      "Epoch: 232 Loss: [11.80096929]\n",
      "Epoch: 233 Loss: [11.75221625]\n",
      "Epoch: 234 Loss: [11.97613343]\n",
      "Epoch: 235 Loss: [11.95729919]\n",
      "Epoch: 236 Loss: [11.75547185]\n",
      "Epoch: 237 Loss: [11.69977219]\n",
      "Epoch: 238 Loss: [11.91878781]\n",
      "Epoch: 239 Loss: [11.91205563]\n",
      "Epoch: 240 Loss: [11.69869607]\n",
      "Epoch: 241 Loss: [11.63319783]\n",
      "Epoch: 242 Loss: [11.7234535]\n",
      "Epoch: 243 Loss: [11.57825998]\n",
      "Epoch: 244 Loss: [11.93867143]\n",
      "Epoch: 245 Loss: [11.83925029]\n",
      "Epoch: 246 Loss: [11.8148269]\n",
      "Epoch: 247 Loss: [11.8429019]\n",
      "Epoch: 248 Loss: [11.80505479]\n",
      "Epoch: 249 Loss: [11.81676392]\n",
      "Epoch: 250 Loss: [11.78563382]\n",
      "Epoch: 251 Loss: [11.78522121]\n",
      "Epoch: 252 Loss: [11.7641249]\n",
      "Epoch: 253 Loss: [11.75115478]\n",
      "Epoch: 254 Loss: [11.73934193]\n",
      "Epoch: 255 Loss: [11.71531636]\n",
      "Epoch: 256 Loss: [11.71395309]\n",
      "Epoch: 257 Loss: [11.49300395]\n",
      "Epoch: 258 Loss: [11.45481012]\n",
      "Epoch: 259 Loss: [11.46102269]\n",
      "Epoch: 260 Loss: [11.38342465]\n",
      "Epoch: 261 Loss: [11.57201034]\n",
      "Epoch: 262 Loss: [11.49088178]\n",
      "Epoch: 263 Loss: [11.4675175]\n",
      "Epoch: 264 Loss: [11.53903511]\n",
      "Epoch: 265 Loss: [11.47526393]\n",
      "Epoch: 266 Loss: [11.48938282]\n",
      "Epoch: 267 Loss: [11.43462802]\n",
      "Epoch: 268 Loss: [11.49745997]\n",
      "Epoch: 269 Loss: [11.42623088]\n",
      "Epoch: 270 Loss: [11.35586217]\n",
      "Epoch: 271 Loss: [11.32526435]\n",
      "Epoch: 272 Loss: [11.33993623]\n",
      "Epoch: 273 Loss: [11.38269272]\n",
      "Epoch: 274 Loss: [11.34595518]\n",
      "Epoch: 275 Loss: [11.33872398]\n",
      "Epoch: 276 Loss: [11.30366644]\n",
      "Epoch: 277 Loss: [11.34913067]\n",
      "Epoch: 278 Loss: [11.30155478]\n",
      "Epoch: 279 Loss: [11.21144427]\n",
      "Epoch: 280 Loss: [11.21388526]\n",
      "Epoch: 281 Loss: [11.20106608]\n",
      "Epoch: 282 Loss: [11.16667999]\n",
      "Epoch: 283 Loss: [11.19116738]\n",
      "Epoch: 284 Loss: [11.24538707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285 Loss: [11.21816087]\n",
      "Epoch: 286 Loss: [11.10487591]\n",
      "Epoch: 287 Loss: [11.11395062]\n",
      "Epoch: 288 Loss: [11.09403861]\n",
      "Epoch: 289 Loss: [11.14808922]\n",
      "Epoch: 290 Loss: [11.10149562]\n",
      "Epoch: 291 Loss: [11.22659163]\n",
      "Epoch: 292 Loss: [11.19375081]\n",
      "Epoch: 293 Loss: [11.12269632]\n",
      "Epoch: 294 Loss: [11.10149651]\n",
      "Epoch: 295 Loss: [11.06539252]\n",
      "Epoch: 296 Loss: [11.0562233]\n",
      "Epoch: 297 Loss: [11.08777657]\n",
      "Epoch: 298 Loss: [11.07057431]\n",
      "Epoch: 299 Loss: [10.93287441]\n",
      "Epoch: 300 Loss: [10.96319339]\n",
      "Epoch: 301 Loss: [10.915873]\n",
      "Epoch: 302 Loss: [10.87614023]\n",
      "Epoch: 303 Loss: [11.17192037]\n",
      "Epoch: 304 Loss: [10.87700395]\n",
      "Epoch: 305 Loss: [10.94379746]\n",
      "Epoch: 306 Loss: [10.95258102]\n",
      "Epoch: 307 Loss: [10.89885206]\n",
      "Epoch: 308 Loss: [10.90864153]\n",
      "Epoch: 309 Loss: [10.87294622]\n",
      "Epoch: 310 Loss: [11.01971073]\n",
      "Epoch: 311 Loss: [11.04573319]\n",
      "Epoch: 312 Loss: [10.80863962]\n",
      "Epoch: 313 Loss: [11.01957094]\n",
      "Epoch: 314 Loss: [11.00475648]\n",
      "Epoch: 315 Loss: [10.72580641]\n",
      "Epoch: 316 Loss: [10.84775573]\n",
      "Epoch: 317 Loss: [11.03241869]\n",
      "Epoch: 318 Loss: [10.74482087]\n",
      "Epoch: 319 Loss: [10.76439449]\n",
      "Epoch: 320 Loss: [10.67554313]\n",
      "Epoch: 321 Loss: [10.6565986]\n",
      "Epoch: 322 Loss: [10.85028513]\n",
      "Epoch: 323 Loss: [10.79404011]\n",
      "Epoch: 324 Loss: [10.72938799]\n",
      "Epoch: 325 Loss: [10.86907952]\n",
      "Epoch: 326 Loss: [10.65787829]\n",
      "Epoch: 327 Loss: [10.74921107]\n",
      "Epoch: 328 Loss: [10.73394773]\n",
      "Epoch: 329 Loss: [10.74146205]\n",
      "Epoch: 330 Loss: [10.79510012]\n",
      "Epoch: 331 Loss: [10.76548332]\n",
      "Epoch: 332 Loss: [10.5570672]\n",
      "Epoch: 333 Loss: [10.63625163]\n",
      "Epoch: 334 Loss: [10.75997881]\n",
      "Epoch: 335 Loss: [10.77961707]\n",
      "Epoch: 336 Loss: [10.53781095]\n",
      "Epoch: 337 Loss: [10.59210083]\n",
      "Epoch: 338 Loss: [10.58342087]\n",
      "Epoch: 339 Loss: [10.61082901]\n",
      "Epoch: 340 Loss: [10.67324651]\n",
      "Epoch: 341 Loss: [10.66035356]\n",
      "Epoch: 342 Loss: [10.44229417]\n",
      "Epoch: 343 Loss: [10.4924851]\n",
      "Epoch: 344 Loss: [10.62210876]\n",
      "Epoch: 345 Loss: [10.62692758]\n",
      "Epoch: 346 Loss: [10.40828404]\n",
      "Epoch: 347 Loss: [10.44129605]\n",
      "Epoch: 348 Loss: [10.49583137]\n",
      "Epoch: 349 Loss: [10.53412773]\n",
      "Epoch: 350 Loss: [10.48802437]\n",
      "Epoch: 351 Loss: [10.47454247]\n",
      "Epoch: 352 Loss: [10.30752146]\n",
      "Epoch: 353 Loss: [10.39092386]\n",
      "Epoch: 354 Loss: [10.51778713]\n",
      "Epoch: 355 Loss: [10.46524856]\n",
      "Epoch: 356 Loss: [10.27840058]\n",
      "Epoch: 357 Loss: [10.34158068]\n",
      "Epoch: 358 Loss: [10.35816164]\n",
      "Epoch: 359 Loss: [10.40787894]\n",
      "Epoch: 360 Loss: [10.33689266]\n",
      "Epoch: 361 Loss: [10.33153951]\n",
      "Epoch: 362 Loss: [10.3645278]\n",
      "Epoch: 363 Loss: [10.30549142]\n",
      "Epoch: 364 Loss: [10.30819325]\n",
      "Epoch: 365 Loss: [10.17003312]\n",
      "Epoch: 366 Loss: [10.17172652]\n",
      "Epoch: 367 Loss: [10.43665693]\n",
      "Epoch: 368 Loss: [10.28740076]\n",
      "Epoch: 369 Loss: [10.20634933]\n",
      "Epoch: 370 Loss: [10.29482875]\n",
      "Epoch: 371 Loss: [10.24846161]\n",
      "Epoch: 372 Loss: [10.10569726]\n",
      "Epoch: 373 Loss: [10.08339197]\n",
      "Epoch: 374 Loss: [10.36064833]\n",
      "Epoch: 375 Loss: [10.18953186]\n",
      "Epoch: 376 Loss: [10.11223553]\n",
      "Epoch: 377 Loss: [10.34214383]\n",
      "Epoch: 378 Loss: [10.16782472]\n",
      "Epoch: 379 Loss: [10.09367334]\n",
      "Epoch: 380 Loss: [10.1660327]\n",
      "Epoch: 381 Loss: [10.20273247]\n",
      "Epoch: 382 Loss: [10.07757991]\n",
      "Epoch: 383 Loss: [10.13132]\n",
      "Epoch: 384 Loss: [10.02403862]\n",
      "Epoch: 385 Loss: [10.15487115]\n",
      "Epoch: 386 Loss: [10.00473668]\n",
      "Epoch: 387 Loss: [10.08088559]\n",
      "Epoch: 388 Loss: [10.10324407]\n",
      "Epoch: 389 Loss: [9.9758777]\n",
      "Epoch: 390 Loss: [10.05393425]\n",
      "Epoch: 391 Loss: [9.92814166]\n",
      "Epoch: 392 Loss: [10.07036098]\n",
      "Epoch: 393 Loss: [9.90988097]\n",
      "Epoch: 394 Loss: [10.00817124]\n",
      "Epoch: 395 Loss: [10.02389006]\n",
      "Epoch: 396 Loss: [9.88539298]\n",
      "Epoch: 397 Loss: [9.98445686]\n",
      "Epoch: 398 Loss: [9.99057088]\n",
      "Epoch: 399 Loss: [9.85796494]\n",
      "Epoch: 400 Loss: [9.96109565]\n",
      "Epoch: 401 Loss: [9.95915195]\n",
      "Epoch: 402 Loss: [9.8276745]\n",
      "Epoch: 403 Loss: [9.93671086]\n",
      "Epoch: 404 Loss: [9.92828703]\n",
      "Epoch: 405 Loss: [9.79552797]\n",
      "Epoch: 406 Loss: [9.91308044]\n",
      "Epoch: 407 Loss: [9.89838466]\n",
      "Epoch: 408 Loss: [9.8606197]\n",
      "Epoch: 409 Loss: [9.75421]\n",
      "Epoch: 410 Loss: [10.00492673]\n",
      "Epoch: 411 Loss: [9.74637592]\n",
      "Epoch: 412 Loss: [9.85815546]\n",
      "Epoch: 413 Loss: [9.81794582]\n",
      "Epoch: 414 Loss: [9.72395924]\n",
      "Epoch: 415 Loss: [9.93134841]\n",
      "Epoch: 416 Loss: [9.64934899]\n",
      "Epoch: 417 Loss: [9.77305993]\n",
      "Epoch: 418 Loss: [9.7418479]\n",
      "Epoch: 419 Loss: [9.61045096]\n",
      "Epoch: 420 Loss: [9.893552]\n",
      "Epoch: 421 Loss: [9.60094033]\n",
      "Epoch: 422 Loss: [9.82867447]\n",
      "Epoch: 423 Loss: [9.60311291]\n",
      "Epoch: 424 Loss: [9.67845832]\n",
      "Epoch: 425 Loss: [9.65087156]\n",
      "Epoch: 426 Loss: [9.51259607]\n",
      "Epoch: 427 Loss: [9.81706947]\n",
      "Epoch: 428 Loss: [9.50345222]\n",
      "Epoch: 429 Loss: [9.75449026]\n",
      "Epoch: 430 Loss: [9.54897501]\n",
      "Epoch: 431 Loss: [9.74013013]\n",
      "Epoch: 432 Loss: [9.52545594]\n",
      "Epoch: 433 Loss: [9.71236874]\n",
      "Epoch: 434 Loss: [9.49935008]\n",
      "Epoch: 435 Loss: [9.68648907]\n",
      "Epoch: 436 Loss: [9.47225086]\n",
      "Epoch: 437 Loss: [9.66204224]\n",
      "Epoch: 438 Loss: [9.38606249]\n",
      "Epoch: 439 Loss: [9.62320912]\n",
      "Epoch: 440 Loss: [9.42343591]\n",
      "Epoch: 441 Loss: [9.62127369]\n",
      "Epoch: 442 Loss: [9.39864728]\n",
      "Epoch: 443 Loss: [9.5973386]\n",
      "Epoch: 444 Loss: [9.37247062]\n",
      "Epoch: 445 Loss: [9.57619651]\n",
      "Epoch: 446 Loss: [9.28934354]\n",
      "Epoch: 447 Loss: [9.54251812]\n",
      "Epoch: 448 Loss: [9.32561197]\n",
      "Epoch: 449 Loss: [9.54469084]\n",
      "Epoch: 450 Loss: [9.30341854]\n",
      "Epoch: 451 Loss: [9.54248017]\n",
      "Epoch: 452 Loss: [9.29822409]\n",
      "Epoch: 453 Loss: [9.51684404]\n",
      "Epoch: 454 Loss: [9.26623555]\n",
      "Epoch: 455 Loss: [9.49412389]\n",
      "Epoch: 456 Loss: [9.24139247]\n",
      "Epoch: 457 Loss: [9.47615311]\n",
      "Epoch: 458 Loss: [9.217231]\n",
      "Epoch: 459 Loss: [9.47923989]\n",
      "Epoch: 460 Loss: [9.21085055]\n",
      "Epoch: 461 Loss: [9.45307773]\n",
      "Epoch: 462 Loss: [9.18027325]\n",
      "Epoch: 463 Loss: [9.43272613]\n",
      "Epoch: 464 Loss: [9.15694837]\n",
      "Epoch: 465 Loss: [9.41612404]\n",
      "Epoch: 466 Loss: [9.13414915]\n",
      "Epoch: 467 Loss: [9.42012672]\n",
      "Epoch: 468 Loss: [9.12902216]\n",
      "Epoch: 469 Loss: [9.44094292]\n",
      "Epoch: 470 Loss: [9.10637088]\n",
      "Epoch: 471 Loss: [9.3915676]\n",
      "Epoch: 472 Loss: [9.08921289]\n",
      "Epoch: 473 Loss: [9.41273963]\n",
      "Epoch: 474 Loss: [9.06696922]\n",
      "Epoch: 475 Loss: [9.36550222]\n",
      "Epoch: 476 Loss: [9.04668225]\n",
      "Epoch: 477 Loss: [9.3820405]\n",
      "Epoch: 478 Loss: [9.02881638]\n",
      "Epoch: 479 Loss: [9.32392269]\n",
      "Epoch: 480 Loss: [8.99730047]\n",
      "Epoch: 481 Loss: [9.32981023]\n",
      "Epoch: 482 Loss: [8.99180477]\n",
      "Epoch: 483 Loss: [9.34937736]\n",
      "Epoch: 484 Loss: [8.97125618]\n",
      "Epoch: 485 Loss: [9.28943513]\n",
      "Epoch: 486 Loss: [8.96789944]\n",
      "Epoch: 487 Loss: [9.01209304]\n",
      "Epoch: 488 Loss: [8.97850513]\n",
      "Epoch: 489 Loss: [9.03023358]\n",
      "Epoch: 490 Loss: [9.07967658]\n",
      "Epoch: 491 Loss: [8.94044774]\n",
      "Epoch: 492 Loss: [9.27042896]\n",
      "Epoch: 493 Loss: [8.92116942]\n",
      "Epoch: 494 Loss: [8.95206601]\n",
      "Epoch: 495 Loss: [8.94545025]\n",
      "Epoch: 496 Loss: [9.04772546]\n",
      "Epoch: 497 Loss: [8.89039363]\n",
      "Epoch: 498 Loss: [9.24200857]\n",
      "Epoch: 499 Loss: [8.84615223]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 10\n",
    "num_batches = num_samples/batch_size\n",
    "alpha = 0.7\n",
    "eps = 1e-3\n",
    "loss1 = []\n",
    "v = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        \n",
    "        W1 = W1 + alpha*v[\"W1\"]\n",
    "        b1 = b1 + alpha*v[\"b1\"]\n",
    "        W2 = W2 + alpha*v[\"W2\"]\n",
    "        b2 = b2 + alpha*v[\"b2\"]\n",
    "        W3 = W3 + alpha*v[\"W3\"]\n",
    "        b3 = b3 + alpha*v[\"b3\"]\n",
    "        for k in range(batch_size):\n",
    "            z1 = relu(np.matmul(W1,x_train_subs[j*batch_size+k]).reshape(-1,1)+b1)\n",
    "        \n",
    "            z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "\n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j*batch_size+k])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j*batch_size+k].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(z2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(z1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j*batch_size+k].reshape(-1,1).T)\n",
    "        \n",
    "        v[\"W1\"] = alpha*v[\"W1\"] - eps*(W1_upd)\n",
    "        v[\"W2\"] = alpha*v[\"W2\"] - eps*(W2_upd)\n",
    "        v[\"W3\"] = alpha*v[\"W3\"] - eps*(W3_upd)\n",
    "        v[\"b1\"] = alpha*v[\"b1\"] - eps*(b1_upd)\n",
    "        v[\"b2\"] = alpha*v[\"b2\"] - eps*(b2_upd)\n",
    "        v[\"b3\"] = alpha*v[\"b3\"] - eps*(b3_upd)\n",
    "        W3 = W3 + v[\"W3\"]\n",
    "        W2 = W2 + v[\"W2\"]\n",
    "        W1 = W1 + v[\"W1\"]\n",
    "        b3 = b3 + v[\"b3\"]\n",
    "        b2 = b2 + v[\"b2\"]\n",
    "        b1 = b1 + v[\"b1\"]\n",
    "    loss1.append(loss)\n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = relu(np.matmul(W1,test_x[4]).reshape(-1,1)+b1)\n",
    "z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.03508093e-06]\n",
      " [9.99051692e-01]\n",
      " [9.47273274e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "acc =accuracy_score(y_pred=preds,y_true=true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,501)\n",
    "plt.plot(epochs,loss1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149.31813844347164,79.4953994978193,55.033863921742864,43.296023241566616,35.89935253087307,30.42329365025284,26.612128727078744,23.616244742838564,20.928178562337305,18.89686713418274,17.460147095599517,16.70606649557535,16.65663074211073,17.019322920423978,17.320181200007248,17.34886524943121,17.191752775923142,17.03936046062264,16.952520089882654,16.55621493062584,16.373181682547468,16.53026483240457,16.504771496971628,16.723766255395724,16.933123717582355,17.090067783649467,17.377818168399678,18.540402267938827,20.859276936172044,21.366858914737932,23.886294690291546,22.379203569958722,21.76182660741379,19.2639511155233,16.276517525891485,14.256883898264046,14.707717850758375,14.21281966572102,14.11084708960717,14.536706505240781,13.79234873455856,13.706892686902266,13.944140451557761,13.445973218186742,13.519718008268324,13.442127433791788,13.296096476219752,13.331451546041617,13.525041349909788,13.187490633511404,13.29258329610067,13.137153268628659,13.228427608947257,13.121745103780661,13.02753110681596,13.117614157073172,12.948642029925745,13.065032321476965,12.886315927441789,12.996675131873758,12.79312833562806,12.954959129177075,12.771551377393353,12.809967292918252,12.732918350305468,12.86977466014998,12.699816228503893,12.75060907509081,12.778382912895054,12.743605717100024,12.614436943378687,12.795442404553553,12.601203145613628,12.699194851106192,12.722128550015185,12.617525828140275,12.710072700009398,12.650356809022643,12.618123446040446,12.79856198897767,12.514341817174875,12.809143893130033,12.527543806039713,12.7018097138515,12.72788341814632,12.516741775522116,12.761706031358788,12.691375789262358,12.530719285658524,12.767159384261658,12.472134530250349,12.760257361993455,12.663936315608995,12.522786592287257,12.736848938474646,12.638646805755071,12.58860891764314,12.676972452355193,12.544911323465158,12.715003848710385,12.644294159241358,12.569244301580818,12.63479681218523,12.824835459592396,12.282439284918912,13.037498189341674,12.316085452417534,12.821549858873297,12.351053121041762,12.79917321090618,12.366387349573927,13.014855755707268,12.167754549724078,13.043254492024877,12.16008360270619,13.05858071278477,12.215381442391207,12.887878394545064,12.502571833385138,12.86314602279577,12.587283605610242,12.820937562728808,12.743674161953729,12.556557388341062,12.891265055912415,12.885521434467538,12.667053350303181,12.9265822950616,12.800787764473808,12.83217054108183,12.964940640449491,12.571051028050844,12.996886073136075,12.87935632895894,12.935345368136144,12.939327149835263,12.956821012501742,12.972700821366082,12.989991171526766,13.00889411700441,13.00376455092546,12.833740476856592,13.113432009798666,13.11110349197036,13.119807886951417,13.136800284698522,13.187897629006216,13.213102968320804,13.218867358663195,13.015696100842026,13.350086968272521,13.368103063062998,13.406874918353484,13.435876432740141,13.491047992701395,13.698149648060086,13.516385230960445,13.57657616622331,13.645726893623229,13.688177633750989,13.710590999767932,13.713417875875097,12.923295217344446,12.751589714537845,12.566455679263973,12.526201133069948,12.540724990409986,12.556612909557574,12.45557996789612,12.625747311095829,12.471118109852965,12.630066478933216,12.471048435960684,12.80354822522304,12.339383495415687,12.590877967999974,12.544956709413254,12.70588876324766,12.425114160824473,12.5912660610152,12.353256784972269,12.39885401776009,12.437111529737022,12.432473816886471,12.44405444005452,12.28702916523315,12.578022119775987,12.271011119543058,12.354756262072033,12.694846816319915,12.280327272315503,12.488217399520792,12.258756336270224,12.35756021264403,12.183362030311615,12.393128256501056,12.27300240192419,12.205704050673205,12.192081482500594,12.479444161697735,12.20125130254076,12.375516214064286,12.105101922358111,12.3129094268698,12.208827364085458,12.153458118733939,12.113397652245949,12.393582626212595,12.15615992097996,12.155206767517098,12.246630961480431,12.104843309581486,12.214230254380317,12.106779862076387,12.062888985444966,11.936261805536855,12.221149604830147,12.068431520657802,12.151985048887452,12.049702669176199,11.97744684227009,11.873943086335034,12.131621629599037,12.059805825964213,12.114739569534049,12.051952021955952,11.874779357434566,11.829645664488366,11.820162908098794,11.797945234742905,12.02712726002334,11.99611151159241,11.800969285291394,11.752216245014417,11.976133433282355,11.957299189310376,11.755471850933738,11.699772187101033,11.918787809740492,11.91205562677642,11.698696068083411,11.633197825494477,11.72345350064874,11.578259977493829,11.938671434500652,11.83925028537809,11.814826900980675,11.842901897642193,11.805054794100796,11.816763917013711,11.785633818252656,11.78522120513149,11.764124895201517,11.751154780437334,11.739341933987882,11.715316355589845,11.713953092506095,11.49300395444133,11.454810115238066,11.461022694892586,11.383424650864102,11.572010337742366,11.490881775303244,11.467517498180126,11.539035114269952,11.475263925709418,11.489382820035358,11.43462801872215,11.497459965198187,11.426230878110662,11.355862168327816,11.32526434885993,11.339936234206725,11.38269272354859,11.34595517558309,11.338723979378438,11.303666444740076,11.349130668497411,11.301554784814536,11.211444268695521,11.213885262138556,11.201066082481798,11.166679986537877,11.191167376108814,11.245387074917545,11.21816087489715,11.104875909690167,11.113950622452467,11.094038611884889,11.148089217685472,11.101495615695866,11.226591626735527,11.193750810075327,11.122696323251832,11.101496513391593,11.065392521302883,11.056223300895184,11.087776573012817,11.070574311215047,10.932874412363502,10.963193386557847,10.915873001319552,10.876140227082962,11.171920373839907,10.877003948155274,10.943797458664625,10.952581016738861,10.89885206489782,10.908641529111156,10.872946217547165,11.019710729916516,11.045733188923505,10.80863962333942,11.019570940950478,11.004756483991999,10.725806414308957,10.84775572742074,11.032418692484063,10.744820865081053,10.764394494904089,10.675543131443545,10.656598595837753,10.850285126892397,10.794040108463744,10.72938799474388,10.869079524797176,10.657878290011501,10.749211071625185,10.733947726066509,10.741462051813063,10.795100119507419,10.765483324534582,10.557067197119794,10.636251626694818,10.759978813565079,10.779617071004356,10.537810947694618,10.592100825689956,10.583420866102475,10.610829010205718,10.673246512865912,10.660353556939334,10.442294173567083,10.492485104707848,10.622108758161422,10.62692758114281,10.408284036840312,10.441296045782702,10.495831373012035,10.534127728679378,10.488024367993047,10.474542468131167,10.307521462903875,10.390923861760307,10.51778712674954,10.465248561109258,10.278400583010756,10.34158067762997,10.358161642167605,10.407878935347755,10.336892661488493,10.331539514207604,10.364527804604526,10.30549141989871,10.308193253965985,10.170033122068878,10.171726522465471,10.436656929776328,10.287400759790735,10.20634933480933,10.294828750496876,10.248461609805855,10.105697261599177,10.083391969983406,10.360648334238416,10.189531864909078,10.112235526703467,10.342143832599549,10.167824717872284,10.093673337584635,10.166032704129298,10.202732472072853,10.077579910352691,10.131320002383497,10.024038618496522,10.154871145581277,10.004736678370834,10.08088559161806,10.103244066568678,9.975877697919374,10.05393424766568,9.928141662141408,10.070360976336357,9.909880974362723,10.008171244379923,10.023890061688281,9.885392983335588,9.98445686364169,9.990570878363313,9.857964941368236,9.961095649200738,9.959151945541683,9.827674500211074,9.936710855078609,9.928287028689804,9.79552796648016,9.913080435711231,9.898384655688387,9.860619702343042,9.754210003826282,10.004926734432184,9.74637591912395,9.85815546430326,9.817945821695798,9.723959244486823,9.931348405798927,9.649348992498718,9.773059931489001,9.741847895538132,9.610450956832244,9.893552004887306,9.600940330949898,9.828674474727631,9.603112914985642,9.678458316211806,9.650871558217428,9.51259607118817,9.817069472535387,9.503452215693441,9.754490259868605,9.548975013933562,9.740130130833595,9.525455940573043,9.712368743896036,9.49935007868151,9.68648906599062,9.472250864213168,9.662042243590186,9.386062487652076,9.623209122748813,9.423435913513462,9.621273691856402,9.39864727918056,9.597338600501926,9.37247061680011,9.576196509048506,9.289343541683435,9.54251812425202,9.325611970116453,9.544690836377312,9.303418541154372,9.542480169616795,9.298224091773685,9.516844038897169,9.26623555449885,9.494123888770071,9.241392468399956,9.476153110578046,9.217231004765246,9.479239888786836,9.21085055453193,9.453077731320008,9.180273249219727,9.432726127362473,9.156948371074462,9.416124040159751,9.134149153029899,9.420126716092891,9.1290221605928,9.44094292211013,9.106370883996567,9.391567599449582,9.089212891249646,9.412739625194286,9.06696921691133,9.36550221751263,9.04668224807996,9.38204050352285,9.028816381969627,9.323922691547937,8.997300471933581,9.329810227181671,8.991804767388066,9.349377356949434,8.97125618152084,9.28943512572513,8.967899444604502,9.012093035404064,8.97850513254383,9.030233579751165,9.079676576847525,8.940447742499094,9.270428958009772,8.921169419820343,8.95206600801766,8.945450251455679,9.047725461668552,8.890393629166493,9.2420085714539,8.846152232283858,"
     ]
    }
   ],
   "source": [
    "for elem in np.array(loss1):\n",
    "    print(str(elem[0])+',' , end = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
