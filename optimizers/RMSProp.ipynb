{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/havish/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [238.09232098]\n",
      "Epoch: 1 Loss: [207.19354434]\n",
      "Epoch: 2 Loss: [178.55530454]\n",
      "Epoch: 3 Loss: [155.10951554]\n",
      "Epoch: 4 Loss: [134.1552848]\n",
      "Epoch: 5 Loss: [115.80221066]\n",
      "Epoch: 6 Loss: [99.11284279]\n",
      "Epoch: 7 Loss: [84.01449088]\n",
      "Epoch: 8 Loss: [73.15397335]\n",
      "Epoch: 9 Loss: [65.53428186]\n",
      "Epoch: 10 Loss: [60.03745119]\n",
      "Epoch: 11 Loss: [55.99601421]\n",
      "Epoch: 12 Loss: [52.93065498]\n",
      "Epoch: 13 Loss: [50.5150736]\n",
      "Epoch: 14 Loss: [48.43249139]\n",
      "Epoch: 15 Loss: [46.54743032]\n",
      "Epoch: 16 Loss: [44.66239883]\n",
      "Epoch: 17 Loss: [42.95403036]\n",
      "Epoch: 18 Loss: [41.21936426]\n",
      "Epoch: 19 Loss: [39.5971054]\n",
      "Epoch: 20 Loss: [38.14815106]\n",
      "Epoch: 21 Loss: [36.83156993]\n",
      "Epoch: 22 Loss: [35.60288605]\n",
      "Epoch: 23 Loss: [34.45208605]\n",
      "Epoch: 24 Loss: [33.37243148]\n",
      "Epoch: 25 Loss: [32.33079285]\n",
      "Epoch: 26 Loss: [31.33709786]\n",
      "Epoch: 27 Loss: [30.38594404]\n",
      "Epoch: 28 Loss: [29.47792922]\n",
      "Epoch: 29 Loss: [28.61472265]\n",
      "Epoch: 30 Loss: [27.77812157]\n",
      "Epoch: 31 Loss: [26.97052277]\n",
      "Epoch: 32 Loss: [26.19576709]\n",
      "Epoch: 33 Loss: [25.45200422]\n",
      "Epoch: 34 Loss: [24.73569655]\n",
      "Epoch: 35 Loss: [24.0425359]\n",
      "Epoch: 36 Loss: [23.38799328]\n",
      "Epoch: 37 Loss: [22.75559812]\n",
      "Epoch: 38 Loss: [22.14603864]\n",
      "Epoch: 39 Loss: [21.55716057]\n",
      "Epoch: 40 Loss: [20.99396755]\n",
      "Epoch: 41 Loss: [20.4543279]\n",
      "Epoch: 42 Loss: [19.92756238]\n",
      "Epoch: 43 Loss: [19.3945501]\n",
      "Epoch: 44 Loss: [18.82443142]\n",
      "Epoch: 45 Loss: [18.18376502]\n",
      "Epoch: 46 Loss: [17.42371517]\n",
      "Epoch: 47 Loss: [16.65244725]\n",
      "Epoch: 48 Loss: [15.99024609]\n",
      "Epoch: 49 Loss: [15.41576097]\n",
      "Epoch: 50 Loss: [14.90279191]\n",
      "Epoch: 51 Loss: [14.47721783]\n",
      "Epoch: 52 Loss: [14.10212733]\n",
      "Epoch: 53 Loss: [13.77896479]\n",
      "Epoch: 54 Loss: [13.49286095]\n",
      "Epoch: 55 Loss: [13.2361207]\n",
      "Epoch: 56 Loss: [13.00222914]\n",
      "Epoch: 57 Loss: [12.79733002]\n",
      "Epoch: 58 Loss: [12.60924595]\n",
      "Epoch: 59 Loss: [12.43556773]\n",
      "Epoch: 60 Loss: [12.27446179]\n",
      "Epoch: 61 Loss: [12.12966938]\n",
      "Epoch: 62 Loss: [11.98618714]\n",
      "Epoch: 63 Loss: [11.86057623]\n",
      "Epoch: 64 Loss: [11.73886323]\n",
      "Epoch: 65 Loss: [11.62754028]\n",
      "Epoch: 66 Loss: [11.52051518]\n",
      "Epoch: 67 Loss: [11.41928167]\n",
      "Epoch: 68 Loss: [11.32346865]\n",
      "Epoch: 69 Loss: [11.23154397]\n",
      "Epoch: 70 Loss: [11.1453005]\n",
      "Epoch: 71 Loss: [11.05968041]\n",
      "Epoch: 72 Loss: [10.98157522]\n",
      "Epoch: 73 Loss: [10.90578412]\n",
      "Epoch: 74 Loss: [10.83214702]\n",
      "Epoch: 75 Loss: [10.76004366]\n",
      "Epoch: 76 Loss: [10.69386825]\n",
      "Epoch: 77 Loss: [10.62918306]\n",
      "Epoch: 78 Loss: [10.5669932]\n",
      "Epoch: 79 Loss: [10.50746538]\n",
      "Epoch: 80 Loss: [10.44986598]\n",
      "Epoch: 81 Loss: [10.39551622]\n",
      "Epoch: 82 Loss: [10.34084806]\n",
      "Epoch: 83 Loss: [10.28854085]\n",
      "Epoch: 84 Loss: [10.23843308]\n",
      "Epoch: 85 Loss: [10.18896427]\n",
      "Epoch: 86 Loss: [10.14241588]\n",
      "Epoch: 87 Loss: [10.09668779]\n",
      "Epoch: 88 Loss: [10.04978296]\n",
      "Epoch: 89 Loss: [10.00637845]\n",
      "Epoch: 90 Loss: [9.96367502]\n",
      "Epoch: 91 Loss: [9.92417585]\n",
      "Epoch: 92 Loss: [9.8928598]\n",
      "Epoch: 93 Loss: [9.85647488]\n",
      "Epoch: 94 Loss: [9.82072502]\n",
      "Epoch: 95 Loss: [9.78471238]\n",
      "Epoch: 96 Loss: [9.75122804]\n",
      "Epoch: 97 Loss: [9.71741426]\n",
      "Epoch: 98 Loss: [9.68418368]\n",
      "Epoch: 99 Loss: [9.65237213]\n",
      "Epoch: 100 Loss: [9.6209749]\n",
      "Epoch: 101 Loss: [9.59034145]\n",
      "Epoch: 102 Loss: [9.56032168]\n",
      "Epoch: 103 Loss: [9.53096554]\n",
      "Epoch: 104 Loss: [9.50202566]\n",
      "Epoch: 105 Loss: [9.46547345]\n",
      "Epoch: 106 Loss: [9.44087291]\n",
      "Epoch: 107 Loss: [9.42490682]\n",
      "Epoch: 108 Loss: [9.39025754]\n",
      "Epoch: 109 Loss: [9.36736413]\n",
      "Epoch: 110 Loss: [9.34397975]\n",
      "Epoch: 111 Loss: [9.32171599]\n",
      "Epoch: 112 Loss: [9.3000268]\n",
      "Epoch: 113 Loss: [9.27771041]\n",
      "Epoch: 114 Loss: [9.25698136]\n",
      "Epoch: 115 Loss: [9.2357886]\n",
      "Epoch: 116 Loss: [9.21514103]\n",
      "Epoch: 117 Loss: [9.19482981]\n",
      "Epoch: 118 Loss: [9.17482723]\n",
      "Epoch: 119 Loss: [9.15511878]\n",
      "Epoch: 120 Loss: [9.13569376]\n",
      "Epoch: 121 Loss: [9.11654322]\n",
      "Epoch: 122 Loss: [9.09766066]\n",
      "Epoch: 123 Loss: [9.07905336]\n",
      "Epoch: 124 Loss: [9.06065936]\n",
      "Epoch: 125 Loss: [9.04252077]\n",
      "Epoch: 126 Loss: [9.02436283]\n",
      "Epoch: 127 Loss: [9.00680433]\n",
      "Epoch: 128 Loss: [8.98943078]\n",
      "Epoch: 129 Loss: [8.97224184]\n",
      "Epoch: 130 Loss: [8.95530132]\n",
      "Epoch: 131 Loss: [8.93857979]\n",
      "Epoch: 132 Loss: [8.92207141]\n",
      "Epoch: 133 Loss: [8.91190948]\n",
      "Epoch: 134 Loss: [8.90024356]\n",
      "Epoch: 135 Loss: [8.88170427]\n",
      "Epoch: 136 Loss: [8.86498523]\n",
      "Epoch: 137 Loss: [8.85542414]\n",
      "Epoch: 138 Loss: [8.84492055]\n",
      "Epoch: 139 Loss: [8.82600725]\n",
      "Epoch: 140 Loss: [8.81474949]\n",
      "Epoch: 141 Loss: [8.80651976]\n",
      "Epoch: 142 Loss: [8.78745313]\n",
      "Epoch: 143 Loss: [8.77772653]\n",
      "Epoch: 144 Loss: [8.76827406]\n",
      "Epoch: 145 Loss: [8.74960701]\n",
      "Epoch: 146 Loss: [8.74281125]\n",
      "Epoch: 147 Loss: [8.73130074]\n",
      "Epoch: 148 Loss: [8.71789414]\n",
      "Epoch: 149 Loss: [8.71039073]\n",
      "Epoch: 150 Loss: [8.69069135]\n",
      "Epoch: 151 Loss: [8.68671837]\n",
      "Epoch: 152 Loss: [8.67312871]\n",
      "Epoch: 153 Loss: [8.66422571]\n",
      "Epoch: 154 Loss: [8.65305381]\n",
      "Epoch: 155 Loss: [8.64255546]\n",
      "Epoch: 156 Loss: [8.63275732]\n",
      "Epoch: 157 Loss: [8.62181723]\n",
      "Epoch: 158 Loss: [8.61237457]\n",
      "Epoch: 159 Loss: [8.60187123]\n",
      "Epoch: 160 Loss: [8.5919734]\n",
      "Epoch: 161 Loss: [8.58260859]\n",
      "Epoch: 162 Loss: [8.57182466]\n",
      "Epoch: 163 Loss: [8.56376681]\n",
      "Epoch: 164 Loss: [8.5518636]\n",
      "Epoch: 165 Loss: [8.54539337]\n",
      "Epoch: 166 Loss: [8.53240603]\n",
      "Epoch: 167 Loss: [8.52791285]\n",
      "Epoch: 168 Loss: [8.51843149]\n",
      "Epoch: 169 Loss: [8.50974056]\n",
      "Epoch: 170 Loss: [8.49947783]\n",
      "Epoch: 171 Loss: [8.48926747]\n",
      "Epoch: 172 Loss: [8.48333752]\n",
      "Epoch: 173 Loss: [8.47479192]\n",
      "Epoch: 174 Loss: [8.46636363]\n",
      "Epoch: 175 Loss: [8.45694617]\n",
      "Epoch: 176 Loss: [8.44567076]\n",
      "Epoch: 177 Loss: [8.44205211]\n",
      "Epoch: 178 Loss: [8.43369671]\n",
      "Epoch: 179 Loss: [8.42367959]\n",
      "Epoch: 180 Loss: [8.41678456]\n",
      "Epoch: 181 Loss: [8.40964784]\n",
      "Epoch: 182 Loss: [8.40073106]\n",
      "Epoch: 183 Loss: [8.39340373]\n",
      "Epoch: 184 Loss: [8.38601445]\n",
      "Epoch: 185 Loss: [8.37734828]\n",
      "Epoch: 186 Loss: [8.37053932]\n",
      "Epoch: 187 Loss: [8.36347031]\n",
      "Epoch: 188 Loss: [8.35385231]\n",
      "Epoch: 189 Loss: [8.34848389]\n",
      "Epoch: 190 Loss: [8.34193339]\n",
      "Epoch: 191 Loss: [8.33019475]\n",
      "Epoch: 192 Loss: [8.32710039]\n",
      "Epoch: 193 Loss: [8.3206997]\n",
      "Epoch: 194 Loss: [8.31265007]\n",
      "Epoch: 195 Loss: [8.30482813]\n",
      "Epoch: 196 Loss: [8.29799378]\n",
      "Epoch: 197 Loss: [8.29227912]\n",
      "Epoch: 198 Loss: [8.28493933]\n",
      "Epoch: 199 Loss: [8.27797218]\n",
      "Epoch: 200 Loss: [8.2703308]\n",
      "Epoch: 201 Loss: [8.26539083]\n",
      "Epoch: 202 Loss: [8.25788423]\n",
      "Epoch: 203 Loss: [8.25082807]\n",
      "Epoch: 204 Loss: [8.24391483]\n",
      "Epoch: 205 Loss: [8.23920214]\n",
      "Epoch: 206 Loss: [8.2322492]\n",
      "Epoch: 207 Loss: [8.22345137]\n",
      "Epoch: 208 Loss: [8.21840352]\n",
      "Epoch: 209 Loss: [8.21393698]\n",
      "Epoch: 210 Loss: [8.20712353]\n",
      "Epoch: 211 Loss: [8.19656978]\n",
      "Epoch: 212 Loss: [8.19386791]\n",
      "Epoch: 213 Loss: [8.18942585]\n",
      "Epoch: 214 Loss: [8.18275766]\n",
      "Epoch: 215 Loss: [8.17629235]\n",
      "Epoch: 216 Loss: [8.16751478]\n",
      "Epoch: 217 Loss: [8.16308396]\n",
      "Epoch: 218 Loss: [8.15910723]\n",
      "Epoch: 219 Loss: [8.15293227]\n",
      "Epoch: 220 Loss: [8.14660188]\n",
      "Epoch: 221 Loss: [8.13773018]\n",
      "Epoch: 222 Loss: [8.13372021]\n",
      "Epoch: 223 Loss: [8.13004925]\n",
      "Epoch: 224 Loss: [8.12408677]\n",
      "Epoch: 225 Loss: [8.11796729]\n",
      "Epoch: 226 Loss: [8.11192534]\n",
      "Epoch: 227 Loss: [8.10548874]\n",
      "Epoch: 228 Loss: [8.09942238]\n",
      "Epoch: 229 Loss: [8.09627652]\n",
      "Epoch: 230 Loss: [8.0906972]\n",
      "Epoch: 231 Loss: [8.08481127]\n",
      "Epoch: 232 Loss: [8.07897773]\n",
      "Epoch: 233 Loss: [8.07163893]\n",
      "Epoch: 234 Loss: [8.06693063]\n",
      "Epoch: 235 Loss: [8.06397004]\n",
      "Epoch: 236 Loss: [8.05839523]\n",
      "Epoch: 237 Loss: [8.05273548]\n",
      "Epoch: 238 Loss: [8.04713655]\n",
      "Epoch: 239 Loss: [8.03919045]\n",
      "Epoch: 240 Loss: [8.03565307]\n",
      "Epoch: 241 Loss: [8.03280913]\n",
      "Epoch: 242 Loss: [8.02740505]\n",
      "Epoch: 243 Loss: [8.02193734]\n",
      "Epoch: 244 Loss: [8.01649468]\n",
      "Epoch: 245 Loss: [8.01129522]\n",
      "Epoch: 246 Loss: [8.00448774]\n",
      "Epoch: 247 Loss: [7.99936408]\n",
      "Epoch: 248 Loss: [7.99756614]\n",
      "Epoch: 249 Loss: [7.99235039]\n",
      "Epoch: 250 Loss: [7.98708029]\n",
      "Epoch: 251 Loss: [7.98202433]\n",
      "Epoch: 252 Loss: [7.97694278]\n",
      "Epoch: 253 Loss: [7.97007094]\n",
      "Epoch: 254 Loss: [7.96543193]\n",
      "Epoch: 255 Loss: [7.96378548]\n",
      "Epoch: 256 Loss: [7.95878171]\n",
      "Epoch: 257 Loss: [7.95385206]\n",
      "Epoch: 258 Loss: [7.94894904]\n",
      "Epoch: 259 Loss: [7.94398503]\n",
      "Epoch: 260 Loss: [7.93909722]\n",
      "Epoch: 261 Loss: [7.93352186]\n",
      "Epoch: 262 Loss: [7.9280051]\n",
      "Epoch: 263 Loss: [7.92603024]\n",
      "Epoch: 264 Loss: [7.92205994]\n",
      "Epoch: 265 Loss: [7.91716817]\n",
      "Epoch: 266 Loss: [7.91237175]\n",
      "Epoch: 267 Loss: [7.90763297]\n",
      "Epoch: 268 Loss: [7.90295171]\n",
      "Epoch: 269 Loss: [7.89762215]\n",
      "Epoch: 270 Loss: [7.89244286]\n",
      "Epoch: 271 Loss: [7.89082607]\n",
      "Epoch: 272 Loss: [7.88685886]\n",
      "Epoch: 273 Loss: [7.88214812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274 Loss: [7.87752337]\n",
      "Epoch: 275 Loss: [7.87296101]\n",
      "Epoch: 276 Loss: [7.86845546]\n",
      "Epoch: 277 Loss: [7.86400147]\n",
      "Epoch: 278 Loss: [7.8599247]\n",
      "Epoch: 279 Loss: [7.85407625]\n",
      "Epoch: 280 Loss: [7.85268903]\n",
      "Epoch: 281 Loss: [7.84832158]\n",
      "Epoch: 282 Loss: [7.84435327]\n",
      "Epoch: 283 Loss: [7.83985617]\n",
      "Epoch: 284 Loss: [7.83549622]\n",
      "Epoch: 285 Loss: [7.83118962]\n",
      "Epoch: 286 Loss: [7.82693004]\n",
      "Epoch: 287 Loss: [7.82319208]\n",
      "Epoch: 288 Loss: [7.81754068]\n",
      "Epoch: 289 Loss: [7.81635819]\n",
      "Epoch: 290 Loss: [7.81216094]\n",
      "Epoch: 291 Loss: [7.80780385]\n",
      "Epoch: 292 Loss: [7.80402543]\n",
      "Epoch: 293 Loss: [7.79975975]\n",
      "Epoch: 294 Loss: [7.79562654]\n",
      "Epoch: 295 Loss: [7.79154047]\n",
      "Epoch: 296 Loss: [7.78789384]\n",
      "Epoch: 297 Loss: [7.78269458]\n",
      "Epoch: 298 Loss: [7.78167576]\n",
      "Epoch: 299 Loss: [7.77765674]\n",
      "Epoch: 300 Loss: [7.77343632]\n",
      "Epoch: 301 Loss: [7.76925709]\n",
      "Epoch: 302 Loss: [7.76567184]\n",
      "Epoch: 303 Loss: [7.76161991]\n",
      "Epoch: 304 Loss: [7.75769044]\n",
      "Epoch: 305 Loss: [7.75380302]\n",
      "Epoch: 306 Loss: [7.7519707]\n",
      "Epoch: 307 Loss: [7.745526]\n",
      "Epoch: 308 Loss: [7.74469003]\n",
      "Epoch: 309 Loss: [7.74066381]\n",
      "Epoch: 310 Loss: [7.7367587]\n",
      "Epoch: 311 Loss: [7.73277311]\n",
      "Epoch: 312 Loss: [7.72883545]\n",
      "Epoch: 313 Loss: [7.72566487]\n",
      "Epoch: 314 Loss: [7.72183808]\n",
      "Epoch: 315 Loss: [7.7197128]\n",
      "Epoch: 316 Loss: [7.71381075]\n",
      "Epoch: 317 Loss: [7.71315477]\n",
      "Epoch: 318 Loss: [7.70945777]\n",
      "Epoch: 319 Loss: [7.70554763]\n",
      "Epoch: 320 Loss: [7.70186735]\n",
      "Epoch: 321 Loss: [7.69805516]\n",
      "Epoch: 322 Loss: [7.69410094]\n",
      "Epoch: 323 Loss: [7.69099654]\n",
      "Epoch: 324 Loss: [7.6875409]\n",
      "Epoch: 325 Loss: [7.68694409]\n",
      "Epoch: 326 Loss: [7.67963177]\n",
      "Epoch: 327 Loss: [7.67927032]\n",
      "Epoch: 328 Loss: [7.67594641]\n",
      "Epoch: 329 Loss: [7.67220633]\n",
      "Epoch: 330 Loss: [7.6684888]\n",
      "Epoch: 331 Loss: [7.66482462]\n",
      "Epoch: 332 Loss: [7.66121334]\n",
      "Epoch: 333 Loss: [7.65765115]\n",
      "Epoch: 334 Loss: [7.65453004]\n",
      "Epoch: 335 Loss: [7.65089233]\n",
      "Epoch: 336 Loss: [7.65014614]\n",
      "Epoch: 337 Loss: [7.64675038]\n",
      "Epoch: 338 Loss: [7.64316141]\n",
      "Epoch: 339 Loss: [7.63959257]\n",
      "Epoch: 340 Loss: [7.63607126]\n",
      "Epoch: 341 Loss: [7.63259919]\n",
      "Epoch: 342 Loss: [7.629438]\n",
      "Epoch: 343 Loss: [7.62614147]\n",
      "Epoch: 344 Loss: [7.62551675]\n",
      "Epoch: 345 Loss: [7.62220668]\n",
      "Epoch: 346 Loss: [7.61870223]\n",
      "Epoch: 347 Loss: [7.61521903]\n",
      "Epoch: 348 Loss: [7.61178375]\n",
      "Epoch: 349 Loss: [7.60839795]\n",
      "Epoch: 350 Loss: [7.60505817]\n",
      "Epoch: 351 Loss: [7.6041315]\n",
      "Epoch: 352 Loss: [7.5956236]\n",
      "Epoch: 353 Loss: [7.59851148]\n",
      "Epoch: 354 Loss: [7.59482431]\n",
      "Epoch: 355 Loss: [7.59145978]\n",
      "Epoch: 356 Loss: [7.58813347]\n",
      "Epoch: 357 Loss: [7.58484993]\n",
      "Epoch: 358 Loss: [7.58323324]\n",
      "Epoch: 359 Loss: [7.57612934]\n",
      "Epoch: 360 Loss: [7.57853444]\n",
      "Epoch: 361 Loss: [7.57498557]\n",
      "Epoch: 362 Loss: [7.57168068]\n",
      "Epoch: 363 Loss: [7.56841076]\n",
      "Epoch: 364 Loss: [7.5651847]\n",
      "Epoch: 365 Loss: [7.56200359]\n",
      "Epoch: 366 Loss: [7.56223383]\n",
      "Epoch: 367 Loss: [7.55357469]\n",
      "Epoch: 368 Loss: [7.55600147]\n",
      "Epoch: 369 Loss: [7.55260486]\n",
      "Epoch: 370 Loss: [7.54939935]\n",
      "Epoch: 371 Loss: [7.54622265]\n",
      "Epoch: 372 Loss: [7.54308783]\n",
      "Epoch: 373 Loss: [7.54258236]\n",
      "Epoch: 374 Loss: [7.53532047]\n",
      "Epoch: 375 Loss: [7.53454887]\n",
      "Epoch: 376 Loss: [7.5340605]\n",
      "Epoch: 377 Loss: [7.53045375]\n",
      "Epoch: 378 Loss: [7.52733813]\n",
      "Epoch: 379 Loss: [7.52428573]\n",
      "Epoch: 380 Loss: [7.52549565]\n",
      "Epoch: 381 Loss: [7.51670765]\n",
      "Epoch: 382 Loss: [7.51614021]\n",
      "Epoch: 383 Loss: [7.51560861]\n",
      "Epoch: 384 Loss: [7.51199261]\n",
      "Epoch: 385 Loss: [7.50907455]\n",
      "Epoch: 386 Loss: [7.50947282]\n",
      "Epoch: 387 Loss: [7.50209078]\n",
      "Epoch: 388 Loss: [7.50140844]\n",
      "Epoch: 389 Loss: [7.50045339]\n",
      "Epoch: 390 Loss: [7.49701994]\n",
      "Epoch: 391 Loss: [7.49413921]\n",
      "Epoch: 392 Loss: [7.49121462]\n",
      "Epoch: 393 Loss: [7.49314049]\n",
      "Epoch: 394 Loss: [7.48437769]\n",
      "Epoch: 395 Loss: [7.4839203]\n",
      "Epoch: 396 Loss: [7.48040055]\n",
      "Epoch: 397 Loss: [7.47982206]\n",
      "Epoch: 398 Loss: [7.4764646]\n",
      "Epoch: 399 Loss: [7.47947596]\n",
      "Epoch: 400 Loss: [7.4697868]\n",
      "Epoch: 401 Loss: [7.46943853]\n",
      "Epoch: 402 Loss: [7.46600269]\n",
      "Epoch: 403 Loss: [7.46536253]\n",
      "Epoch: 404 Loss: [7.46655886]\n",
      "Epoch: 405 Loss: [7.45875928]\n",
      "Epoch: 406 Loss: [7.45827052]\n",
      "Epoch: 407 Loss: [7.45480849]\n",
      "Epoch: 408 Loss: [7.45389836]\n",
      "Epoch: 409 Loss: [7.45069437]\n",
      "Epoch: 410 Loss: [7.45370146]\n",
      "Epoch: 411 Loss: [7.44466482]\n",
      "Epoch: 412 Loss: [7.44433163]\n",
      "Epoch: 413 Loss: [7.4409511]\n",
      "Epoch: 414 Loss: [7.44000971]\n",
      "Epoch: 415 Loss: [7.43687763]\n",
      "Epoch: 416 Loss: [7.44098088]\n",
      "Epoch: 417 Loss: [7.43099701]\n",
      "Epoch: 418 Loss: [7.43078441]\n",
      "Epoch: 419 Loss: [7.42747376]\n",
      "Epoch: 420 Loss: [7.42648072]\n",
      "Epoch: 421 Loss: [7.42877828]\n",
      "Epoch: 422 Loss: [7.42077039]\n",
      "Epoch: 423 Loss: [7.42038864]\n",
      "Epoch: 424 Loss: [7.41705371]\n",
      "Epoch: 425 Loss: [7.41357478]\n",
      "Epoch: 426 Loss: [7.41926957]\n",
      "Epoch: 427 Loss: [7.40977802]\n",
      "Epoch: 428 Loss: [7.40950527]\n",
      "Epoch: 429 Loss: [7.40623541]\n",
      "Epoch: 430 Loss: [7.40280896]\n",
      "Epoch: 431 Loss: [7.40933247]\n",
      "Epoch: 432 Loss: [7.39906609]\n",
      "Epoch: 433 Loss: [7.39889055]\n",
      "Epoch: 434 Loss: [7.39567527]\n",
      "Epoch: 435 Loss: [7.39229583]\n",
      "Epoch: 436 Loss: [7.39945509]\n",
      "Epoch: 437 Loss: [7.38861912]\n",
      "Epoch: 438 Loss: [7.38852046]\n",
      "Epoch: 439 Loss: [7.38535336]\n",
      "Epoch: 440 Loss: [7.38201543]\n",
      "Epoch: 441 Loss: [7.38967301]\n",
      "Epoch: 442 Loss: [7.37841406]\n",
      "Epoch: 443 Loss: [7.37837686]\n",
      "Epoch: 444 Loss: [7.37525269]\n",
      "Epoch: 445 Loss: [7.37195257]\n",
      "Epoch: 446 Loss: [7.38001245]\n",
      "Epoch: 447 Loss: [7.36843192]\n",
      "Epoch: 448 Loss: [7.36844546]\n",
      "Epoch: 449 Loss: [7.3653602]\n",
      "Epoch: 450 Loss: [7.36209497]\n",
      "Epoch: 451 Loss: [7.37049074]\n",
      "Epoch: 452 Loss: [7.35865794]\n",
      "Epoch: 453 Loss: [7.35871443]\n",
      "Epoch: 454 Loss: [7.35566526]\n",
      "Epoch: 455 Loss: [7.35243266]\n",
      "Epoch: 456 Loss: [7.36112742]\n",
      "Epoch: 457 Loss: [7.34908359]\n",
      "Epoch: 458 Loss: [7.34917883]\n",
      "Epoch: 459 Loss: [7.34616315]\n",
      "Epoch: 460 Loss: [7.34296107]\n",
      "Epoch: 461 Loss: [7.35189988]\n",
      "Epoch: 462 Loss: [7.33969782]\n",
      "Epoch: 463 Loss: [7.33982474]\n",
      "Epoch: 464 Loss: [7.33684086]\n",
      "Epoch: 465 Loss: [7.33351895]\n",
      "Epoch: 466 Loss: [7.34291736]\n",
      "Epoch: 467 Loss: [7.33050264]\n",
      "Epoch: 468 Loss: [7.33065051]\n",
      "Epoch: 469 Loss: [7.32754666]\n",
      "Epoch: 470 Loss: [7.32449702]\n",
      "Epoch: 471 Loss: [7.33385915]\n",
      "Epoch: 472 Loss: [7.32162694]\n",
      "Epoch: 473 Loss: [7.32151592]\n",
      "Epoch: 474 Loss: [7.31861092]\n",
      "Epoch: 475 Loss: [7.31555717]\n",
      "Epoch: 476 Loss: [7.32511271]\n",
      "Epoch: 477 Loss: [7.31261325]\n",
      "Epoch: 478 Loss: [7.31277202]\n",
      "Epoch: 479 Loss: [7.30987322]\n",
      "Epoch: 480 Loss: [7.30672606]\n",
      "Epoch: 481 Loss: [7.31658933]\n",
      "Epoch: 482 Loss: [7.30385661]\n",
      "Epoch: 483 Loss: [7.30408701]\n",
      "Epoch: 484 Loss: [7.30122055]\n",
      "Epoch: 485 Loss: [7.29815838]\n",
      "Epoch: 486 Loss: [7.30813395]\n",
      "Epoch: 487 Loss: [7.29529742]\n",
      "Epoch: 488 Loss: [7.29555813]\n",
      "Epoch: 489 Loss: [7.29272033]\n",
      "Epoch: 490 Loss: [7.29790869]\n",
      "Epoch: 491 Loss: [7.29041725]\n",
      "Epoch: 492 Loss: [7.29020328]\n",
      "Epoch: 493 Loss: [7.28724321]\n",
      "Epoch: 494 Loss: [7.28413189]\n",
      "Epoch: 495 Loss: [7.29053812]\n",
      "Epoch: 496 Loss: [7.28175132]\n",
      "Epoch: 497 Loss: [7.28166684]\n",
      "Epoch: 498 Loss: [7.2787655]\n",
      "Epoch: 499 Loss: [7.27570065]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 10\n",
    "delta_const = 1e-10\n",
    "num_batches = num_samples/batch_size\n",
    "eps = 1e-3\n",
    "rho = 0.1\n",
    "loss1 = []\n",
    "r = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        for k in range(batch_size):\n",
    "            x1 = np.matmul(W1,x_train_subs[j*batch_size+k]).reshape(-1,1)+b1\n",
    "            z1 = relu(x1)\n",
    "            \n",
    "            x2 = np.matmul(W2,z1).reshape(-1,1)+b2\n",
    "            z2 = relu(x2)\n",
    "\n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j*batch_size+k])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j*batch_size+k].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(x2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(x1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j*batch_size+k].reshape(-1,1).T)\n",
    "        r[\"W1\"] = (1-rho)*r[\"W1\"] + rho*W1_upd*W1_upd\n",
    "        r[\"W2\"] = (1-rho)*r[\"W2\"] + rho*W2_upd*W2_upd\n",
    "        r[\"W3\"] = (1-rho)*r[\"W3\"] + rho*W3_upd*W3_upd\n",
    "        r[\"b1\"] = (1-rho)*r[\"b1\"] + rho*b1_upd*b1_upd\n",
    "        r[\"b2\"] = (1-rho)*r[\"b2\"] + rho*b2_upd*b2_upd\n",
    "        r[\"b3\"] = (1-rho)*r[\"b3\"] + rho*b3_upd*b3_upd\n",
    "        W3 = W3 - (eps*W3_upd)/np.sqrt(delta_const + r[\"W3\"])\n",
    "        W2 = W2 - (eps*W2_upd)/np.sqrt(delta_const + r[\"W2\"])\n",
    "        W1 = W1 - (eps*W1_upd)/np.sqrt(delta_const + r[\"W1\"])\n",
    "        b3 = b3 - (eps*b3_upd)/np.sqrt(delta_const + r[\"b3\"])\n",
    "        b2 = b2 - (eps*b2_upd)/np.sqrt(delta_const + r[\"b2\"])\n",
    "        b1 = b1 - (eps*b1_upd)/np.sqrt(delta_const + r[\"b1\"])\n",
    "    loss1.append(loss)\n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = relu(np.matmul(W1,test_x[4]).reshape(-1,1)+b1)\n",
    "z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.92912534e-11]\n",
      " [9.98196417e-01]\n",
      " [1.80358319e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc =accuracy_score(y_pred=preds,y_true=true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,501)\n",
    "plt.plot(epochs,loss1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238.09232098132347,207.1935443392004,178.5553045369865,155.1095155440195,134.15528479780264,115.80221065814715,99.11284279085812,84.01449087976515,73.153973346641,65.53428185840983,60.0374511899548,55.996014206239906,52.93065497828168,50.51507360112955,48.43249138583043,46.54743032429699,44.66239882583107,42.95403035931178,41.2193642572531,39.59710540250523,38.14815106307132,36.831569927156956,35.602886047407566,34.4520860483485,33.37243148097362,32.33079285344525,31.337097862035016,30.385944037980803,29.477929222529305,28.614722653942607,27.778121573899934,26.9705227666933,26.19576709369456,25.45200422105666,24.735696547355886,24.042535903906273,23.387993281606512,22.755598118914467,22.146038639317382,21.55716056771085,20.993967545768623,20.45432789743822,19.927562376783335,19.394550101652992,18.824431419792237,18.183765017025344,17.423715166132787,16.65244725437301,15.990246086522127,15.415760974707727,14.902791909948636,14.477217834183369,14.102127326566631,13.778964790232385,13.49286095214211,13.23612070339569,13.002229137015489,12.79733002356562,12.609245953316377,12.435567728028214,12.274461793336249,12.129669375219537,11.986187142175162,11.860576232986343,11.7388632289028,11.627540279844963,11.520515176929901,11.41928166972473,11.323468647705129,11.23154396933691,11.145300496580587,11.059680414477873,10.981575215940206,10.905784122976547,10.832147016152309,10.760043655142775,10.69386824580924,10.629183064935951,10.566993204753246,10.507465380136672,10.449865983631588,10.39551622100868,10.340848063561571,10.288540847584699,10.238433083296119,10.188964267076095,10.14241588038229,10.096687788188746,10.04978295752323,10.006378452911548,9.96367502265142,9.924175852093393,9.892859801694764,9.856474884934755,9.820725021695718,9.784712381288829,9.75122803581104,9.717414257248446,9.684183677595081,9.652372131378511,9.620974899213431,9.590341454359825,9.560321683359946,9.53096553768977,9.502025662116695,9.465473452633466,9.440872912150205,9.424906815849631,9.390257538444752,9.367364133242576,9.343979748271911,9.3217159932669,9.300026799631118,9.27771040721448,9.256981359483126,9.235788599306307,9.215141032163967,9.194829813122151,9.174827232876124,9.155118776078208,9.135693758337524,9.116543219467985,9.097660661874349,9.079053363894545,9.060659359613599,9.042520771107702,9.024362825777297,9.00680432933074,8.989430784676346,8.972241843306467,8.95530132239798,8.938579786218229,8.922071406470467,8.91190948494747,8.900243563074982,8.881704274354759,8.86498522819407,8.855424136815117,8.844920552045256,8.826007248109432,8.814749492656334,8.806519756108875,8.787453131553729,8.77772653160885,8.768274056020811,8.749607010417218,8.74281125488951,8.731300737626592,8.717894139924766,8.710390727242986,8.690691347705183,8.686718373433145,8.673128705496586,8.664225706854447,8.65305381396235,8.642555456800672,8.63275731502371,8.621817233005164,8.612374570388155,8.601871230697139,8.591973401983715,8.58260858906794,8.571824655539086,8.563766812336107,8.551863599916683,8.545393373612743,8.532406026933698,8.52791285293153,8.518431491314173,8.50974056159103,8.49947783334679,8.489267470082755,8.483337519578686,8.474791924922405,8.466363632858375,8.456946171491241,8.445670760238297,8.442052111468247,8.433696708284435,8.423679588584326,8.416784559997918,8.409647838295154,8.4007310593471,8.393403727450831,8.386014454781444,8.377348280238397,8.370539324343993,8.363470307997883,8.353852306173653,8.34848388658761,8.341933389369787,8.330194753665122,8.327100394587525,8.320699697774273,8.31265007305325,8.304828133353388,8.297993780541024,8.292279115988793,8.284939334472131,8.27797217727603,8.270330803331774,8.265390832129746,8.257884228260739,8.250828068283287,8.243914825728739,8.23920214045502,8.232249204260107,8.223451372565739,8.218403515761889,8.213936983111392,8.207123531371074,8.19656978228829,8.193867905159811,8.18942585075864,8.182757664096828,8.176292351588803,8.167514783828226,8.163083956163986,8.159107231094616,8.15293227212075,8.146601880007154,8.13773018484349,8.13372020654249,8.130049249794173,8.124086765375667,8.11796729379453,8.11192534361741,8.105488741300729,8.09942237846439,8.096276516431475,8.09069719731274,8.084811268065108,8.078977732280125,8.071638930489062,8.066930625223064,8.063970037430348,8.058395230916895,8.052735475055671,8.04713654821246,8.039190446935716,8.035653068722219,8.032809134054736,8.027405045055064,8.02193733909879,8.016494681691483,8.011295220038276,8.004487740739453,7.999364078118551,7.9975661391964055,7.9923503867051515,7.987080291335716,7.982024329891114,7.976942782981592,7.970070941223638,7.965431931085691,7.963785482943311,7.958781713799636,7.953852058640065,7.948949041895865,7.943985034358615,7.939097224106237,7.933521855044625,7.928005099917496,7.9260302385906165,7.9220599372454865,7.917168168923932,7.912371750083221,7.907632970274742,7.902951712456114,7.897622145734247,7.892442858966548,7.89082606953096,7.886858862098258,7.882148119440242,7.877523368005334,7.872961014887492,7.868455457261056,7.864001471338808,7.859924702470869,7.854076251554312,7.852689030763061,7.848321579567496,7.844353273648265,7.839856166236959,7.835496216945362,7.831189617467386,7.826930041995556,7.823192079104174,7.817540676066579,7.816358191550143,7.812160943934593,7.807803851261471,7.804025425929559,7.799759753329784,7.795626543979923,7.7915404685845555,7.787893843793902,7.7826945802423735,7.781675763868767,7.777656740217797,7.773436324197905,7.7692570863958785,7.765671840730356,7.761619912181744,7.757690443489395,7.753803020557728,7.7519707001180285,7.7455259962063945,7.744690034536028,7.740663813570737,7.736758702371319,7.732773106965737,7.728835450426488,7.72566487269848,7.721838079401448,7.7197128000537,7.713810746562496,7.713154769168799,7.70945776510564,7.705547634388283,7.701867352939623,7.6980551578247844,7.694100940256312,7.690996539654309,7.687540902064261,7.686944093413431,7.679631768707981,7.679270321470816,7.675946410548973,7.672206332165975,7.668488804349918,7.664824623159157,7.661213342977785,7.657651148518724,7.654530035276133,7.650892325706503,7.650146143535116,7.646750382417895,7.643161408502936,7.6395925718030755,7.636071262146283,7.63259919326982,7.629437995828781,7.626141474627274,7.625516745188135,7.622206684755756,7.618702231848879,7.615219029345372,7.6117837465176414,7.6083979503415184,7.6050581726674125,7.604131498269627,7.595623602569177,7.598511484820332,7.594824309961584,7.591459779029554,7.588133471218705,7.584849932813784,7.58323324241093,7.576129338704773,7.578534436195046,7.574985565108522,7.571680681530498,7.568410761886491,7.5651846967422625,7.562003592559449,7.5622338262762545,7.553574686369853,7.556001465906087,7.5526048637427206,7.54939935180649,7.546222649400021,7.543087825511454,7.542582355186552,7.535320472818579,7.534548867229815,7.534060499850238,7.530453749516189,7.52733813090529,7.524285726745843,7.525495650692433,7.516707653415394,7.5161402054936906,7.5156086126845905,7.511992605495116,7.50907454839268,7.509472821302061,7.502090775321645,7.501408437148387,7.500453388270047,7.497019936227547,7.494139206126694,7.491214619378092,7.493140492014523,7.484377694176136,7.483920300145876,7.480400552339714,7.4798220576799554,7.476464599590076,7.479475962885239,7.469786797628561,7.469438531398144,7.466002687003305,7.465362527408581,7.466558862218519,7.458759283944237,7.4582705193111805,7.454808489796122,7.4538983641968315,7.450694371130685,7.453701456871973,7.444664824202834,7.444331630613366,7.440951097727292,7.44000970554914,7.4368776310649505,7.440980884219924,7.430997005891296,7.430784413946018,7.4274737553611905,7.4264807160577675,7.4287782843224175,7.420770393307538,7.42038863502914,7.417053710787656,7.41357478191398,7.419269569586389,7.409778020042623,7.409505269223211,7.406235406822581,7.4028089610925045,7.409332466342604,7.3990660943446285,7.3988905510632685,7.395675268941154,7.392295834233073,7.399455088686687,7.388619120599423,7.388520459307633,7.385353361173622,7.382015430585989,7.38967300615378,7.378414060779541,7.378376860966936,7.375252689297891,7.37195256705019,7.380012445660878,7.368431921407435,7.36844545722552,7.365360202138463,7.362094967324645,7.370490736513182,7.35865793813965,7.358714431461079,7.3556652573498615,7.3524326573450045,7.361127421724468,7.349083589007539,7.349178834196816,7.3461631539556835,7.342961065903198,7.351899878577621,7.339697822638406,7.33982474335522,7.336840863938104,7.333518951303713,7.3429173644022026,7.330502636912906,7.3306505066181975,7.327546663663181,7.3244970176,7.3338591523423435,7.3216269359664,7.321515917092163,7.318610918222035,7.31555716515506,7.325112706919401,7.312613245119579,7.312772021999265,7.309873216795378,7.3067260573771176,7.31658933412898,7.303856607913721,7.304087010633685,7.301220548917055,7.298158384485546,7.3081339510489185,7.295297419073587,7.2955581347420875,7.292720325416915,7.297908694175214,7.290417246239791,7.2902032759928765,7.287243210097376,7.284131892893195,7.290538121619492,7.281751319191226,7.281666837320806,7.278765502521321,7.275700645495024,"
     ]
    }
   ],
   "source": [
    "for elem in np.array(loss1):\n",
    "    print(str(elem[0])+',' , end = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
